[{"title":"MIPProject","url":"/2022/MIPProject/","content":"[DCE](/2022/DCE/)\n","tags":["class"]},{"title":"DCE","url":"/2022/DCE/","content":"# 分割小腿断层剖图6块肌肉\n\n说明：Medical Image Processing课程project 2的源码和实现过程（有问题的可以私聊我，欢迎交流\n\n目标：分割小腿断层剖图6块肌肉\n\n源码：https://github.com/JoyceLiang-sudo/DCE （欢迎✨\n\n1. 准备数据\n\n   1. 总说\n\n      - 这次任务很简单，所以只用了19张图片和对应的标注文件（其实我感觉10张就够了，但是没试过\n      - 用matlab或者百度easydata标注图片，现在标注分割图片都是点几个点就可以了，都不用画轮廓\n      - easydata有智能标注工具，只要手标10张，它就能帮你把剩下的图片都标注了（但是这里没必要要那么多张训练集\n      - 模型输入是png图片，标注是COCO格式的\n   2. 具体实现（python\n\n      - 老师只给了一个128*128*247的mat格式文件，先把它分为247个mat，表示247张图片\n\n        ```python\n        def divide_mat():\n            data = sio.loadmat('DCE_IMs.mat')\n            ims = data['IMs']\n            for img_id in range(247):\n                name = './mat/ims'+str(img_id)+'.mat'\n                scio.savemat(name, {'IMs': ims[:, :, img_id]})\n        ```\n      - mat转为png图片\n\n        ```python\n        def mat_png():\n            path = './images/'\n            data = sio.loadmat('DCE_IMs.mat')\n            ims = data['IMs']\n            pyplot.imshow(ims[:,:,0])\n            pyplot.show()\n            for img_id in range(247):\n                im = ims[:, :, img_id] /430 * 255\n                new_im = Image.fromarray(im.astype(np.uint8))\n                new_im.save(path+'ims'+str(img_id)+ '.png')  # 保存图片\n        ```\n      - 去easydata标注10张图片\n\n        ![Untitled](http://img.peterli.club/joy/Untitled.png)\n\n        10张图片的标注信息都存在一个文件里\n\n        ![Untitled](http://img.peterli.club/joy/Untitled%201.png)\n2. 配置环境\n\n   - 任务很小，不用拿gpu跑，我是拿了笔记本Apple M1 Pro 16GB，跑了200个epochs，用了一分钟都没有？差不多\n   - 装anaconda 创虚拟环境这些基本操作这里就不说了，不懂的建议bing搜索或者来问我\n   - 直接装requirements.txt里的package，pytorch装的是官方支持Mac加速的\n\n     > `conda install pytorch torchvision torchaudio -c pytorch`\n     >\n   - AI建议还是用linux或mac，配环境win分分钟会让你想死（无穷无尽的各种奇奇怪怪的问题，相对来说还是Linux好用，适配性好很多\n3. 训练\n\n   - config.yml 里面装的是各种参数，自调\n   - 算法用的是segresnet，随便选的，这个小任务感觉随便找个算法效果都挺好的\n   - 训练代码在train.py，直接跑这个就可以，要是想改算法，就改下图中的SegResNet吧啦吧啦那一行，换成别的算法\n\n     ![Untitled](http://img.peterli.club/joy/Untitled%202.png)\n4. 验证\n\n   - model文件夹里存的是准确率最高的模型\n   - 可以用logs里的文件可视化训练效果，loss和accuracy啥的，命令为（记得pip install tensorboard\n\n     `tensorboard: python3 -m tensorboard.main --logdir ./ --bind_all`\n   - visualization.py用来可视化分割效果的，随便找张不是训练集里的图片测试一下就行（尽力画的好看了\n\n作业结束，非常简单\n\n此处感谢李云灏先生的帮助，感恩！\n","tags":["class"]},{"title":"2.10 Dedicated database KEGG, OMIM","url":"/2022/2-10-Dedicated-database-KEGG-OMIM/","content":"# 2.10 专用数据库：KEGG、OMIM\n\n1. [KEGG](https://www.genome.jp/kegg/) 是关于基因、蛋白质、生化反应以及通路的综合生物信息数据库，由多个子库构成\n\n   <img src=\"http://img.peterli.club/joy/202210211052313.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   - KEGG PATHWAY：包含了大量物种的代谢与生物信号传导通路信息\n\n     <img src=\"http://img.peterli.club/joy/202210211052318.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n     - 总图\n\n       <img src=\"http://img.peterli.club/joy/202210211052326.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n     - 代谢专题\n\n       <img src=\"http://img.peterli.club/joy/202210211052327.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n     - 生化代谢图\n\n       <img src=\"http://img.peterli.club/joy/202210211052334.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n     - 放大圆圈，每一个点代表一个化合物，每条线代表生化反应\n\n       <img src=\"http://img.peterli.club/joy/202210211052337.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n     - 三羧酸循环详细通路图\n\n       <img src=\"http://img.peterli.club/joy/202210211052809.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n     - KO数据库\n\n       <img src=\"http://img.peterli.club/joy/202210211052830.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n   - Organismal Systems\n\n     <img src=\"http://img.peterli.club/joy/202210211052028.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n     - 信号传导通路图\n\n       <img src=\"http://img.peterli.club/joy/202210211052130.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n     - 蛋白质详细信息\n\n       <img src=\"http://img.peterli.club/joy/202210211052150.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n     - DRUG\n\n       <img src=\"http://img.peterli.club/joy/202210211052175.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n2. [OMIM](https://www.omim.org/) 人类孟德尔遗传在线数据库 (Online Mendel Inheritance Inheritance in Man) ，是一个将遗传病分类，并链接到相关人类基因组中的数据库\n\n   OMIM为临床医生和科研人员提供了权威可信的关于遗传疾病及相关疾病基因位点的详细信息\n\n   <img src=\"http://img.peterli.club/joy/202210211052284.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   - 搜索\n\n   <img src=\"http://img.peterli.club/joy/202210211052334.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   <img src=\"http://img.peterli.club/joy/202210211052425.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n   - 这个附近基因的列表以及引发的各种疾病\n\n   <img src=\"http://img.peterli.club/joy/202210211052527.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   - 查看基因的详细信息\n\n   <img src=\"http://img.peterli.club/joy/202210211052653.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.9 Secondary Protein Database","url":"/2022/2-9-Secondary-Protein-Database/","content":"\n# 2.9 二级蛋白质数据库：Pfam、CATH、SCOP2\n\n1. [Pfam](http://pfam.xfam.org)数据库 是一个蛋白质结构域家族的集合\n\n   <img src=\"http://img.peterli.club/joy/202210211034685.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   \n\n   - 搜索\n\n   <img src=\"http://img.peterli.club/joy/202210211034691.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   \n\n   - 输入蛋白质序列\n\n   <img src=\"http://img.peterli.club/joy/202210211034694.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n   \n\n   - 找到四个结构域\n\n   <img src=\"http://img.peterli.club/joy/202210211034698.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n   \n\n   - 查看TIR\n\n     - Summary：获得这个结构域的功能信息及功能注释及结构信息\n\n       <img src=\"http://img.peterli.club/joy/202210211034702.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n       \n\n     - Domain organisation：目前有多少蛋白质拥有TIR结构域以及TIR结构域和其他结构域的组合搭配关系\n\n       <img src=\"http://img.peterli.club/joy/202210211034704.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n       \n\n     - Structures：列出目前所有包含TIR结构域的蛋白质结构，以及他们在序列数据库Uniprot和结构数据库PDB中的链接\n\n       <img src=\"http://img.peterli.club/joy/202210211034323.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n       \n\n2. [CATH 数据库](https://www.cathdb.info/)：结构分类数据库\n\n\n   - Gene3D 里的信息为绝大多数还未解析 3D 结构的蛋白质提供了重要的功能研究依据\n\n   <img src=\"http://img.peterli.club/joy/202210211034489.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   \n\n   - CATH给每一层的每一种结构分类命名，因此每个结构域会有像黄色那样的分类代码\n\n     第一个数字：C            第二个数字：A            第三个数字：T            第四个数字：H\n\n   <img src=\"http://img.peterli.club/joy/202210211034567.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   \n\n   - CATH-Gene3D 还为超过 500 万条来自公共数据库的蛋白质序列进行了结构分类预测\n\n     - 输入\n\n       <img src=\"http://img.peterli.club/joy/202210211034594.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n       \n\n     - 结构分类代码是2.70.40.10\n\n       <img src=\"http://img.peterli.club/joy/202210211034615.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n       \n\n     - 可以获得各个层次具体的结构分类信息以及各种结构相关分析信息\n\n       <img src=\"http://img.peterli.club/joy/202210211034761.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n       \n\n       把所有拥有2.70.40.10.10结构分类的结构域根据序列相似度进行了一下聚类，不同深浅的圈代表不同的相似度\n\n       三个黄圆代表3H6X这个结构涉及的三个结构域，因为这个结构有三条量，三条量在序列水平上是完全相同的\n\n       <img src=\"http://img.peterli.club/joy/202210211034066.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n       \n\n     从这个分类里挑出了19个有代表性的结构域，并且把他们的3D结构叠加在了一起，图上可以看到这个分类的总体特征以及差异产生的位置\n\n   <img src=\"http://img.peterli.club/joy/202210211034226.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   \n\n3. [SCOP2 数据库](https://scop.mrc-lmb.cam.ac.uk/scop2/)：结构分类数据库。更多考虑蛋白质的进化关系，分类依赖于人工验证\n\n   SCOP2 分类基于四个层次：从顶部到底部分别为类 (Class)、家族 (Family)、超家族 (Super family) 和折叠 (Fold)\n\n   <img src=\"http://img.peterli.club/joy/202210211034423.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n   \n\n   <img src=\"http://img.peterli.club/joy/202210211034455.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n   \n\n   分类层次\n\n   <img src=\"http://img.peterli.club/joy/202210211034654.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   \n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.8 Primary Protein Structure Database PDB","url":"/2022/2-8-Primary-Protein-Structure-Database-PDB/","content":"# 2.8 一级蛋白质结构数据库：PDB\n\n- 蛋白质的结构可分为四级：\n\n  - 一级结构 Primary structure: 氨基酸序列\n  - 二级结构 Secondary structure：周期性的结构构象，α 螺旋，β折叠等\n  - 三级结构 Tertiary structure：整条多肽链的三维空间结构，3D 结构\n  - 四级结构 Quaternary structure：几个蛋白质分子（亚基）形成的复合体，如四聚体\n\n  <img src=\"http://img.peterli.club/joy/202210211024480.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n- **蛋白质结构数据库** (Protein Data Bank, [PDB](https://www.rcsb.org/)) 是全世界唯一存储**生物大分子 3D 结构**的数据库。这些生物大分子除了**蛋白质**以外还包括**核酸**及两者的**复合物**。只有通过**实验方法**获得的 3D 结构才会被收入其中。目前 PDB 数据库每周更新一次，至今，PDB 收录的结构数据已超过十二万条，其中 90%以上为蛋白质结构\n\n  <img src=\"http://img.peterli.club/joy/202210211024487.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  - 把作者的名字和蛋白质的名字一起搜索\n\n    <img src=\"http://img.peterli.club/joy/202210211024492.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - **PDB ID：**数据库检索号，**一个结构对应一个 PBD ID**，而不是一个蛋白质对应一个 PBD ID，因为可以有很多结构\n\n    <img src=\"http://img.peterli.club/joy/202210211024484.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - **PDB 文件注释解读：**[一级蛋白质结构数据库：PDB-02 P21](https://www.bilibili.com/video/BV13t411372E?p=21)\n\n    - 基本信息部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024500.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n      - HEADER\n\n        <img src=\"http://img.peterli.club/joy/202210211024486.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - TITLE\n\n        <img src=\"http://img.peterli.club/joy/202210211024946.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - COMPND\n\n        <img src=\"http://img.peterli.club/joy/202210211024982.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - SOURCE\n\n        <img src=\"http://img.peterli.club/joy/202210211024008.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - KEYWDS\n\n        <img src=\"http://img.peterli.club/joy/202210211024034.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - EXPDTA\n\n        <img src=\"http://img.peterli.club/joy/202210211024060.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - AUTHOR\n\n        <img src=\"http://img.peterli.club/joy/202210211024309.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - REVDAT\n\n        <img src=\"http://img.peterli.club/joy/202210211024400.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - JRNL\n\n        <img src=\"http://img.peterli.club/joy/202210211024641.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - REMARK\n\n        <img src=\"http://img.peterli.club/joy/202210211024668.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 一级结构信息部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024700.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n      - DBREF\n\n        <img src=\"http://img.peterli.club/joy/202210211024800.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - SEQRES\n\n        <img src=\"http://img.peterli.club/joy/202210211024009.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n      - MODRES\n\n        <img src=\"http://img.peterli.club/joy/202210211024247.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 非标准残基部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024269.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 二级结构部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024334.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n      - LINK\n\n        <img src=\"http://img.peterli.club/joy/202210211024360.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 实验参数部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024697.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 3D坐标部分\n\n      <img src=\"http://img.peterli.club/joy/202210211024822.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - CONECT\n\n      <img src=\"http://img.peterli.club/joy/202210211024933.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    - 档案管理部分\n\n<img src=\"http://img.peterli.club/joy/202210211024984.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- **PDB 文件 3D 展示 JSmal：**[一级蛋白质结构数据库：PDB-03 P22](https://www.bilibili.com/video/BV13t411372E?p=22)\n\n  <img src=\"http://img.peterli.club/joy/202210211024166.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.7 Primary Protein Sequence Database UniProt","url":"/2022/2-7-Primary-Protein-Sequence-Database-UniProt/","content":"\n# 2.7 一级蛋白质序列数据库 UniProt\n\n一级蛋白质数据库分为蛋白质序列数据库和蛋白质结构数据库，里面的数据都是通过实验方法直接得到的基础数据\n\n二级蛋白质数据库都是在一级数据库的基础上分析整理加工出来的\n\n<img src=\"http://img.peterli.club/joy/202210211008316.png\" alt=\"image-20221021100846294\" style=\"zoom: 33%;\" />\n\n- [UniProt](https://www.uniprot.org/) = Swiss-Prot + TrEMBL + PIR\n\n<img src=\"http://img.peterli.club/joy/202210211009832.png\" alt=\"image-20221021100911809\" style=\"zoom: 33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210211008928.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n- **UniProt 三个层次数据库**：\n  - **UniParc**: 收录所有 UniProt 数据库子库中的蛋白质序列，量大，粗糙\n  - **UniRef**: 归纳 UniProt 几个主要数据库并将重复序列去除后的数据库\n  - **UniProtKB**: 有详细注释并与其他数据库有链接的数据库，分为 UniProtKB/Swiss-Prot(人工注释，reviewed)和 UniProtKB/TrEMBL(计算机自动注释，not reviewed)\n- **详见视频**：**[一级蛋白质序列数据库：UniProtKB-02 P18](https://www.bilibili.com/video/BV13t411372E?p=18)**\n\n- 查找 注：TrEMBL数据库中的数量远远大于Swiss-Prot数据库中的，前者是自动注释的没有经过检查，后者是人工注释的并且经过检查\n\n<img src=\"http://img.peterli.club/joy/202210211008930.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- UniProtKB\n\n  - 检索号和名称都是唯一的，有第三列的符号表示在Swiss-Prot里面，经过人工注释和检查的，没有的表示在TrEMBL里的\n\n    <img src=\"http://img.peterli.club/joy/202210211008932.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - 标签\n\n    <img src=\"http://img.peterli.club/joy/202210211008933.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Function\n\n    <img src=\"http://img.peterli.club/joy/202210211008927.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Names & Taxonomy\n\n    <img src=\"http://img.peterli.club/joy/202210211008942.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Subcellular location\n\n    成熟的蛋白质必须在特定的细胞部位才能发挥其生物学功能，蛋白质在细胞内不同组分中的定位即为蛋白质的亚细胞定位\n\n    亚细胞定位对蛋白质的生理功能有着直接的影响，处于合适的亚细胞定位的蛋白质才能行使其正常的功能，目前研究亚细胞定位的数据来源基本都是属于Swiss-Prot数据库\n\n    <img src=\"http://img.peterli.club/joy/202210211008782.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n    \n\n  - Pathology & Biotech\n\n    比如99位丝氨酸会突变成丙氨酸，从而导致磷酸化的缺失\n\n    <img src=\"http://img.peterli.club/joy/202210211008807.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - PTM/Processing\n\n    比如信号肽在到达了指定位置之后，要被剪切掉，有些氨基酸位点上会发生乙酰化、甲基化、磷酸化等翻译后修饰\n\n    <img src=\"http://img.peterli.club/joy/202210211008833.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Expression\n\n    <img src=\"http://img.peterli.club/joy/202210211008857.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Interaction\n\n    <img src=\"http://img.peterli.club/joy/202210211008879.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Structure\n\n    <img src=\"http://img.peterli.club/joy/202210211008397.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Family & Domains\n\n    <img src=\"http://img.peterli.club/joy/202210211008447.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Sequences\n\n    含有多个异构体的蛋白质会显示多条序列。这个蛋白质有两个异构体，一个线粒体型的，一个细胞核型的，所以会显示两条序列\n\n    <img src=\"http://img.peterli.club/joy/202210211008472.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Cross-references\n\n    <img src=\"http://img.peterli.club/joy/202210211008551.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Entry information\n\n    <img src=\"http://img.peterli.club/joy/202210211008704.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Miscellaneous\n\n    <img src=\"http://img.peterli.club/joy/202210211008134.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Similar proteins\n\n    <img src=\"http://img.peterli.club/joy/202210211008159.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - Flat File\n\n    <img src=\"http://img.peterli.club/joy/202210211008252.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n    <img src=\"http://img.peterli.club/joy/202210211008405.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.6 Secondary Nucleic Acid Database","url":"/2022/2-6-Secondary-Nucleic-Acid-Database/","content":"# 2.6 二级核酸数据库\n\n- [RefSeq](https://www.ncbi.nlm.nih.gov/refseq/) 数据库：**参考序列**数据库，是通过自动及人工精选出的非冗余数据库，包括基因组序列、转录序列和蛋白质序列\n\n  <img src=\"http://img.peterli.club/joy/202210210953987.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n- **dbEST** 数据库：**表达序列标签**数据库，包含来源于不同物种的表达序列标签 (EST)\n\n  <img src=\"http://img.peterli.club/joy/202210210953990.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **Gene** 数据库：为用户提供基因序列注释和检索服务，收录了来自 5300 多个物种的 430 万条基因记录\n\n  <img src=\"http://img.peterli.club/joy/202210210953992.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **非编码 RNA** 数据库：不编码但是起调节作用，[ncRNA databases 汇总](https://zhuanlan.zhihu.com/p/116542550)\n\n<img src=\"http://img.peterli.club/joy/202210210953997.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.5 Genome Database","url":"/2022/2-5-Genome-Database/","content":"\n# 2.5 基因组数据库\n\n# 2.5.1 人基因组数据库：Ensemble\n\n<img src=\"http://img.peterli.club/joy/202210210949854.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- **[Ensemble](http://asia.ensembl.org/index.html)**\n\n  <img src=\"http://img.peterli.club/joy/202210210949860.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - 查看染色体\n\n  <img src=\"http://img.peterli.club/joy/202210210949862.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - DUT基因在15号染色体上\n\n  <img src=\"http://img.peterli.club/joy/202210210949867.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - 选择染色体概要，得到一览图，包括编码蛋白的基因、非编码基因、假基因，分别在染色体上不同区段内的含量，以及里面的红线为GC百分比，黑线为卫星DNA百分比\n\n  <img src=\"http://img.peterli.club/joy/202210210949873.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - DUT基因位于15号染色体条带21.1附近\n\n    <img src=\"http://img.peterli.club/joy/202210210949870.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n    <img src=\"http://img.peterli.club/joy/202210210949409.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - 这是以DUT基因为中心显示的放大图谱，点击DUT基因对应的区域，并在弹出的概况窗口中选择Ensemble数据库的检索号\n\n    <img src=\"http://img.peterli.club/joy/202210210949441.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n  - DUT基因在Ensemble数据库中的详细记录\n\n<img src=\"http://img.peterli.club/joy/202210210949526.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n# 2.5.2 微生物宏基因组数据库：JCVI\n\n- 美国国立卫生研究所 (NIH) 建立了人类微生物组学计划 (Human Microbiome Project，HMP)\n\n  目前 HMP 主要包括了人类鼻腔、口腔、皮肤、胃肠道和泌尿生殖道的宏基因组样本数据和分析流程\n\n  <img src=\"http://img.peterli.club/joy/202210210949577.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n- HMP\n\n  <img src=\"http://img.peterli.club/joy/202210210949674.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  <img src=\"http://img.peterli.club/joy/202210210949822.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - 这是所有HMP中微生物的基因组，这些微生物在人体中存在的位置，测序及注释是已完成还是在分析中\n\n  <img src=\"http://img.peterli.club/joy/202210210949997.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - WGS：全基因组鸟枪法测序项目数据库记录\n\n  <img src=\"http://img.peterli.club/joy/202210210949036.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - SRA：高通量测序数据库\n\n  <img src=\"http://img.peterli.club/joy/202210210949062.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - Annotation：在Genbank中所有注释的链接\n\n  <img src=\"http://img.peterli.club/joy/202210210949088.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n- **[Human Microbiome Project Data Portal](https://portal.hmpdacc.org/)**\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.4 Primary Nucleic Acid Database","url":"/2022/2-4-Primary-Nucleic-Acid-Database/","content":"\n# 2.4 一级核酸数据库\n\n- ****INSDC = Genbank + ENA + DDBJ****\n  1. **[NCBI GenBank](http://www.ncbi.nlm.nih.gov/)**\n  2. **[ENA](http://www.ebi.ac.uk/ena/)** 欧洲核苷酸序列数据集\n  3. [DDBJ](http://www.ddbj.nig.ac.jp/) 日本 DNA 数据库\n\n<img src=\"http://img.peterli.club/joy/202210210947868.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- Genbank，ENA 与 DDBJ 共同构成国际核酸序列数据库合作联盟 (International Nucleotide Sequence Database Collaboration, **INSDC**)。通过 [INSDC](https://link.csdn.net/?target=https%3A%2F%2Fwww.insdc.org%2F)，三大核酸数据库的信息每日相互交换、更新汇总，这使得他们几乎在任何时候都享有相同的数据\n\n  <img src=\"http://img.peterli.club/joy/202210210947460.png\" alt=\"image-20221021094745436\" style=\"zoom: 33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.4.2and3 GenBank:Eukaryotic Nucleic Acid Sequences mRNA and DNA","url":"/2022/2-4-2and3-GenBank-Eukaryotic-Nucleic-Acid-Sequences-mRNA-and-DNA/","content":"\n# 2.4.2&3 GenBank: 真核生物核酸序列mRNA&DNA\n\n# 真核生物核酸序列mRNA\n\n- **Nucleotide**中搜索 dUTPase 的 **成熟 mRNA(剪切掉内含子，只剩外显子)**序列信息 **[U90223](https://www.ncbi.nlm.nih.gov/nuccore/U90223)**\n\n<img src=\"http://img.peterli.club/joy/202210210941304.png\" alt=\"image-20221021094110267\" style=\"zoom: 33%;\" />\n\n- 跟原核生物那个差不多，挑特别的两点来说\n\n<img src=\"http://img.peterli.club/joy/202210210941450.png\" alt=\"image-20221021094132426\" style=\"zoom: 33%;\" />\n\n- 注意看清 CDS 的 note 部分，这里编码的是**线粒体型**的\n\n  <img src=\"http://img.peterli.club/joy/202210210940563.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  \n\n- **CDS**和 **mat_peptide**末尾差了 3 个碱基，因为编码区最后 3 个碱基是终止密码子，不翻译氨基酸\n\n  <img src=\"http://img.peterli.club/joy/202210210940553.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  \n\n  <img src=\"http://img.peterli.club/joy/202210210940550.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n# 真核生物核酸序列DNA\n\n**Nucleotide** 中搜索 dUTPase 的 **基因组 DNA** 序列信息 **[AF018430](https://www.ncbi.nlm.nih.gov/nuccore/AH005568)**。\n\n<img src=\"http://img.peterli.club/joy/202210210940552.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\nLOCUS和ACCESSION可以不相同\n\n<img src=\"http://img.peterli.club/joy/202210210940556.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- **source / map**\n\n  <img src=\"http://img.peterli.club/joy/202210210940565.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n- **gene / mRNA**\n\n  AF018429上的1到1735的碱基连上当前序列的1到1177号碱基，连上AF018431这条序列上的1到45号碱基，以此类推连起来就是一条完整基因\n\n  <img src=\"http://img.peterli.club/joy/202210210940087.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  \n\n- 剪切后形成的 mRNA 有 2 种：\n\n  上面的 mRNA 比下面的在前端多一个外显子，将被翻译成定位线粒体的**信号肽**，从而翻译出**线粒体型（`mitochondrial form`）**蛋白质\n\n  下面没有信号肽的 mRNA 将被翻译成**细胞核型（`nuclear form`）**蛋白质\n\n  <img src=\"http://img.peterli.club/joy/202210210940112.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  \n\n  - 这个表清晰的列出了四个片段中所有外显子的位置，能够清楚的看到线粒体型的比细胞核型的多了一个翻译成信号肽的外显子，其他的翻译成熟肽段的外显子都是一样的\n\n    虽然信号肽最终也会被切掉，但是由此产生了两种亚细胞定位的蛋白质，有信号肽的会到线粒体中去，没有的将留在细胞核\n\n    <img src=\"http://img.peterli.club/joy/202210210940139.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    \n\n- **exon**：当前这个序列所包含的外显子的位置及编号\n\n  <img src=\"http://img.peterli.club/joy/202210210940166.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  560到651号碱基是“DUT”基因的第3号外显子\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.4.1 GenBank Prokaryotic nucleic acid sequences","url":"/2022/2-4-1-GenBank-Prokaryotic-nucleic-acid-sequences/","content":"# 2.4.1 GenBank 原核生物核酸序列\n\n原核生物与真核生物基因的不同：\n\n1. 原核基因组小，真核基因组大\n2. 原核基因密度高，真核基因密度低\n3. 原核编码区含量高，真核编码区含量低\n4. 原核成线性分布(没有内含子)，真核非线性(因为翻译蛋白质的外显子被内含子分隔开来，也就是真核生物的RNA要经历剪切的过程，剪切后成熟的mRNA才能进行翻译)\n\n<img src=\"http://img.peterli.club/joy/202210210916793.png\" alt=\"image-20221021091616772\" style=\"zoom: 50%;\" />\n\n## 例：大肠杆菌 **Nucleotide** 中搜索 **[X01714](https://www.ncbi.nlm.nih.gov/nuccore/X01714)**\n\n<img src=\"http://img.peterli.club/joy/202210210916950.png\" alt=\"image-20221021091649929\" style=\"zoom: 50%;\" />\n\n    `<img src=\"http://img.peterli.club/joy/202210210921506.png\" style=\"zoom:50%;\" />`\n\n**LOCUS：基因名**\n\n`<img src=\"/Users/joyce/Library/Application Support/typora-user-images/image-20221021092713118.png\" alt=\"image-20221021092713118\" style=\"zoom:50%;\" />`\n\n- **DEFINTION**\n\n<img src=\"http://img.peterli.club/joy/202210210928636.png\" alt=\"image-20221021092824616\" style=\"zoom:50%;\" />\n\n- **ACCESSION VERSION**：数据库里的编号，检索号，唯一不变\n\n<img src=\"http://img.peterli.club/joy/202210210929029.png\" alt=\"image-20221021092906003\" style=\"zoom:50%;\" />\n\n- **KEYWORD**\n\n<img src=\"http://img.peterli.club/joy/202210210929240.png\" alt=\"image-20221021092927213\" style=\"zoom:50%;\" />\n\n- **SOURCE ORGANISM**\n\n<img src=\"http://img.peterli.club/joy/202210210929164.png\" alt=\"image-20221021092945134\" style=\"zoom:50%;\" />\n\n- **REFERENCE**\n\n<img src=\"http://img.peterli.club/joy/202210210930182.png\" alt=\"image-20221021093002156\" style=\"zoom:50%;\" />\n\n- **COMMENT**\n\n<img src=\"http://img.peterli.club/joy/202210210931101.png\" alt=\"image-20221021093109073\" style=\"zoom:50%;\" />\n\n- **FEATURES**：描述核酸序列中各个已确定的片段区域，包含很多子条目，比如来源 (source)，启动子 (promoter) 等\n\n  <img src=\"http://img.peterli.club/joy/202210210931922.png\" alt=\"image-20221021093129890\" style=\"zoom:50%;\" />\n\n  - **source：**说明了核酸序列的来源，据此可以容易的分辨出该序列是来源于**克隆载体**还是**基因组**。当前序列（全长）来源于大肠杆菌的基因组 DNA。\n\n  <img src=\"http://img.peterli.club/joy/202210210931086.png\" alt=\"image-20221021093153056\" style=\"zoom: 33%;\" />\n\n  - **promoter**：列出了启动子的位置。细菌有两个启动子区，一个 **-35 区 (5’-TTGACA-3’)** 位置在第 286 个碱基到第 291 个碱基，一个 **-10 区 (5’-TATAAT-3’)** 位置在第 310 个碱基到第 316 个碱基。\n\n  <img src=\"http://img.peterli.club/joy/202210210932624.png\" alt=\"image-20221021093224593\" style=\"zoom: 33%;\" />\n\n  - **misc_feature**\n\n  <img src=\"http://img.peterli.club/joy/202210210932792.png\" alt=\"image-20221021093250757\" style=\"zoom: 33%;\" />\n\n  - **RBS**\n\n  <img src=\"http://img.peterli.club/joy/202210210933655.png\" alt=\"image-20221021093315623\" style=\"zoom: 33%;\" />\n\n  - **CDS**(Coding Segment) : 记录了一个 **ORF**(open reading frame),从第 343 个碱基开始的 ATG(起始密码子)到第 798 个碱基结束的TAA (结束密码子)。除了第一行的位置信息，还包括翻译产物(蛋白质)的诸多信息。\n\n    <img src=\"http://img.peterli.club/joy/202210210933291.png\" alt=\"image-20221021093346250\" style=\"zoom:50%;\" />\n\n    `<img src=\"http://img.peterli.club/joy/202210210934758.png\" alt=\"image-20221021093439739\" style=\"zoom:50%;\" />`\n- **ORIGIN**\n\n  <img src=\"http://img.peterli.club/joy/202210210935305.png\" alt=\"image-20221021093505266\" style=\"zoom: 33%;\" />\n\n## **FASTA 格式**：\n\n- 第一行，**大于号**加名称或其它注释\n- 第二行以后：序列，每行**60**个字母(早已被打破，80，100都可以\n\n<img src=\"http://img.peterli.club/joy/202210210935996.png\" alt=\"image-20221021093520972\" style=\"zoom:50%;\" />\n\n## **Graphics**：获得序列的图形概览\n\n<img src=\"http://img.peterli.club/joy/202210210935253.png\" alt=\"image-20221021093536225\" style=\"zoom:50%;\" />\n\n## **下载纯文本格式 (Flat File) 的数据库记录**\n\n<img src=\"http://img.peterli.club/joy/202210210935967.png\" alt=\"image-20221021093549933\" style=\"zoom:50%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.3 PubMed","url":"/2022/2-3-PubMed/","content":"\n# 2.3 文献数据库PubMed\n\n# 引言\n\n<img src=\"http://img.peterli.club/joy/202210210904022.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n# PubMed是啥？\n\n<img src=\"http://img.peterli.club/joy/202210210905856.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- MedLine —— [PubMed](https://pubmed.ncbi.nlm.nih.gov/) 文献记录的内部结构\n\n<img src=\"http://img.peterli.club/joy/202210210901149.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- 按照不同规则搜索Down这个词\n  1. 按**作者名 AU** 搜索：Down [AU]\n  2. 按**标题 TI** 搜索：Down [TI]\n  3. 按实验室**地址** AD 搜索：Down [AD]\n  4. 在任意地方搜索：Down\n\n- **关于使用 PubMed 的几个小建议：**\n  使用引号（“down syndrome” )\n  使用逻辑词 AND, OR, NOT（dUTPase [TI] AND bacteria [TI] NOT Smith [AU])\n  使用正确的名字缩写 (“Abergel C”)\n  使用每篇文献唯一的 PubMed ID（PMID: 24933525)\n\n<img src=\"http://img.peterli.club/joy/202210210901144.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.1and2 BioDatabase","url":"/2022/2-1and2-BioDatabase/","content":"\n# 2.1 为什么需要生物数据库&2.2 生物数据库分类\n\n什么时候该用什么数据库，如何在数据库中查找想要的信息，以及如何解读这些信息\n\n<img src=\"http://img.peterli.club/joy/202210210857261.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n超过1000个物种通过测序技术获得了全部基因组序列\n\n<img src=\"http://img.peterli.club/joy/202210210857273.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210210857268.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- 核酸数据库：与核酸有关的数据库\n\n- 蛋白质数据库：与蛋白质有关的数据库\n\n- 专用数据库：针对某一主题的数据库或综合性的数据库以及无法归入其他两类的数据库\n\n- 一级数据库：存储的是通过各种科学手段得到的最直接的基础数据\n\n  比如测序获得的核酸序列，或者X射线衍射法获得的蛋白质三维结构\n\n- 二级数据库：通过对一级数据库的资源进行分析、整理、归纳、注释而构建的具有特殊生物学意义和专门用途的数据库\n\n  比如从三大核酸数据库和基因组数据库中提取并加工的果蝇和蠕虫数据库，或者根据蛋白质三级结构数据库中的结构信息分析统计出的蛋白质结构分类数据库CATH、SCOP等\n\n<img src=\"http://img.peterli.club/joy/202210210857286.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"1 SDU Introduction","url":"/2022/1-SDU-Introduction/","content":"# 1. 简介\n\n<img src=\"http://img.peterli.club/joy/202210210852196.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n# 生物数据库\n\n<img src=\"http://img.peterli.club/joy/202210210852208.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 序列比较\n\n<img src=\"http://img.peterli.club/joy/202210210852250.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 分子进化系统发生\n\n<img src=\"http://img.peterli.club/joy/202210210852220.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 蛋白质结构预测与分析\n\n<img src=\"http://img.peterli.club/joy/202210210852214.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210210852241.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 基因组学 蛋白组学\n\n<img src=\"http://img.peterli.club/joy/202210210852371.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 序列算法\n\n<img src=\"http://img.peterli.club/joy/202210210852795.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n# 统计基础\n\n<img src=\"http://img.peterli.club/joy/202210210852908.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n# 数据挖掘\n\n<img src=\"http://img.peterli.club/joy/202210210852633.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210210853959.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"2.5 Processivity and 2.6 DNA Polymerase  Processivity Assay","url":"/2022/2-5-Processivity-and-2-6-DNA-Polymerase-Processivity-Assay/","content":"# 2.5 Processivity & 2.6 DNA Polymerase  Processivity Assay\n\n# Processivity\n\n DNA聚合酶的另一性质，这个性质叫做持续合成能力，它类似于描述聚合酶工作效率的意思，更精确的定义\n\n- 普遍定义\n\n  持续合成能力与作用于聚合物或者合成聚合物的反应有关，通常简单地定义为与每个聚合物分子结合的酶的反应循环数，所以这是作用于聚合物的酶的一种性质\n- 对于DNA聚合酶的定义\n\n  即在每次酶结合引物-模板接头时，加入的dNTP的个数\n\n  为什么要关心这个？\n\n  - 在溶液中DNA聚合酶结合引物-模板接头需要约一秒钟，这是扩散控制反应\n  - 与之相反，一旦结合后DNA聚合酶只需约一毫秒来添加dNTP\n  - 这意味着如果你有一个已经与模板结合的DNA聚合酶，在另一个DNA聚合酶与新模板结合期间，它能向模板添加1000个碱基对\n  - 显然，如果你想要合成大量DNA，你需要一个能有效持续合成的DNA聚合酶\n\n  事实上，DNA聚合酶在持续合成能力方面千差万别，它的差别基于聚合酶的功能\n\n  比如说，DNA修复聚合酶的持续合成能力很低，它通常只需要修复非常短的DNA片段，因此它们的持续合成能力为每次结合引物-模板接头时，合成10到20个碱基对，每次结合模板时，10到20个碱基对\n\n  相反，DNA复制或基因组DNA复制时，聚合酶具有不可估量的持续合成能力，我们那轻易合成的最大人工模板链的长度为50000碱基对，它们的持续合成能力在每次结合引物-模板接头时，高于50000碱基对\n\n# DNA Polymerase  Processivity Assay\n\n## 1. 模板竞争分析(template challenge assay)\n\n- Make Two PTJ ——> 1 is Labeled On Primer ($P^*TJ$)，1 is Not Labeled($PTJ$)\n\n  制备两种引物-模板接头，其中一种的引物经过标记，另一种的引物没有任何标记，我们需要这两种不同的引物-模板接头\n\n  - 在标记过的引物-模板接头里\n\n    1. Add 2x Excess Of DNA Polymerase Over $P^*TJ$——加入超过$P^*TJ$两倍的DNA聚合酶\n\n       如果有2皮摩尔的$P^*TJ$，我们就加入4皮摩尔的聚合酶\n    2. Add Unlabeled dNTP + 1000x PTJ(unlabeled)\n\n       确保反应所需的缓冲液和其他条件都没问题，然后我们就在这些材料中加入未标记的dNTP来启动反应，同时还加入1000倍的未标记的引物-模板接头\n    3. Allow Run To Proceed Long Enough To SyNT\n\n       让反应时间足够长，足以合成完整的模板，或者应该说是一个全长产物\n\n       在我们的例子里，这里指酶与模板结合一次以后，合成5000个碱基对，如果用之前讲过的超快的DNA聚合酶，每秒钟能合成1000个碱基对，这个反应只需要进行5秒\n\n       一般来说，酶不会有那么高效，所以我们可能要给他30秒来完成这个反应\n    4. Stop Reaction And Separate Product On Denat Gel\n\n       终止反应，并在变性凝胶(denaturing gel)中分离产物\n\n       - 得到如下结果，引物、高持续合成力的、低持续合成力的\n\n         1. 如果没有聚合酶，那就没有信号\n         2. 假设是5000碱基对片段的电泳位置，就会在5000碱基对的位置看到一个条带\n\n            如果有持续合成五万或五万多碱基对的高级聚合酶，我们就合成完整的产物\n\n            如果用了持续合成力低的酶，在40-45碱基对的地方可能会看到产物一片模糊(因为低效，没合成结束，就被终止了，合成的都是小片段分子，所以条带是散开的)\n         3. 阴性对照中，引物会在最左下角，一点都不会扩增，因为它是标记了的\n\n         <img src=\"http://img.peterli.club/joy/202210201435930.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n       - 为什么这个反应可以让我们只关注酶的结合，只关注第一次结合之后会发生什么？\n\n         一开始所有标记过的引物-模板接头都结合了聚合酶，我们加入核苷酸启动反应，同时我们还加入了完全相同但未标记的引物-模板接头，加入的量是聚合酶的1000倍，聚合酶不会区分模板上是否连接了荧光或放射性标记物，所以他就开始合成了，然后根据酶的特性，它会以一定速率脱落\n\n         如果是一个持续合成力超强的酶，可能直到模板终点都不会脱落\n\n         如果是一个持续合成力弱的酶，它的持续合成力是20到25个碱基对，到了这个长度，他就从模板上脱落，它得花一秒钟时间去找一个新模板，一旦从模板上脱落，它就不一定只会重新结合原来的模板，还有同样的可能结合任何其他模板\n\n         它再次结合一个放射性标记模板的几率有多大？\n\n         千分之一，因此一般来说，它会结合一个未标记的模板，在那些未标记的模板上，它能愉快地继续合成更多DNA，我们在分析结果中看不到，我们只能在首次结合的模板时看到它，与仅有标记过的模板时的信号相比，重新结合标记过的模板产生的信号仅占千分之一，那就基本是0\n\n       由于加入了这些竞争性模板，这个实验被称为模板竞争分析，我们也因此只测量第一次结合和合成的情况，一旦它掉落到充满未标记模板的国度，我们就不管他了，所以这个方法非常快捷简单测量DNA聚合酶的持续合成力\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"2.4 Primer Extension Assay","url":"/2022/2-4-Primer-Extension-Assay/","content":"# 2.4 Primer Extension Assay\n\n希望无论是10碱基对还是1000碱基对的产物，他们的标记量都相同，这就要用到一个不同的分析实验\n\n这跟掺入法不同，这叫引物延伸实验，与掺入法不同的是，被标记的是底物，我们在引物中掺入荧光基团或放射性标记的核苷酸，这个引物从5’端到3‘羟基端和先前的引物都一样，因此合成了一个标记过的引物-模板接头\n\n<img src=\"http://img.peterli.club/joy/202210201419897.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- Make A Labeled PTJ (label on primer)\n\n  Perform Run Same Way\n\n  标记通常必须位于引物上，因为合成完成后，引物就成为产物的一部分，而模板一旦分离，就不再属于产物了，反应完全是一样的\n- Do Not Label dNTPs\n\n  我们不标记dNTP，我们改变了标记方式，现在是在引物上，dNTP全是未被标记的\n- Can’t Analyze By Filter Binding\n\n  那么你运行反应，你不能用滤纸结合分析来检测\n\n  如果反应中完全没有DNA聚合酶，会有什么结果？\n\n  滤纸上会有大量放射性标记的DNA，如果你完成了整个产物的合成反应，也会得到大量产物，因为起始产物和终产物被标记的数量基本相同\n- Can By Gel Separation\n\n  能用凝胶分离法来检测，如果用溴化乙锭，结果看上去几乎和先前的结果是一样的，因为我们进行的就是相同的反应，但是如果用放射性同位素或者荧光法，会看到产物越来越长，但强度是一致的，因为无论是100碱基、1000碱基还是5000碱基，它们都连接着同样标记的引物，没有新的标记掺入\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"2.2and2.3 Filter Binding and Electrophoresis Incorporation Assay","url":"/2022/2-2and2-3-Filter-Binding-and-Electrophoresis-Incorporation-Assay/","content":"# 2.2 & 2.3 Filter Binding Incorporation Assay & Electrophoresis Incorporation Assay\n\n# How To Separate DNA From dNTPs\n\n## 1.  Filter Binding Assay —— 滤纸结合分析\n\n最老套但也可能是最容易定量的一个\n\n### — Positivecy Charged Filter Paper(DE-81)\n\n总的来说，你要用一张带正电荷的滤纸，一般来说是一种叫做DE-81的滤纸或者其他种类的滤纸\n\n### — Spot Run On Paper\n\n把反应物点样到纸上，好处是能使这个反应速战速决，因为他会很快变干，干掉的反应是无法进行下去的，继续反应需要水\n\n### — Wash With Buffer That Has Salt At Level That Releases dNTPs But Not DNA\n\n然后就能拿着这张膜，用含盐的缓冲液漂洗，盐浓度应该能够洗脱dNTP，而不能洗脱DNA\n\n为什么它能分清楚这两者呢？\n\n- 因为平均每个核苷酸上有3个负电荷\n- 虽然通常在短时间内，一个磷酸有可能会多带一个负电荷，但一般来说，平均每个磷酸只带一个负电荷，所以一个dNTP有三个负电荷\n- 那么这里的DNA底物有多少负电荷？\n- 即使没有发生合成，它的负电荷也多于5000个，因为它上面的每一个碱基都有一个负电荷，那么DNA就会有比如说几千\n\n下面来论证一下\n\n- 要想使一个分子从滤纸上洗脱，必须使它所有的负电荷同时从滤纸上脱落，盐(离子)能做到这一点，因为当它趋近并结合(DNA)时，它会取代滤纸上的正电荷\n- 比如说，我们用氯化钠，钠离子会过来并占据这个位置，但它一次只能占那么多，这取决于不同的盐浓度，但这一般都发生得很快，当只有三个负电荷时，比如说每两到三毫秒其中一个电荷会脱落，总体上这对于所有的电荷都是差不多的，这意味着比如说大约每过五秒，这三个电荷会在某一时刻被同时释放，但是5000个电荷会被同时释放的可能性几乎是0，所以你只需要漂洗它们\n- 一般来说洗三次，通常会用放射性同位素标记来进行这个实验，这样就可以取上清液，可以检查上清液中能否检出放射性，然后就一直重复这个过程，直到滤纸的洗脱液中检测不到放射性为止，然后你只需取出滤纸，然后测定滤纸上吸附的标记物的含量\n- 如果是荧光标记，测起来可能没那么简单，但是原理是一样的，洗三次就完成了。如果选对盐浓度DNA就从不易洗脱变成了无法洗脱，一般洗脱dNTP会用50毫摩尔的NaCl，它立刻就能掉下来，而DNA在这样的条件下永远无法被洗脱，你可能需要2-3摩尔的盐才有可能洗脱DNA之类\n\n### —  Count Or Measure Fluorescence Or Radioactivity Associate\n\n你就能取出这张膜，然后计数或者说测量它上面吸附的荧光或放射性强度，接着就能计算出掺入的核苷酸的含量，可以精确到飞摩尔(femtomole)甚至阿摩尔(attomole)\n\n如果你准确地知道实验最初用的核苷酸上携带的荧光或放射性标记物的强度，那么你就能很精确地测量出有多少核苷酸掺入到了你的模板上，这个方法的优势就在于迅速，整个过程大约八分钟就可以完成了\n\n- 优点 —— Fast, Quantitative\n\n  如果你排除了液体闪烁计数法(scintillation counting)或者测量荧光强度的某方法所需的时间，他快的简直不可思议，每次漂洗只需30秒，然后使其干燥，然后进行液体闪烁计数，这个方法也是定量的\n\n  而且当你分析在多长的时间内向多少模板中掺入多少核苷酸这类问题时，这几乎是定量准确性最高的一种方法了\n- 缺点 —— No Information About Length\n\n  它无法提供任何有关长度的信息，无论是1000个10碱基对的延伸，还是一次10000碱基对的延伸，在这个分析实验中都会得到完全相同的结果，用这个分析实验无法区分这两者，你只能测出掺入了多少核苷酸，却不能确定产物到底是什么样的\n\n所以这个分析实验一般只用于分级分离(fractionating)时，你只想找出聚合酶在哪个组分中，因为它真的很快捷，比如，当你从某细胞的粗提物中获得100份不同的柱层析组分时，你用这个分析实验就能迅速地判断出这就是含有DNA聚合酶活性的那个组分\n\n## 2.  Gel Electrophoresis\n\n很多时候我们想知道片段有多长，如果你想知道这些产物多长，你怎么把它们分开呢？\n\n用凝胶来分开。这种情况下，我们进行完全一样的反应，但在反应最后我们要分离产物\n\n- Separate products on A Gel(Denaturing)\n\n  如果DNA变性呢？如果我迫使新生链从5000碱基对的环状产物中脱离呢？我是不是就能更好地测量距离或者长度了？\n\n  这就是凝胶的关键，变性。你怎样使它变性呢？\n\n  - Heat + NaOH(Agarose) or Urea(Polyacrylamide)\n\n    通常是95度，如果是用琼脂糖凝胶，通常会需要氢氧化钠，如果用聚丙烯酰胺，就要加尿素\n\n    这些试剂的作用是，一开始它们并不能完全导致产物分离，这就需要加热的作用，它们能非常有效的阻止分离的DNA分子再次退火\n\n    一旦把它们热变性了，如果它们处在碱或者尿素中，它们就无法恢复结合状态，也不能再次退火，然后你就用凝胶上分离它们，结果会是怎么样的呢？\n\n    如果你只是用溴化乙锭(ethidium bromide)染胶（能染所有的核酸），无论它们是否被标记，这里显示的是离心管里所有的DNA。在反应之前加的引物，是最下面这个25碱基对的短条带，上面第二行是5000碱基对的模板，它们已经分离，所以它们的电泳位置完全不同。它们从上往下迁移，下面是正极，上面就是负极，较大的分子移动得慢，较小的分子移动得快\n\n    合成开始以后，核苷酸会掺入到新的DNA链中，这就会慢慢分开，30秒时你会看见一些合成的产物，60秒时则更多，90秒，120秒，它会稳定地延伸这些初始引物的长度\n\n    从这张图上能看见全部(DNA)，这也是一个非常高效的反应，通常这个反应不会用掉所有的模板，可以看到这里大概三分之二的模板被用掉了，这可以用溴化乙锭染色来检测\n\n    但是如果我们只检测放射性或者荧光，我们就只能看见延伸中的DNA分子，看不见引物，反应初始的引物并没有被标记，看不见模板，初始的模板也没被标记，只能看见新的产物\n\n    <img src=\"http://img.peterli.club/joy/202210201416772.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  很明显，与滤纸结合分析法相比，凝胶电泳给我们提供了很多新的信息\n\n  - 优点 —— Length Information\n\n    告诉我们关于产物长度的信息，如果我们只合成了100碱基对的产物，如果这个聚合酶再也无法合成更长的产物，我们看到的图像就会与这里显示的持续合成的图像完全不同\n  - 缺点 —— Slower、Less Quantitative\n\n    它更慢，不管是丙烯酰胺还是琼脂糖凝胶，根据胶的种类，至少需要20分钟，最长几个小时，定量效果也较差，测量胶上的带比测量掺入到模板中的放射性的总量要难多了\n\n    比如说，通过测量放射性你能得到非常精确的数值，但这(电泳条带的定量)只是大概的估计，只能给你一些idea\n\n  当产物越来越长，信号的强度也越来越强，因为越来越多标记的核苷酸掺入到产物里\n\n当你希望越来越深入地鉴定酶的特征时，你会想要同时用两种方法\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"2.1 Incorporation Assay","url":"/2022/2-1-Incorporation-Assay/","content":"# 2.1 DNA复制检测——Incorporation Assay\n\n你如何分辨给你的澄清溶液？\n\n因为大部分酶类都是无色的，所以生物学家们经常要和澄清溶液打交道，少数和离子、绿色荧光蛋白(GTP)或其它荧光蛋白结合的有颜色，但对于大部分酶而言，如果是纯溶液，它就是澄清的，更重要的是，除非上面有标签，不然你不会得到任何提示那是什么，大多数情况下，当你试图了解某个复杂反应，是不会有相关的标签出现的。你只有一些试管，不知道内容物是否有酶活性，那么怎样检测DNA的合成呢？\n\n- Assay For DNA Polym Activity —— DNA聚合酶活性的检测方法\n\n  1. Simple Incorporation Assay\n\n     - 应用\n\n       基本上这种方法可以用于任何聚合物，不仅可以用于生物方面，还可以用在任何其它聚合物合成方面，可以用这种方法检测聚合物的形成和合成情况\n     - 要求——Need To Be Able To Separate Polymer(DNA) From Its Building Blocks(dNTP)\n\n       需要你能把聚合物从原料中分离出来，这个例子聚合物就是DNA\n\n       尽管我们在讨论DNA复制时首先提到这个方法，但这个方法同样可用于转录、蛋白质合成方面，以及其它你能想到的各种形式的聚合物合成\n\n       比如用在检验糖原合成，糖原合成也是一种聚合物合成反应\n     - How to do it?\n\n       1. Create A PTJ Substrate —— 要合成具有引物-模板接头的底物\n\n          首先要有一个primer:template junction(引物-模板接头)，因为任何DNA合成反应都需要这个，要合成具有引物-模板接头的底物。\n\n          在DNA合成反应中，人们一般会使用噬菌体M13，他有一个大的单链DNA基因组，你能合成这些环状单链DNA，我们知道噬菌体基因组的序列，这样你就能合成它了\n\n          对于M13来说，它的DNA环有5000碱基，你可以合成一段与之不成比例的引物，通常引物有20-25个碱基对，这样我们就有了引物-模板接头。箭头一端与DNA聚合酶扩增的方向一致，总是3’OH，另一端则是5’磷酸基\n\n          怎么获得引物呢？\n\n          如今只要把引物输入电脑发送给公司，他们会为你合成引物，第二天早上就可以送到你手里，无论想要什么序列，只要长度小于50个碱基对，第二天就能拿到\n\n          <img src=\"http://img.peterli.club/joy/202210201412523.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n          有了引物-模板接头，把引物和单链DNA放在一起，合成底物，用加热板将其加热到100度，大约需要3分钟左右，然后取出加热板，然后去吃个午饭，回来就完成退火了\n       2. Add Buffer + DNA Polym\n\n          加入反应所需的缓冲液，对于大多数聚合酶而言，缓冲液通常是中性的，常含有50-100毫摩尔的盐，以及醋酸镁或氯化镁或其它的含镁溶液，通常需要大约1-2毫摩尔\n\n          这样有了合适的缓冲环境，再加上DNA聚合酶\n       3. Start Run By Adding dNTPs\n\n          — Atleast One dNTP Must Be Label\n\n          — Fluorescent or Radioactive Label 荧光标记或放射性同位素标记\n\n          加上dNTP开始反应，这个检测的关键是至少要有一种被标记的dNTP，通常可以是荧光标记或放射性同位素标记\n\n          荧光标记几乎总是加在一个胸苷类似物上，因此它是加在碱基上，实际上，只要胸苷上的甲基在那里，你就可以在甲基上加上几乎任何你想放的东西，加的东西可以很大，可以在甲基末端连上一个类似七环的结构\n\n          <img src=\"http://img.peterli.club/joy/202210201412522.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n          放射性同位素标记，可以用氚替换碱基上的氢，或者用P32替换其中一种磷酸基，修饰$\\alpha$，因为 $\\beta或\\gamma$不是产物的一部分\n\n          <img src=\"http://img.peterli.club/joy/202210201412520.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n       4. Take Time Points And At Each Time Points Separate(Stop Run With EDTA or SDS) DNA From Labeled dNTPs\n\n          反应继续，因为所有的组分都在里面，一般来说，你能取时间点，在每个时间点通常就加入SDS或其他物质来终止反应或者可以加入EDTA，你可以终止反应\n\n          在每个时间点，你要把DNA从标记过的dNTP中分离出来\n\n          如果合成反应成功进行，一部分标记过的dNTP就会掺入DNA聚合物中，所以要用一个有效的方法分离DNA和那些标记过的dNTP，然后在那个时间点，我们只需测量DNA中有多少被标记的dNTP，[测量方法](2%202%20&%202%203%20Filter%20Binding%20Incorporation%20Assay%20&%20Ele%20353e9c26fc3641e78d1958136c0b1f4a.md)\n\n          如果假设我们正在观察这个底物，大部分或所有DNA一开始都没有被标记，接着当我们合成新DNA的时候，新DNA被标记了，只需要找到方法分离那些粉红色DNA和漂浮在周围的粉红色小核苷酸，这样的方法其实有很多种，至少有几种\n\n          <img src=\"http://img.peterli.club/joy/202210201412527.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.8and9 Mistakes of DNA Polymerase","url":"/2022/1-8and9-Mistakes-of-DNA-Polymerase/","content":"# 1.8&1.9 DNA聚合酶发生的错误及修复——Mistakes of DNA Polymerase\n\n犯错的频率是多少？为什么会犯错？\n\n一个DNA聚合酶自身，大约每合成十万对碱基会发生一个错误\n\n为什么会发生这么多错误？\n\n但是其实大部分都不是DNA聚合酶本身的错，大多数错误都是嘧啶替嘧啶，或者嘌呤代嘌呤的错误\n\n- Due Tautomer Formation\n\n  这是由于互变异构体(Tautomer)的形成，展示两种不同的互变异构体，所有的四种碱基都能够在酮(keto)和烯醇(enol)、氨基(amino)和亚氨基(imino)式中相互转换\n\n  <img src=\"http://img.peterli.club/joy/202210201403595.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  拿胞嘧啶(Cytosine)来说，通常它处于氨基(amino)的状态，但是在很罕见的情况下，它会变成亚氨基(imino)的状态，要注意的是，当它转变时，它的氢键模式也改变了，上面有受体、受体、供体，但转变后就突然变成了受体、供体、受体，这会改变它配对的碱基\n\n  <img src=\"http://img.peterli.club/joy/202210201403608.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  酮式(keto)变成烯醇式(enol)，从供体、供体、受体变为供体、受体、受体。它的作用就是，当这种情况发生时，鸟嘌呤(Guanosine)正常的状态，这是它大多数时间保持的状态，是酮式(keto)状态，在它的酮态，它会完成自己的使命，它通过供体、供体、受体的位置关系和胞嘧啶配对\n\n  但基本上每十万个碱基中就有一个变化，它会罕见地转变为另一种构象，变为它的烯醇(enol)状态，它能与胸腺嘧啶(Thymine)紧密地连接\n\n  那么聚合酶做的工作就是，它通过接触碱基来弄清楚是否掺入到正确的位置，我还是得到了沃森克里克碱基配对，因为如果这里是腺嘌呤的话，结构也是一模一样的，当碱基变成另一种构象时，聚合酶无法区分出来\n\n  它很高兴这里加入了一个胸腺嘧啶，不幸的是，一旦你放进一个胸腺嘧啶，鸟嘌呤就不再是烯醇态了，它实际上非常快速地转变回酮态。酶会说“噢 糟糕”，因为当它移动到下一个位置的时候，那个刚加进的碱基现在是错误的了，所以DNA不再会变成双链状态，它的末端会散开，它不再会形成小沟里面的相互作用方式，这会使聚合酶停止工作\n\n  <img src=\"http://img.peterli.club/joy/202210201403607.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  考虑这个3‘羟基的位置，刚好能和这个$\\alpha$磷酸基团相互作用，如果这个碱基对错误了，它们两个就会相互排斥，这大概就是氢键不再互补时发生的情况，那个羟基不会出现在$\\alpha$磷酸基团需要的位置了\n\n  <img src=\"http://img.peterli.club/joy/202210201403613.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  庆幸的是，一旦出现了错误，聚合酶会停下来并思考一下，互变异构体导致了错配，是暂时的，但是错配会减慢随后的合成，这会给聚合酶一点时间来改正自己，原来它使用校读型核酸外切酶(proofreading exonuclease)来修正，可以想象成聚合酶上的退格键或是删除键\n\n# proofreading exonuclease\n\n## 校读型核酸外切酶是怎样工作的呢？\n\n反应的示意图\n\n<img src=\"http://img.peterli.club/joy/202210201403602.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n这是否只是合成的逆反应？因为你可能会认为 它是这样工作的，你只需让反应逆向进行 对不对？为什么不能逆向进行？脱落下来的是脱氧核苷单磷酸，如果是逆反应，脱落下来的是脱氧核苷三磷酸，反应式顶端会有两个无机磷酸，如果幸运可能能找到一个焦磷酸，所以这不是逆反应\n\n因为一旦你将那两个磷酸分开，合成反应就变得不可逆，酶其实可以聪明点，让一个焦磷酸留在附近一会儿，等着看合成的产物是不是正确，但是由于之前讨论过的原因，这实际上相当困难\n\nproofreading exonuclease remove mismatches\n\n- exonuclease degrades DNA from one end\n\n  首先，一个核酸外切酶可以从一端删除，或者说降解DNA。\n\n  核酸外切酶的替代物是什么？hint: 在做克隆时，用什么来切DNA? 哪一种核酸酶？\n\n  核酸内切酶。核酸内切酶内部切开DNA片段，而核酸外切酶需要一个末端才能工作，一般来说，只会从末端开始，咬掉一点DNA\n- proofreading exonuclease acts 3’-5’ direction\n\n  校读型核酸外切酶沿3‘到5‘的方向工作，在这种情况下，我们无需担心别人不知道我们在说哪个方向，因为唯一可以供酶作用的就是一条DNA和它的3‘端，这段DNA处在双链DNA的哪里并不要紧，只要有一个3‘端，这些核酸外切酶就能降解它\n\n  这是一个3‘到5’方向的核酸外切酶，是与合成方向相反的，它从3‘端开始降解，而不是像DNA聚合酶一样从3‘端开始延伸\n- typically part of the same Polypeptide as Polymerase\n\n  当出现错配时，反应会变慢，除此之外，DNA末端就已经变得不稳定了，至少最后一个碱基对没能互补配对\n\n  通常外切酶活性与聚合酶活性都位于同一条多肽链上，因此，只要有校读型外切核酸酶活性，它就几乎总是属于一个双酶多肽，也就是说，这条多肽链一部分具有合成活性，而另一部分具有核酸外切酶活性\n- frayed/non BP end of DNA has low affinity for  polymerase active site, single-stranded DNA end of primer - 3'OH end has high affinity for exonuclease active site\n\n  不稳定的或错配的DNA与聚合酶活性位点的亲和力很弱，但它有引物的单链DNA末端或3‘端羟基，于是与核酸外切酶的活性位点有高亲和力\n\n具体示意图\n\n这里有一个错配，它的末端散开了，对于聚合酶来说，这不是一个好底物，这段单链区域会从活性位点脱落下来，然后移动到手掌的根部，通常这里就是核酸外切酶的活性位点\n\n一般来说，酶会去掉一至三个核苷酸，所以往往不仅移除了错配的碱基，也会多去掉几个核苷酸，这是没问题的，因为你会情愿把几个正确的核苷酸和错误的一起移除，然后重新合成，补上完全正确的，这样你只需关心有多少核苷酸需要重新合成\n\n生物倾向于多付出一些来保证正确，而不是偷工减料而造成失误，付出一些努力来保证输出的正确是很值得的。\n\n<img src=\"http://img.peterli.club/joy/202210201403600.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201403905.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201403934.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n把它切掉后，这条单链DNA会重新退火(reanneal)，正确的结合整个模板，然后会将3’端羟基移动(shift)到合适的位置，以便继续合成\n\n因此，这个不稳定的末端，对核酸外切酶来说是个不错的底物，一旦将它移除，就有了碱基完全互补配对的双链区域，将会很快退火，然后就会回到正常的合成反应\n\n<img src=\"http://img.peterli.club/joy/202210201403960.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201403986.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201403028.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n## 修复的作用有多大\n\n- proofreading exonuclease increases accuracy to 1 mistakes $10^7 /BP$\n\n  聚合酶中增加了校读型核酸外切酶活性，会使精确度提高100倍，所以每合成$10^7$个碱基对才会出一次错，现在距离最后的数值，我们只差3个数量级了。\n\n在第五、六、七讲，我们将讨论DNA修复机制的工作原理，它能让你从$10^7$上升到$10^{10}$，尤其是错配修复(mismatch repair)\n\n如果由于某种原因造成一个错配，但合成继续进行，不知为什么没有足够长的时间停下来，让校读型核酸外切酶工作，如果这段DNA某处最终形成了一个错配，一旦聚合酶延伸到这里，完全无法辨识出错配，只会继续合成下去\n\n但是有一整套完全不同的机制，在基因组中在巡回并且寻找错配，事实上，错配就这样被识别出来，这就是这种修复方式的特别之处，它会找到那个错误的碱基，识别它并且把它移除，因为想象一下，一旦出现了错配，你没法想当然地知道，当一个C和一个T配对时，最初正确的是C还是T，细胞却有办法知道，我们会讲到它的工作机理的\n\n<img src=\"http://img.peterli.club/joy/202210201403084.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.6&7 The Structure of DNA Polymerase","url":"/2022/1-6and7-The-Structure-of-DNA-Polymerase/","content":"# 1.6&1.7 DNA聚合酶的结构——The Structure of DNA Polymerase\n\nDNA聚合酶有很多种，大多数情况下，它们有着相同的基本结构，有些晶体学家叫它们“手”\n\n<img src=\"http://img.peterli.club/joy/202210201347160.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n## Palm: 聚合酶结合引物-模板接头(PTJ)以及部分新合成的DNA\n\n对所有聚合酶来说，这个结构有个有趣的特性，活性位点后面就是模板，黄色圈圈的是没配对的碱基，后面剩余的单链DNA呈$45度$弯曲，这很重要，因为那意味着活性位点附近唯一能配对的核苷酸，是紧接着前面配对完的那个，正与手掌发生着相互作用，手掌被认为是右边黄色大圈圈的区域\n\n<img src=\"http://img.peterli.club/joy/202210201347094.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n蛋白质如何结合所有的DNA？\n\n- Binds Phosphate Backbone\n\n  它与固定且无关序列的磷酸骨架结合，如果我说主要作用不是在骨架上呢？那它会在哪里结合？\n\n  它会与大沟(major groove)结合。大沟里的氢键供体和受体有很多有趣的类型，它们的区别很大\n\n  A：氢键受体                 D：氢键供体                 M：甲基                 H：非极性氢\n\n  受体-受体-供体-氢(AADH)，氢是非极性的\n\n  <img src=\"http://img.peterli.club/joy/202210201346552.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  氢-供体-受体-受体(HDAA)\n\n  <img src=\"http://img.peterli.club/joy/202210201346551.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  受体-供体-受体(ADA),有个甲基(M),甲基会有很不一样的相互作用\n\n  <img src=\"http://img.peterli.club/joy/202210201346554.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  甲基-受体-供体-受体(MADA)\n\n  <img src=\"http://img.peterli.club/joy/202210201346556.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  按照你得到的碱基对，你能读到这个碱基对的所有信息，通过观察大沟，可以区分这是TA或是AT残基，或者分辨它是CG还是GC残基\n- Interactions that non-sequence specific but watson/ crick base pair specfic in minor groove\n\n  我们喜欢小沟(minor groove)，因为小沟除了中间的氢键不一样，每个碱基对两侧的氢键都是一样的，它们在完全相同的位置，无论碱基对是AT、CG、GC或是TA，小沟的氢键受体对都是一样的\n\n  在小沟中，其相互作用没有序列特异性，但有沃森-克里克碱基配对特异性，因为很多蛋白能非特异性地识别DNA，正是通过这种相互作用，这很重要，因为这使得聚合酶不仅能无视碱基对识别DNA，还能直接识别这就是碱基对\n\n  如果这里是AG，这就全乱套了，它们位置不对，聚合酶能察觉这点，如果这里是CC、CT或其它错配的碱基对，这使两个受体扭曲了，聚合酶会说“新合成的DNA不对”，它会对其作出反应，它不仅能检测出所有不同的序列，还能检测出碱基配对是不是正确，是不是遵循沃森-克里克碱基配对\n\n<img src=\"http://img.peterli.club/joy/202210201346558.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201346559.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n- dNTP binding is mediated by base pairing and binding to divalent metal cations($Mg^{+2}$)\n\n  dNTP(核苷酸)结合是由碱基配对介导的，还螯合了二价金属阳离子，通常是镁离子，如果你用EDTA类的二价阳离子螯合剂，可以使任何DNA聚合酶失活\n\n  这是个准备掺入的核苷酸，这里会形成氢键(黄色粗线)，氢键是结合核苷酸的一种方式，另一种则是三磷酸和一对二价金属阳离子间的作用，这可以在碱基配对外提供额外支持，把dNTP固定在正确位置，为催化提供条件\n\n  <img src=\"http://img.peterli.club/joy/202210201346909.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n## Finger Domain Closes On Bound dNTP\n\n一旦上述图发生，另一件事情也会发生，那就是O-螺旋，这是fingers domain 指部结构的一部分。指部结构会在结合的dNTP周围开合，这就涉及O-螺旋，这是一种$\\alpha$-螺旋\n\n- O-helix($\\alpha$-helix)\n- Tyr-forms  $\\pi-\\pi$ Bounds w/ Base ($\\pi-\\pi$键结合碱基)\n- Lys-Arg Bind Triphosphate(赖氨酸和精氨酸结合三磷酸盐)\n\n  这是由于酪氨酸的环状结构与掺入的核苷酸之间形成了$\\pi-\\pi$键，相互作用发生在六元环状结构部分，无论是有一个六元环和五元环的嘌呤，还是仅有一个六元环的嘧啶，都能和酪氨酸(Tyrosine)发生反应，这能促使O-螺旋靠近dNTP\n\n  赖氨酸(Lysine)和精氨酸(Arginine)也是如此，它们会和$\\beta$位及$\\gamma$位的磷酸相互作用，这些相互作用使得O-螺旋被压下来，压在碱基对的上面，其重要的作用是防止水解发生，因此，这时候水就不能进入活性部位，那么唯一能发生的反应就是，羟基进攻磷酸基团(右图橙色线)\n\n  这里显示了加入新核苷酸时，中间体的真实晶体结构，橙色是O-螺旋，粉红色的是酪氨酸。蓝色是赖氨酸，红色是精氨酸，它们和二磷酸产生相互作用，黑色为$\\alpha、\\beta、\\gamma$位磷酸，绿色的为两个金属离子，负责固定住三磷酸盐，紫色为primer\n\n  O-螺旋被压的很紧密，催化反应发生了，那么接下去会发生什么？\n\n## Release of Pyrophosphate Releases O-helix\n\n释放焦磷酸盐和O-螺旋。当焦磷酸盐的$\\beta、\\gamma$位这两个磷酸被释放，对于位于此处的碱基的吸引力会弱很多，它就会弹回去，如果回到原来的状态，从👈这个有相互作用力的状态，回到👉这个没有相互作用力的状态\n\n<img src=\"http://img.peterli.club/joy/202210201346943.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201346973.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n那么最后一件事情就会发生，那就是当我们将核苷酸添加到此，它就不会是原来的样子。这里就是一个碱基对，但碱基对和聚合酶之间没有很强的相互作用力，聚合酶希望这里黄色圈圈，而非下一个位置能有个3‘端羟基，这里怎么才能有个羟基？\n\n把下一个单链DNA模板移动到这里，只需将DNA整体下移(中图)，聚合物和这种形式(右图)的底物有较高相互作用力，中图到右图只需要移动一个位置，因为酶与双链DNA的相互作用是非碱基特异的\n\n从能量的角度说，移动DNA的过程就相当于在手掌位置移动一个碱基对，移动一个碱基对不会改变与聚合酶间的相互作用力，但和3‘端羟基移动后的位置相比，原来的位置与聚合酶间的相互作用力要弱很多，移动一个碱基对就能让3‘端羟基到右图的位置，这个过程非常快，每加一个碱基只要一毫秒\n\n当反应发生时，磷酸基为什么不留在原来的位置？\n\n部分是因为它们不再和核苷酸相连了，原先核苷酸是和磷酸基相连的，一旦引物羟基在这里与磷酸结合，剩余的结合键就无法维持，这就是它们被释放的最主要原因，这时候就是因为熵，哪怕它们离开一点点时间，它们就永远离开了，因为不再有共价键了\n\n<img src=\"http://img.peterli.club/joy/202210201346008.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201346192.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201346261.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.5 The DNA Polymerase Active Site","url":"/2022/1-5-The-DNA-Polymerase-Active-Site/","content":"# 1.5 DNA聚合酶的活性位点——The DNA Polymerase Active Site\n\n- DNA聚合酶可以催化多少种反应？\n\n  4种，每种核苷酸算作1种，例G催化C结合反应\n\n  但如果考虑到之前的碱基，引物也是底物的一部分，每个新来的核苷酸可以接在4种不同的核苷酸之上，如果先前的核苷酸也参与反应，如果前面是一个C，那么所进行的反应就会不同于G，或与嘌呤、嘧啶的反应都有区别。所以你们可以认为DNA聚合酶催化至少16种不同的反应\n- DNA聚合酶有几个活性位点？\n\n  一个\n\n  尽管他催化很多种不同的反应，但它只有一个活性位点，那么一个活性位点如何催化多种dNTP呢？\n\n  我们通常考虑到的酶与之不同，例如 如果你想要将亮氨酸换成精氨酸，或在合成这些氨基酸的时候加上一个羧基，给精氨酸和亮氨酸加羧基的过程，需要两种完全不同的酶来催化，这里只有一种酶却能做到这些，那他是怎么做到的呢？\n\n  DNA聚合酶是利用盲文来工作的，它之所以不关心是什么核苷酸在进行聚合，就是因为这种特性。DNA结构非常规律规整，这些中间蓝色区域的碱基对，每一对都是什么并不重要，它们看起来都一样，它们之间只有一些细微的差别，但是结构非常相似\n\n  四种核苷酸产生的碱基配对，TA GC AT CG 都具有相同的结构，或者更准确的说，它们具有相同的规格，这个距离(黑色线)，在四种碱基配对方式之间是相等的，这四种核苷酸是以完全相同的方式排列的，这样一来，只需一种酶就可以催化那些不同的反应了\n\n  <img src=\"http://img.peterli.club/joy/202210201319412.png\" alt=\"image-20221020131946391\" style=\"zoom: 33%;\" />\n\n  - 原理\n\n    核苷酸必须保证碱基互补配对，这样催化反应才能发生，当碱基配对正确之后，你只需将这个三磷酸基团放到正确的位置，引物上3’端的羟基就可以进行一项细胞内的大多数化学反应中都存在的反应即双分子亲核攻击，发生在箭头指的那个磷酸上，敲掉焦磷酸盐，聚合反应继续，但如果你想要让腺嘌呤和腺嘌呤来配对，尽管在结合处有一定的亲和力，但磷酸无法处在正确的位置上，因而也无法让羟基与之作用，它所在的位置会随着错配类型的不同而改变，但没有一种错配会让它处在正确的位置上\n\n    之所以将碱基配对作为催化的前提，是因为这样反应物才能处在正确的位置上使催化反应，关键是$\\alpha$-磷酸和3‘OH的位置，需要处在正确的位置上，这样它们才能进行反应\n\n    这其中关键之处在于，各种碱基对的尺寸相似，正确的碱基配对能让3’OH所处的位置离dNTP的$\\alpha$-磷酸最近，细胞就是通过这种方式，保证DNA聚合时双链正确配对，基本上这一过程产生的错误越多，反应进行的就越慢\n\n    事实上错配也有可能会发生，但总体来讲，这种反应发生的速率太慢，以至于在错配可能发生之前，这个核苷酸就已经离开活性位点了，在碱基没有配对的情况下，亲和力很低，不足以让那核苷酸在活性位点上停留足够长时间，来允许催化反应的发生，但是正确配对的碱基对不仅会结合的更加紧密，而且所处的相对位置会使得反应更快地进行，因此时间和亲和力这两点保证了正确反应的发生\n\n    <img src=\"http://img.peterli.club/joy/202210201319055.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210201319048.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.4 The Chemistry of DNA Replication","url":"/2022/1-4-The-Chemistry-of-DNA-Replication/","content":"# 1.4 DNA复制的化学过程——The Chemistry of DNA Replication\n\n- DNA复制的底物——引物-模版接头(primer:template junction——PTJ)\n\n  <img src=\"http://img.peterli.club/joy/202210201301015.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  引物在3‘端有一个未被占领的羟基(OH)，羟基进攻脱氧核苷三磷酸(dNTP)上的$\\alpha$-磷酸基，并脱去一分子焦磷酸盐(pyrophosphate)，从而完成这个催化反应。只有在这个碱基与模版上的碱基正确配对时，反应才会发生\n\n  <img src=\"http://img.peterli.club/joy/202210201301024.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  - 为什么要用三磷酸盐？而不是二磷酸盐之类的\n\n    这里提到的所有前体都是三磷酸盐，一个重要原因是，第二个反应分解了两个磷酸互相结合成的焦磷酸盐，得到两分子磷酸，这样就推动了反应，从而使这个反应变得不可逆\n\n    在焦磷酸酶(pyrophosphotase)作用下，这个反应的$\\Delta G$=-7千卡，并且不可逆，没有焦磷酸酶的话，$\\Delta G$=-3.5千卡。为了确保这个反应正向进行，细胞不仅释放焦磷酸酶，而且迅速捕获焦磷酸，并将它分解为两分子磷酸\n  - 反应是怎样被催化的？\n\n    DNA聚合酶(DNA polymerase)催化了反应，它需要什么条件呢？\n\n    1. 需要一个3’羟基端引物(3’ OH primer)\n    2. 引物必须与另一条更长的单链DNA退火(变性的DNA重新恢复为双螺旋的过程)，也就是需要3’羟基端引物紧挨着单链DNA\n\n       1、2结合在一起就是引物-模板接头——底物\n    3. 需要全部四种dNTP（除非你要复制的聚合物只有一种碱基对\n    4. 加入的dNTP必须能与模板进行碱基配对\n    5. DNA的合成方向往往是5’端到3‘端，这是为啥呢？\n\n       这个化学反应要通过合成DNA来延伸(引物的)3’羟基端，始终是引物被延长，当你说5’端到3’端的时候，就不用担心搞不清这是指模板还是引物\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.3 DNA Replication","url":"/2022/1-3-DNA-Replication/","content":"# 1.3 DNA复制(replication)\n\n* 细胞需要进行DNA复制的根本原因：\n  为了复制细胞的基因组，基因组包含了所有细胞功能的蓝本，一个细胞所有的基本活动都由基因组控制\n* 复制过程发生了错误\n\n  * 积极：可能会产生一种有利于生存的新特质，这会以某种频率发生\n  * 坏的：可能从直接致死，生物不能继续生长到产生癌症之类或使生物生长能力下降\n\n  一般来说出现不好结果的可能性要大于好的结果，我们希望这个复制过程能尽可能精确\n* DNA复制的特征\n\n  * 速度很快,一个普通的DNA复制机器而言,合成速度在100-1000 bp/sec，bp-base pair 碱基对\n  * 完备的(complete)，除了少数例外，基因组中的每个碱基对都会被复制，端粒的复制是一个存在碱基对不被复制的例子\n  * 精确的(accurate)，通常情况下，每合成100亿个碱基对，复制机器才会出现一次错误。\n    人的基因组有30亿个碱基对，这意味着每三次细胞分裂就有一个复制错误，但是100亿个碱基对中的大部分都是没有编码功能的，大部分都不必完全正确，但如果复制错误出现在一个编码区的中间，它会改变编码出来的氨基酸和蛋白质，这就会出现问题了\n* DNA复制的应用\n\n  * 我们能用分子克隆拼接基因，或是制造各种各样不同的东西，这项技术直接产生于对DNA复制的研究，PCR(Polymerase Chain Reaction 聚合酶链式反应)也产生于此\n  * 大规模测序，我们之所以能测序整个基因组，包括人类基因组，都是研究DNA复制的结果\n  * DNA复制还经常被用做药物靶点，大部分一线化疗药物都以DNA复制作为靶点，很多人都在研究，希望能得到比DNA复制靶点药物更好的药物，因为现在这些药不仅攻击你的肿瘤细胞，还攻击你其它细胞，但是如果你得了癌症，不使用DNA复制抑制剂类药物的可能性非常非常小，因为那些新药，要么要与DNA复制抑制剂联合使用，要么不能应用在你患的那种肿瘤上，因为这些药大部分的靶点只是众多肿瘤中的极少数\n\n    另外大多数抗病毒药物也以DNA复制为作用靶点，举例来说，最好的抗艾滋病药物之一，就是以病毒基因组的DNA复制或者基因组RNA/DNA复制为靶点的，它们是重要的药物靶点，所以理解复制能够帮助我们开发新的靶点或是开发出一些副作用较小的药物\n\n    通俗解释：DNA复制为靶点就是为了抑制癌细胞生长，但是药的特异性不够，不仅会攻击癌细胞也会攻击你的正常细胞，世界万物的DNA复制过程都有共性，但是也有特异性，问题就是特异性很难找\n\n    打个比方，假设药物对DNA复制中的酶a有作用，但是人的细胞中也有酶a，那这个药只是特异性只表现在酶之间，但我们要的特异性不只是酶之间，还是正常细胞和癌细胞之间\n\n    所以这段话的主要表达就是，好好理解DNA是怎么复制的，仔细研究和DNA复制相关的因子，找到正常细胞和癌细胞之间不同的那个，去找到特异性更高的靶点，研究药物作用于这个靶点。这个靶点可以直接作用于DNA，也可以作用于“工具”。（DNA复制过程中会用到什么工具和辅助工具，这些工具可以成为药物靶点）\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1.2 DNA Structure","url":"/2022/1-2-DNA-Structure/","content":"# 1.2 DNA结构\n\nDNA：deoxyribonucleic acid 脱氧核糖核酸\n\n<img src=\"http://img.peterli.club/joy/202210201030234.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n- dsDNA\n\n  在细胞内部，最常会看到双链DNA，两条链缠绕在一起形成双螺旋，DNA双螺旋最常见的形式也被称为B型DNA\n\n  <img src=\"http://img.peterli.club/joy/202210201030299.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n- 双螺旋解开\n\n  <img src=\"http://img.peterli.club/joy/202210201030290.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210201030272.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201030241.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210201030245.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- 内部化学结构\n\n  每一条链都是多聚核苷酸链\n\n  <img src=\"http://img.peterli.club/joy/202210201030966.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n  意味着DNA链由许多核苷酸(nucleotide)单位组成，一个核苷酸有三部分\n\n  <img src=\"http://img.peterli.club/joy/202210201030988.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  - 一个磷酸基团(phosphate)、一个戊糖(sugar)、四种含氮碱基之一\n\n    <img src=\"http://img.peterli.club/joy/202210201030077.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    腺嘌呤(adenine)\n\n    <img src=\"http://img.peterli.club/joy/202210201030102.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    鸟嘌呤(guanine)\n\n    <img src=\"http://img.peterli.club/joy/202210201030195.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    胸腺嘧啶(thymine)\n\n    <img src=\"http://img.peterli.club/joy/202210201030344.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    胞嘧啶(cytosine)\n  - 含氮碱基总是连接在戊糖的1’碳上，5’碳和相邻戊糖的3’碳之间有一个磷酸基团\n\n    <img src=\"http://img.peterli.club/joy/202210201030397.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n  - 戊糖为脱氧核糖，因为在2’碳位脱去了一个核糖中本该存在的羟基，DNA的核苷酸叫做脱氧核糖核苷酸\n\n    <img src=\"http://img.peterli.club/joy/202210201030520.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n  - DNA中的核苷酸通过磷酸二酯键相互连接，核苷酸上的磷酸基团，连接上邻位核苷酸的3’碳\n\n    <img src=\"http://img.peterli.club/joy/202210201030546.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n  - 脱氧核糖和磷酸基团组成了DNA骨架\n\n    碳原子的编号对于描述DNA 5’到3’的方向性至关重要\n\n    DNA内部两条链，顶部链5‘碳都在左边，3’碳都在右边，方向为5’➡️3’，底部链相反\n\n    <img src=\"http://img.peterli.club/joy/202210201030746.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n  - 氢键\n\n    - 核苷酸通过共价键聚合在一起构成骨架\n    - 两条DNA链通过碱基间的非共价键——氢键相互作用，每个碱基都会和另一条链上的互补碱基生成多个氢键\n\n    <img src=\"http://img.peterli.club/joy/202210201030179.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n    - 通过氢键连接的那一个单位叫做碱基对，氢键使得碱基对存在一定的特异性\n\n    <img src=\"http://img.peterli.club/joy/202210201030201.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210201030224.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    腺嘌呤(adenine)和胸腺嘧啶(thymine)配对形成两个氢键\n\n    <img src=\"http://img.peterli.club/joy/202210201030418.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    胞嘧啶(cytosine)和鸟嘌呤(guanine)配对形成三个氢键\n  - 嘧啶和嘌呤的结构\n\n    胸腺嘧啶和胞嘧啶是嘧啶，以单环结构为特征\n\n    <img src=\"http://img.peterli.club/joy/202210201030458.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n    腺嘌呤和鸟嘌呤是嘌呤，它们都有双环\n\n    <img src=\"http://img.peterli.club/joy/202210201030504.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - AT或者TA、GC或者CG碱基对的几何形态是相同的，使得螺旋具有对称性，碱基也能堆积起来，这和骨架之间的距离和碱基连接到骨架上的角度有关\n\n    <img src=\"http://img.peterli.club/joy/202210201030934.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - 其它碱基对例如GT，没有这类的几何构型，不能形成作用较强的氢键，并且会阻碍螺旋的形成\n\n    <img src=\"http://img.peterli.club/joy/202210201030071.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - DNA的双螺旋结构具有很强的规律性，每圈螺旋大概是10碱基对长\n\n    <img src=\"http://img.peterli.club/joy/202210201030384.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - 除了碱基之间的氢键，碱基堆积也加固了双螺旋结构\n\n    <img src=\"http://img.peterli.club/joy/202210201030540.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210201030632.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n  - 当碱基里相邻芳香环堆积在一起，共享电子，就形成了$\\pi-\\pi$键的相互作用\n\n    <img src=\"http://img.peterli.club/joy/202210201030926.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  - 规律性的螺旋结构形成两部分重复交替的空间，叫大沟(major grooves) 和 小沟(minor grooves)\n\n    <img src=\"http://img.peterli.club/joy/202210201030107.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    这些沟的作用是识别碱基对以及作为蛋白质的结合位点\n\n    大沟含有携带特殊信息的碱基对，小沟主要是非特异性碱基对\n\n    <img src=\"http://img.peterli.club/joy/202210201030447.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    因为蛋白质能作用的氢键受体和供体位于沟内，这样DNA既可以被看作为特异性序列，也能被看作为非特异性序列，使得蛋白质能够结合到正确的基因上，完成指定任务\n\n    <img src=\"http://img.peterli.club/joy/202210201030453.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"1 Introduction","url":"/2022/1-Introduction/","content":"# 1.1 介绍\n\n第一部分内容包括基因组维护的调控，染色体复制、修复和重组\n\n细胞怎样复制它们的基因组？\n\n出现错误或DNA损伤时，细胞怎样修复DNA？\n\n在DNA断裂反应中或减数分裂时，它们是怎样重组DNA的？\n\n介绍这个过程的概念与步骤，从DNA复制开始，细胞怎样完成这个过程？具体的分子机制是什么？每个过程中，讨论所需的蛋白质、RNA和DNA，并解释这些酶与核酸的协同作用及调控，以确保它们正常发挥功能，DNA复制和修复过程是相当保守的\n\n要关注科学家们如何探索这些机制\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"Molecular Biology","url":"/2022/MolecularBiology/","content":"# 分子生物学\n\n【【MIT麻省理工精品课程】分子生物学系列第一部分：DNA复制与修复【全中英字幕】】https://www.bilibili.com/video/BV1Kt4y1k7Aw?vd_source=5ec85dfc5468a21a485b1b1d4d271219\n\n# 第一部分 DNA复制与修复\n\n[1.1 介绍](/2022/1-Introduction/)\n\n[1.2 DNA结构](/2022/1-2-DNA-Structure/)\n\n[1.3 DNA复制(replication)](/2022/1-3-DNA-Replication/)\n\n[1.4 DNA复制的化学过程——The Chemistry of DNA Replication](/2022/1-4-The-Chemistry-of-DNA-Replication/)\n\n[1.5 DNA聚合酶的活性位点——The DNA Polymerase Active Site](/2022/1-5-The-DNA-Polymerase-Active-Site/)\n\n[1.6 &amp; 1.7 DNA聚合酶的结构——The Structure of DNA Polymerase](/2022/1-6and7-The-Structure-of-DNA-Polymerase/)\n\n[1.8 &amp; 1.9 DNA聚合酶发生的错误及修复——Mistakes of DNA Polymerase](/2022/1-8and9-Mistakes-of-DNA-Polymerase/)\n\n---\n\n\n\n[2.1 DNA复制检测——Incorporation Assay](/2022/2-1-Incorporation-Assay/)\n\n[2.2 &amp; 2.3 Filter Binding Incorporation Assay &amp; Electrophoresis Incorporation Assay](/2022/2-2and2-3-Filter-Binding-and-Electrophoresis-Incorporation-Assay/)\n\n[2.4 Primer Extension Assay](/2022/2-4-Primer-Extension-Assay/)\n\n[2.5 Processivity &amp; 2.6 DNA Polymerase  Processivity Assay](/2022/2-5-Processivity-and-2-6-DNA-Polymerase-Processivity-Assay/)\n","tags":["BioInformatics","Learning Note","Molecular Biology"],"categories":["Learning Note","BioInformatics","Molecular Biology"]},{"title":"SDUBioInformatics","url":"/2022/SDUBioInformatics/","content":"# 山东大学生物信息学\n\n山东大学 B站\n\n[https://www.bilibili.com/video/BV13t411372E?spm_id_from=333.337.search-card.all.click&amp;vd_source=b60872cd374332ccebd93d8a3907fbc7](https://www.bilibili.com/video/BV13t411372E?spm_id_from=333.337.search-card.all.click&vd_source=b60872cd374332ccebd93d8a3907fbc7)\n\nCSDN [https://blog.csdn.net/zea408497299/article/details/125100031](https://blog.csdn.net/zea408497299/article/details/125100031)\n\n- 框架图\n\n  ![Untitled](http://img.peterli.club/joy/202210191842220.png)\n\n### 第一章 绪论\n\n[1. 简介](2022/1-SDU-Introduction/)\n\n---\n\n\n\n### 第二章 生物数据库（第一部分）\n\n[2.1 为什么需要生物数据库&amp;2.2 生物数据库分类](/2022/2-1and2-BioDatabase/)\n\n[2.3 文献数据库PubMed](/2022/2-3-PubMed/)\n\n[2.4 一级核酸数据库](/2022/2-4-Primary-Nucleic-Acid-Database/)\n\n[2.4.1 GenBank 原核生物核酸序列](/2022/2-4-1-GenBank-Prokaryotic-nucleic-acid-sequences/)\n\n[2.4.2&amp;3 GenBank: 真核生物核酸序列mRNA&amp;DNA](/2022/2-4-2and3-GenBank-Eukaryotic-Nucleic-Acid-Sequences-mRNA-and-DNA/)\n\n[2.5 基因组数据库：Ensemble &amp; JCVI](/2022/2-5-Genome-Database/)\n\n[2.6 二级核酸数据库](/2022/2-6-Secondary-Nucleic-Acid-Database/)\n\n---\n\n\n\n### 第二章 生物数据库（第二部分）\n\n[2.7 一级蛋白质序列数据库 UniProt](/2022/2-7-Primary-Protein-Sequence-Database-UniProt/)\n\n[2.8 一级蛋白质结构数据库：PDB](/2022/2-8-Primary-Protein-Structure-Database-PDB/)\n\n[2.9 二级蛋白质数据库：Pfam、CATH、SCOP2](/2022/2-9-Secondary-Protein-Database/)\n\n[2.10 专用数据库：KEGG、OMIM](/2022/2-10-Dedicated-database-KEGG-OMIM/)\n\n---\n\n\n\n### 第三章 序列比较（第一部分）\n\n[3.1 认识序列](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%201%20%E8%AE%A4%E8%AF%86%E5%BA%8F%E5%88%97%20e945ca22dd604acfa3c9c3345f7347e4.md)\n\n[3.2 序列相似性](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%202%20%E5%BA%8F%E5%88%97%E7%9B%B8%E4%BC%BC%E6%80%A7%201d3b719cc50145ce92aabefaa353ed4e.md)\n\n[3.3 替换记分矩阵——序列长一样](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%203%20%E6%9B%BF%E6%8D%A2%E8%AE%B0%E5%88%86%E7%9F%A9%E9%98%B5%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E9%95%BF%E4%B8%80%E6%A0%B7%20b6275be39be640bb858f36da38552000.md)\n\n---\n\n[3.4 序列两两比较：打点法——序列长不一样(粗略估计)](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%204%20%E5%BA%8F%E5%88%97%E4%B8%A4%E4%B8%A4%E6%AF%94%E8%BE%83%EF%BC%9A%E6%89%93%E7%82%B9%E6%B3%95%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E9%95%BF%E4%B8%8D%E4%B8%80%E6%A0%B7(%E7%B2%97%E7%95%A5%E4%BC%B0%E8%AE%A1)%20559e0eb5caaf4568828fc9dfb6437d1a.md)\n\n---\n\n[3.5 序列两两比较：序列比对法——序列长不一样(定量)](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%205%20%E5%BA%8F%E5%88%97%E4%B8%A4%E4%B8%A4%E6%AF%94%E8%BE%83%EF%BC%9A%E5%BA%8F%E5%88%97%E6%AF%94%E5%AF%B9%E6%B3%95%E2%80%94%E2%80%94%E5%BA%8F%E5%88%97%E9%95%BF%E4%B8%8D%E4%B8%80%E6%A0%B7(%E5%AE%9A%E9%87%8F)%2065194e0f934543ebac1a1fbf0594c30c.md)\n\n---\n\n[3.6 一致度和相似度](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%206%20%E4%B8%80%E8%87%B4%E5%BA%A6%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%20dbc1820ae2df45218ce57d2aad6eff11.md)\n\n### 第三章 序列比较（第二部分）\n\n---\n\n[3.7 在线双序列比对工具](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%207%20%E5%9C%A8%E7%BA%BF%E5%8F%8C%E5%BA%8F%E5%88%97%E6%AF%94%E5%AF%B9%E5%B7%A5%E5%85%B7%20f2879482fd014fbcb15daf8e7765c014.md)\n\n---\n\n[3.8 BLAST搜索](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%208%20BLAST%E6%90%9C%E7%B4%A2%207fc9b0b9b3fd4d58a125b2dbf567b61f.md)\n\n### 第三章 序列比较（第三部分）\n\n---\n\n[3.9 多序列比对介绍](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%209%20%E5%A4%9A%E5%BA%8F%E5%88%97%E6%AF%94%E5%AF%B9%E4%BB%8B%E7%BB%8D%20df6b70c6e7b349969cae44ded1477d22.md)\n\n---\n\n[3.10 在线多序列比对工具](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%2010%20%E5%9C%A8%E7%BA%BF%E5%A4%9A%E5%BA%8F%E5%88%97%E6%AF%94%E5%AF%B9%E5%B7%A5%E5%85%B7%20021bc8094812466692e9ba9ae2047d0a.md)\n\n---\n\n[3.11 多序列比对的编辑和发布](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%2011%20%E5%A4%9A%E5%BA%8F%E5%88%97%E6%AF%94%E5%AF%B9%E7%9A%84%E7%BC%96%E8%BE%91%E5%92%8C%E5%8F%91%E5%B8%83%200d7316c9c957484798706fccfae80aa9.md)\n\n---\n\n[3.12 寻找保守区域](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/3%2012%20%E5%AF%BB%E6%89%BE%E4%BF%9D%E5%AE%88%E5%8C%BA%E5%9F%9F%20b012f2cccd1049c5b568cd5beb63837a.md)\n\n### 第四章 分子进化与系统发生\n\n---\n\n[4.1 进化的故事](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/4%201%20%E8%BF%9B%E5%8C%96%E7%9A%84%E6%95%85%E4%BA%8B%203f4248d097574e50a7f43a5d44f9043d.md)\n\n---\n\n[4.2 基本概念](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/4%202%20%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%20a819accbbbd34f299fd2a4f38b21db00.md)\n\n---\n\n[4.3 系统发生树](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/4%203%20%E7%B3%BB%E7%BB%9F%E5%8F%91%E7%94%9F%E6%A0%91%20f4f39e939d3f47a7b7fb81792358b33e.md)\n\n---\n\n[4.4 系统发生树的构建](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/4%204%20%E7%B3%BB%E7%BB%9F%E5%8F%91%E7%94%9F%E6%A0%91%E7%9A%84%E6%9E%84%E5%BB%BA%2018d2bc4bcd234c5c91a80daf3bb2042c.md)\n\n---\n\n[4.5 MEGA7构建NJ树](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/4%205%20MEGA7%E6%9E%84%E5%BB%BANJ%E6%A0%91%20b8f97d7ea64945fd9bb65d686bf80954.md)\n\n---\n\n### 第五章 蛋白质结构预测与分析（第一部分）\n\n---\n\n[5.1 蛋白质的结构](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%201%20%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%9A%84%E7%BB%93%E6%9E%84%2020c55c77e1044a7b8ca5288740985a3a.md)\n\n---\n\n[5.2 蛋白质的二级结构](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%202%20%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%9A%84%E4%BA%8C%E7%BA%A7%E7%BB%93%E6%9E%84%20c3b01844b34f40268503ea1346f6ee79.md)\n\n---\n\n[5.3 蛋白质的三级结构](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%203%20%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%9A%84%E4%B8%89%E7%BA%A7%E7%BB%93%E6%9E%84%20bd1ca3bda7b34eb0b11d68e8ef768a7a.md)\n\n---\n\n[5.4 三级结构可视化软件：VMD](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%204%20%E4%B8%89%E7%BA%A7%E7%BB%93%E6%9E%84%E5%8F%AF%E8%A7%86%E5%8C%96%E8%BD%AF%E4%BB%B6%EF%BC%9AVMD%2081c19112128a4c09ba14d2987716d142.md)\n\n### 第五章 蛋白质结构预测与分析（第二部分）\n\n---\n\n[5.5 计算方法预测三级结构](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%205%20%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E9%A2%84%E6%B5%8B%E4%B8%89%E7%BA%A7%E7%BB%93%E6%9E%84%2013d5fbe1ec2743b5a08e4be23cff4af7.md)\n\n---\n\n[5.6 三级结构的比对](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%206%20%E4%B8%89%E7%BA%A7%E7%BB%93%E6%9E%84%E7%9A%84%E6%AF%94%E5%AF%B9%204c05068caf9a432bbf3059dfa54ce19e.md)\n\n---\n\n[5.7 蛋白质分子表面性质](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%207%20%E8%9B%8B%E7%99%BD%E8%B4%A8%E5%88%86%E5%AD%90%E8%A1%A8%E9%9D%A2%E6%80%A7%E8%B4%A8%2059f886fc2cdd47068041924abc97bf99.md)\n\n### 第五章 蛋白质结构预测与分析（第三部分）\n\n---\n\n[5.8 获取蛋白质四级结构](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%208%20%E8%8E%B7%E5%8F%96%E8%9B%8B%E7%99%BD%E8%B4%A8%E5%9B%9B%E7%BA%A7%E7%BB%93%E6%9E%84%2017c6f33cd9e74ff0a0331383b0f21851.md)\n\n---\n\n[5.9 蛋白质-蛋白质分子对接](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%209%20%E8%9B%8B%E7%99%BD%E8%B4%A8-%E8%9B%8B%E7%99%BD%E8%B4%A8%E5%88%86%E5%AD%90%E5%AF%B9%E6%8E%A5%207bc26a05c0774992b0ce47ac031eeb2b.md)\n\n---\n\n[5.10 蛋白质-小分子分子对接](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%2010%20%E8%9B%8B%E7%99%BD%E8%B4%A8-%E5%B0%8F%E5%88%86%E5%AD%90%E5%88%86%E5%AD%90%E5%AF%B9%E6%8E%A5%202c221d64bdd64d4c829e40196a6d6875.md)\n\n---\n\n[5.11 虚拟筛选与反向对接](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%2011%20%E8%99%9A%E6%8B%9F%E7%AD%9B%E9%80%89%E4%B8%8E%E5%8F%8D%E5%90%91%E5%AF%B9%E6%8E%A5%20c41e6e76a2334fe1b539ed077f014522.md)\n\n---\n\n[5.12 分子动力学模拟](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/5%2012%20%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E6%A8%A1%E6%8B%9F%2050b65fefe2eb47ff94e6a17bbc589d4d.md)\n\n### 第六章 高通量测序技术及应用\n\n---\n\n[6.1 基因组学与测序技术](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%201%20%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E4%B8%8E%E6%B5%8B%E5%BA%8F%E6%8A%80%E6%9C%AF%20aad52fbc5a784b1fbb692fd907a616ee.md)\n\n---\n\n[6.2 高通量测序技术在精准医学中的应用](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%202%20%E9%AB%98%E9%80%9A%E9%87%8F%E6%B5%8B%E5%BA%8F%E6%8A%80%E6%9C%AF%E5%9C%A8%E7%B2%BE%E5%87%86%E5%8C%BB%E5%AD%A6%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8%2050f4ca4b9e2d4851955cb6dc918a6d90.md)\n\n---\n\n[6.3 生物信息学面临的挑战](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%203%20%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98%20a174a3e191024499919c90f6780365f3.md)\n\n---\n\n[6.4 从头测序](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%204%20%E4%BB%8E%E5%A4%B4%E6%B5%8B%E5%BA%8F%20e53f4b98c36c468a8111cc5211cce139.md)\n\n---\n\n[6.5 重测序](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%205%20%E9%87%8D%E6%B5%8B%E5%BA%8F%20da35f9d120574745b3b2d254f4585677.md)\n\n---\n\n[6.6 转录组测序 mRNA-seq](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%206%20%E8%BD%AC%E5%BD%95%E7%BB%84%E6%B5%8B%E5%BA%8F%20mRNA-seq%207f6d966f7c044972b7e5053e8287cea9.md)\n\n---\n\n[6.7 表观基因组学 ChIP-seq](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%207%20%E8%A1%A8%E8%A7%82%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%20ChIP-seq%20137fdf3f1d4e4211afaad6d525ba2446.md)\n\n---\n\n[6.8 猛犸象基因组测序计划](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%208%20%E7%8C%9B%E7%8A%B8%E8%B1%A1%E5%9F%BA%E5%9B%A0%E7%BB%84%E6%B5%8B%E5%BA%8F%E8%AE%A1%E5%88%92%20bf0fed9730614a1cb607304e1da238c1.md)\n\n---\n\n[6.9 古基因组学面临的挑战](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%209%20%E5%8F%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%8C%91%E6%88%98%208caef98d692e4b3fb7679e1e070d755b.md)\n\n---\n\n[6.10 古基因组学研究中的生物信息技术](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/6%2010%20%E5%8F%A4%E5%9F%BA%E5%9B%A0%E7%BB%84%E5%AD%A6%E7%A0%94%E7%A9%B6%E4%B8%AD%E7%9A%84%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E6%8A%80%E6%9C%AF%20062d773b2a084f50b714b8967b9e4fd3.md)\n\n### 第七章 统计基础与序列算法\n\n---\n\n[7.1 贝叶斯公式及其生物学应用](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/7%201%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%85%B6%E7%94%9F%E7%89%A9%E5%AD%A6%E5%BA%94%E7%94%A8%20cc9a47bf583a4c7e95d325f583b93a3c.md)\n\n---\n\n[7.2 二元预测的灵敏度和特异度](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/7%202%20%E4%BA%8C%E5%85%83%E9%A2%84%E6%B5%8B%E7%9A%84%E7%81%B5%E6%95%8F%E5%BA%A6%E5%92%8C%E7%89%B9%E5%BC%82%E5%BA%A6%20fa1030a9c02e4fc7ad7a23fcd0697313.md)\n\n---\n\n[7.3 基本序列算法](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/7%203%20%E5%9F%BA%E6%9C%AC%E5%BA%8F%E5%88%97%E7%AE%97%E6%B3%95%20fa54490344994f218e8b8c844e9fad75.md)\n\n### 第八章 数据挖掘\n\n---\n\n[8.1 什么是数据挖掘](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/8%201%20%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%208ed26bc1afbb4e03a170274d7f3f6c11.md)\n\n---\n\n[8.2 数据库系统](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/8%202%20%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%2079a24d96f7f64ce08c52b7aaac9670ad.md)\n\n---\n\n[8.3 机器学习](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/8%203%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2088eb011d16764fd8a7b098314764db71.md)\n\n---\n\n[8.4 WEKA](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/8%204%20WEKA%20abe4cb73b72a4769a6fa6edb9d9310a3.md)\n\n### 第九章 编程基础与网页制作（第一部分）\n\n---\n\n[9.1 Linux操作系统](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%201%20Linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%20af24c6af9bab47bcae4cf8b3c89e7316.md)\n\n---\n\n[9.2 Linux基本命令](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%202%20Linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%20159bc5e591bb4162b5b7497d536fcb02.md)\n\n---\n\n[9.3 Perl语言基础入门](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%203%20Perl%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%20732c1a42ca9647cfa15dea8849dae78a.md)\n\n---\n\n[9.4 Perl语言基础高级](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%204%20Perl%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E9%AB%98%E7%BA%A7%20c43fcaf7d8534cf8a08bb966a914c1d3.md)\n\n### 第九章 编程基础与网页制作（第二部分）\n\n---\n\n[9.5 前端开发和HTML介绍](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%205%20%E5%89%8D%E7%AB%AF%E5%BC%80%E5%8F%91%E5%92%8CHTML%E4%BB%8B%E7%BB%8D%20edc9844ff2ba4da08c7032b5343e06b7.md)\n\n---\n\n[9.6 HTML常用标签](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%206%20HTML%E5%B8%B8%E7%94%A8%E6%A0%87%E7%AD%BE%2096ebca44509b4bd8938932be6182fdf0.md)\n\n---\n\n[9.7 设计简单的网页](%E5%B1%B1%E4%B8%9C%E5%A4%A7%E5%AD%A6%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%20459dd1598f104bf0b18a0176f6483896/9%207%20%E8%AE%BE%E8%AE%A1%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E9%A1%B5%20a2db40a512f947f49ee583bd2c8dd3c6.md)\n\n---\n\n[9.8 HTML与CGI简单交互](\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","SDUBioInformatics"]},{"title":"4.4 Supplementary materials","url":"/2022/4-4-Supplementary-materials/","content":"\n# 4.4 关于回帖、变异鉴定的补充材料\n\n- BWA & BWT algorithm\n\n  介绍回帖reads的参考基因组的程序BWA所使用的BWT的压缩和比对算法\n\n  - The compression algorithm used in BWA\n  - Lossless compression\n  - Sort and transform the char matrix\n\n- Variant caller\n\n  — samtools\n\n  — GATK\n\n- Demonstration\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"4.3 Analysis and Demonstration","url":"/2022/4-3-Analysis-and-Demonstration/","content":"\n# 4.3 序列回帖和变异鉴定分析与演示\n\nDemonstration of reads mapping and variants calling\n\n从原始数据出发，最终寻找到我们感兴趣的突变位点\n\n用bwa工具将fastq文件比对到基因组上\n\nfastq文件\n\n![Untitled](http://img.peterli.club/joy/202210191648532.png)\n\n人类的线粒体基因组\n\n![Untitled](http://img.peterli.club/joy/202210191648529.png)\n\nbwa分析\n\n1. 将线粒体基因组进行索引\n\n   ![Untitled](http://img.peterli.club/joy/202210191648534.png)\n\n2. 将分别对高通量测序得到的双端的Reads进行mapping\n\n   ![Untitled](http://img.peterli.club/joy/202210191648538.png)\n\n   ![Untitled](http://img.peterli.club/joy/202210191648536.png)\n\n3. \n\n   ![Untitled](http://img.peterli.club/joy/202210191648546.png)\n\n4. 查看比对生成的文件\n\n   ![Untitled](http://img.peterli.club/joy/202210191648149.png)\n\n5. 对bam文件进行排序索引\n\n   ![Untitled](http://img.peterli.club/joy/202210191648171.png)\n\n6. 使用RealignerTargetCreator工具寻找需要alignment的位点\n\n   ![Untitled](http://img.peterli.club/joy/202210191648194.png)\n\n7. 使用IndelRealigner来对bam文件进行真正的处理\n\n   ![Untitled](http://img.peterli.club/joy/202210191648217.png)\n\n8. 来对测序得到的进行调整\n\n   需要提供一个用于训练的已知的变异位点的测序数据集合\n\n   ![Untitled](http://img.peterli.club/joy/202210191648574.png)\n\n   来生成最终的test.final.bam文件\n\n   ![Untitled](http://img.peterli.club/joy/202210191648610.png)\n\n9. 对比对结果进行varient calling工作\n\n   ![Untitled](http://img.peterli.club/joy/202210191648635.png)\n\n   另一种varient calling方法\n\n   ![Untitled](http://img.peterli.club/joy/202210191648658.png)\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"4.2 Sequence Reply and Mutation Identification","url":"/2022/4-2-Sequence-Reply-and-Mutation-Identification/","content":"\n# 4.2 序列回帖和变异鉴定 NGS：Reads Mapping\n\n- **Reads Mapping 是啥？**\n\n  是指测序得到的DNA片段也就是Reads，定位到基因组上，通过Reads Mapping 在克服了深度测序产生的Reads过短导致的技术困难的同时，也方便利用基因组位置作为桥梁来将测序得到的数据与前期研究产生的注释结果进行有机的整合\n\n- **作用**\n\n  往往被作为深度测序数据分析的第一步，其质量的好坏以及速度的快慢都会对后续的分析产生影响\n\n  <img src=\"http://img.peterli.club/joy/202210191637467.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- **Mapping:** **Input Data**\n\n  本质上还是双序列比对问题\n\n  - Reference Genome\n\n    — Nucleotide\n\n    — **Length**: Hundreds of Mb per chromosome\n\n    — ～3Gb in total(for human genome)\n\n  - Reads\n\n    — Nucleotide, with **various qualities**(relatively **high error rate** 1e-2 ~ 1e-5)\n\n    — **Length**: 36~80 bp per read\n\n    — Hundreds of Gbs per run\n\n  <img src=\"http://img.peterli.club/joy/202210191637461.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191637461.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- BLAST Ideas: Seeding-and-extending\n\n  通过对基因组进行索引，从而将Reads快速定位，而后再通过标准动态规划算法来构建最终的alignment\n\n  1. Find matches(seed) between the query and subject 寻找查询和主题之间的匹配\n\n  2. Extend seed into High Scoring Segment Pairs(HSPs) 将种子扩展为高分段对(HSP)\n\n     — Run Smith-Waterman algorithm on the specified region only 仅在指定区域内运行Smith-Waterman算法\n\n  3. Assess the reliability of the alignment 评估对齐的可靠性\n\n  <img src=\"http://img.peterli.club/joy/202210191637462.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  - 数据索引\n\n    所谓的索引本质上就是对数据的分组，基于对原始数据中的keys应用索引函数来进行处理，可以将原始数据划分为若干更小的组，并通过将与keys相关的检索限制在某个小组里，来缩小搜索空间，从而降低搜索时间\n\n    <img src=\"http://img.peterli.club/joy/202210191637466.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    - 哈希\n\n      <img src=\"http://img.peterli.club/joy/202210191637464.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n      例：对一段基因组序列，首先参考BLAST思路，将之切分成若干的细的片段，而后针对每一个细的片段计算其哈希值作为其在索引表中的地址，并在地址表中保存这段序列及其在基因组中的坐标，在Reads中再次出现这个片段的时候就可以在地址表中迅速找到其在基因组中出现的位置\n\n      <img src=\"http://img.peterli.club/joy/202210191637755.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    - 抽屉原理 提高Reads Mapping速度\n\n      在Reads Mapping过程中，为了提高灵敏度，通常我们会允许若干碱基的错配，这时我们就可以依据抽屉原理，将Reads划分为若干不重叠的块。\n\n      <img src=\"http://img.peterli.club/joy/202210191637784.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n      例如我们将Reads划分为3个不重叠的块，那么就可以确保在错配不超过两个的情况下，至少有一个片段可以作为与基因组完全匹配的种子来查询索引，从而大大提高了Reads Mapping的速度\n\n      <img src=\"http://img.peterli.club/joy/202210191637834.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n    - 前缀树、后缀树 —— 大大提高内存利用的效率和比对的速度\n\n      早期的Reads  Mapping算法ELAND、MAQ、SOAP1等等均广泛的应用了这个思路，但是如果搜索的时候允许较多的错配位点，就需要把序列分成很多小的区块，那这会导致性能急剧下降。因此，09年开始用于数据压缩算法的前缀树和后缀树开始被应用于Reads Mapping，这些数据结构通过合并共享子串降低内存消耗并且可以方便实现最长公共子串的查找。\n\n      <img src=\"http://img.peterli.club/joy/202210191637869.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n      例BOWTIE、BWA、SOAP3等工具均采用了基于后缀的BWT转换，可以逐位对比对片段基因延伸，大大提高内存利用的效率和比对的速度\n\n      <img src=\"http://img.peterli.club/joy/202210191637891.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n  - 找位置\n\n    通过索引找到相应的地址之后，我们就可以参照之前BLAST思路，向左右两个方向延伸扩展之后用动态规划算法来确定最后的比对\n\n    <img src=\"http://img.peterli.club/joy/202210191637066.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    与BLAST搜索不同的是，新一代测序技术的错误率很高，对于一个测序位点我们就需要考虑它是由测序错误引起的假象的可能性，因此08年提出了Mapping Quality 分数的概念来处理这一问题\n\n    <img src=\"http://img.peterli.club/joy/202210191637089.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n    首先假定看到的错配都是由于测序错误引起的，这个时候的概率就是各个错配碱基所对应的概率乘积，将其做过log transformation后的数值称之为SQ。\n\n    <img src=\"http://img.peterli.club/joy/202210191637228.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n    在所有的Mapping位置都计算了一个相应的SQ值之后，就可以进一步的对每个位置计算出错误mapping的概率E，这里计算E的时候是同时考虑序列的相似程度和测序质量，因此在实际的测序中，会更多的使用Mapping Quality而不是序列比对分数来筛选真正的Reads Mapping的位置\n\n    <img src=\"http://img.peterli.club/joy/202210191637250.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- Genetic Variants —— 遗传变异\n\n  当将测序得到的Reads正确Map到基因组之后，就可以开始来鉴定遗传变异，根据遗传变异的尺寸，可以将之分为在单个碱基水平的单核苷酸变异即SNV，和涉及到多个碱基的结构变异SV\n\n  - SNV：Single Nucleotide Variant  单核苷酸变体\n\n    — Substitution(SNP)  碱基替换\n\n    — Indel: insertion/deletion\n\n  - Structural Variation(SV)  结构变异\n\n    — Large-scale insertion/deletion  大规模插入/删除\n\n    — Inversion  倒置\n\n    — Translocation  易位\n\n    — Copy Number Variation(CNV)  拷贝数变异\n\n  <img src=\"http://img.peterli.club/joy/202210191637279.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  - SNP Calling is NOT Genotyping\n\n    - \"SNP calling aims to determine in which positions there are polymorphisms or in which positions at least one of the bases differs from a reference sequence”\n\n      \"SNP调用的目的是确定在哪些位置存在多态性，或在哪些位置至少有一个碱基与参考序列不同”\n\n    - \"Genotype calling is the process of determining the genotype for each individual and is typically only done for positions in which a SNP or a 'variant' has already been called.”\n\n      \"基因型调用是确定每个个体的基因型的过程，通常只针对已经调用了SNP或'变体'的位置。”\n\n  - Counting: an intuitive (and naïve) approach\n\n    在Reads被正确Map的前提下，我们可以参考特定位点上的Map的Reads，运用经验规则来基于计数的方法同时完成SNP calling和Genotyping \n\n    <img src=\"http://img.peterli.club/joy/202210191637305.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n    例：\n\n    - Counting high-confident, non - reference allele (i.e. Quality >= 20)\n\n      —  Freq(突变位点的频率) <20% or > 80%: homozygous genotype 纯合子变异\n\n      — Otherwise: heterozygous 杂合变异位点\n\n    - Works well for \"deeply sequenced regions\" (DSR), i.e. depth > 25x\n\n      对 \"深度测序区域\"（DSR）效果良好，即深度大于25倍\n\n      — But suffer from under-calling of heterozygous genotypes for low -coverage regions \n\n      但对低覆盖率区域的杂合基因型则存在低估的问题\n\n      — And can't give an objective measurement for reliability\n\n      不能对可靠性进行客观的测量\n\n  - A Simple Probabilistic Model for Genotyping\n\n    1. For a diploid genome, there will be at most two different alleles (A and a)observed at a given site: \n\n       对于一个二倍体基因组，在一个给定的部位最多可以观察到两种不同的等位基因(A和a)\n\n       — 3 possible genotypes: <A,A>, <A,a>, <a,a> — Number of A： k;  Number of a:  n-k\n\n    2. Then, the probability for each genotypes is\n\n       <img src=\"http://img.peterli.club/joy/202210191637523.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    3. Bayes Formula can be further employed to calculate posterior probabilities, i.e. P(<A,A>| D), P(<a, a>|D), and P(<A,a>| D) if we can estimate the prior probabilities P(<A,A>), P(<a,a>) and P(<A,a>)\n\n  <img src=\"http://img.peterli.club/joy/202210191637673.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"4.1 Next Generation Sequencing","url":"/2022/4-1-Next-Generation-Sequencing/","content":"\n# 4.1 新一代测序——Next Generation Sequencing(NGS): Reads Mapping\n\n## From Sequencing to NGS\n\n1977 测序方法\n\n<img src=\"http://img.peterli.club/joy/202210191630909.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n新一代测序方法 能得到更深的测序深度\n\n<img src=\"http://img.peterli.club/joy/202210191630904.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- **Read**：A short DNA fragment which is read out by sequencer\n\n  由测序仪读出的一个短的DNA片段\n\n  <img src=\"http://img.peterli.club/joy/202210191630899.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- **Quality**：Given p = the probability of a base calling is wrong(碱基调用错误的概率), its Quality Score can be written as\n\n  <img src=\"http://img.peterli.club/joy/202210191630900.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191630906.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n  质量分数小于20，错误率大于0.01的碱基，认为是不可靠的，如果这样的碱基超过reads的20%，就考虑将此条reads丢弃掉\n\n- Paired-End Reads 同时对序列两端较长的片段进行测序\n\n  <img src=\"http://img.peterli.club/joy/202210191630912.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n- 新一代测序的出现，促进了很多的相关领域的研究\n\n  <img src=\"http://img.peterli.club/joy/202210191630438.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- RNA-Seq：Explore the transcriptome\n\n  利用深度测序来研究转录组的技术——能让研究人员快速确定转录组\n\n  <img src=\"http://img.peterli.club/joy/202210191630457.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191630493.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"3.3 Predict with Hidden Markov Model","url":"/2022/3-3-Predict-with-Hidden-Markov-Model/","content":"\n# 3.3 用隐马尔可夫模型建立预测模型—— Predict with Hidden Markov Model\n\n<img src=\"http://img.peterli.club/joy/202210191620276.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n符号->状态路径\n\n对每个可能的状态路径计算其产生观测符号序列的可能性，其中概率最大的路径，也就是最可能产生这个串的路径。参考：[https://blog.csdn.net/GUET_DM_LQ/article/details/106244074](https://blog.csdn.net/GUET_DM_LQ/article/details/106244074)\n\n## 引例 The Most Simple Gene Predictor(MSGP)\n\n​\t\tGiven a stretch of genomic sequence, where are the coding regions and where are noncoding regions?\n\n给定一段基因序列，预测其中的编码区\n\n> ACCCTAACCCTAACCCTCGCGGTACCCTCAGCCCGAAAAAATCG\n\n解：\n\n1. 区分不能直接观测的状态和可以直接观测到的符号\n\n   可观测到 —— 给定的基因组序列\n\n   不可观测到 —— 编码和非编码\n\n   <img src=\"http://img.peterli.club/joy/202210191620281.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n2. 画出状态转换图 转移概率矩阵(Transition Probability)\n\n   <img src=\"http://img.peterli.club/joy/202210191620286.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n3. 生成概率(Emission Probability) (在编码条件下和非编码条件下，A,T,G,C出现的概率，这个需要根据大量的统计数据集进行统计分析或者预测分析) 参考：[https://blog.csdn.net/leianuo123/article/details/115832259](https://blog.csdn.net/leianuo123/article/details/115832259)\n\n   <img src=\"http://img.peterli.club/joy/202210191620291.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n4. 训练模型(Training the model)\n\n   - What we need to train?\n\n     — Transition Probabilities between states 状态间的转移概率\n\n     — Emission Probabilities for each state 每个状态的生成概率\n\n   - Estimate Probabilities from known \"Training set”\n\n     从已知的 \"训练集 \"估算概率\n\n     — An annotated genomic region, with coding/noncoding sequences labeled.\n\n     一个有注释的基因组区域，标有编码/非编码序列。序列要比较长，来保证充足的数据,来得到上面矩阵中的数据即编码和非编码的概率以及在编码和非编码的情况下的A,T,C,G的概率。参考：[https://blog.csdn.net/leianuo123/article/details/115832259](https://blog.csdn.net/leianuo123/article/details/115832259)\n\n     <img src=\"http://img.peterli.club/joy/202210191620296.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n     已知训练集的相关数据\n\n     <img src=\"http://img.peterli.club/joy/202210191620292.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n   - 根据这些数据来对一个未知的给定基因组序列反推出最可能的状态路径（概率最大的状态路径）\n\n     <img src=\"http://img.peterli.club/joy/202210191620568.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n     利用动态规划算法写出迭代公式以及最后的终止点公式\n\n     <img src=\"http://img.peterli.club/joy/202210191620601.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n   - 算 用对数 —— 乘法太慢 乘多了\n\n     <img src=\"http://img.peterli.club/joy/202210191620626.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n5. Testing Sequence: CGAAAAAATCG\n\n   <img src=\"http://img.peterli.club/joy/202210191620665.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   <img src=\"http://img.peterli.club/joy/202210191620727.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n   - n、c 红色数字： 根据已经注明编码非编码的核苷酸序列计算而来的，-0.097是非编码转换为非编码的概率，-0.699是非编码转换为编码的概率\n\n   - n ➡️ C -0.097 + -0.523 = -0.62 相加是因为log(a*b) = log a + log b\n\n     n ➡️ G -0.62 + -0.097 + -0.523 = -1.24 and -1.40 + -0.398 + -0.523  = -2.321\n\n   - 找到最’大‘值-7.774进行回溯\n\n     <img src=\"http://img.peterli.club/joy/202210191620812.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n应用：5’剪切位点的预测\n\n<img src=\"http://img.peterli.club/joy/202210191620836.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"3.2 Hidden Markov Model","url":"/2022/3-2-Hidden-Markov-Model/","content":"\n# 3.2 隐马尔可夫模型(Hidden Markov Model)\n\n## 引言\n\n之前\n\n<img src=\"http://img.peterli.club/joy/202210191612163.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n存在的问题：仅靠👆还不足以真正完成序列比对，因为现有的状态模型只是区分了空位状态X Y M，而没有考虑具体的残基\n\n<img src=\"http://img.peterli.club/joy/202210191612169.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n解决\n\n## 隐马尔可夫模型(Hidden Markov Model HMM)\n\n- The observable symbols (\"tokens\", y(t)) are generated according to their corresponding states (x(t))\n\n  可观察的符号（\"tokens\"，y(t)）是根据其相应的状态（x(t)）生成\n\n  在状态的基础上增加了符号的概念\n\n  <img src=\"http://img.peterli.club/joy/202210191612171.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- In addition to State Transition Probability, each state of HMM has a probability distribution over the possible output tokens(Emission Probability).\n\n  除了状态转移概率外，HMM的每个状态都有一个关于可能的输出标记的概率分布（生成概率）\n\n  除了状态转移概率之外，隐马尔可夫模型进一步引入了生成概率的概念，每个状态都有自己的生成概率分布，可以按照不同的概率产生一组可以被观测到的符号\n\n- Thus, a HMM is consist of two strings of information.\n\n- The state path\n\n- The token path (emitted sequence).\n\n  因此，HMM是由两串信息组成的。\n  — 状态路径\n  — 符号路径（生成序列）。\n\n  <img src=\"http://img.peterli.club/joy/202210191612171.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n- But the state path is not directly visible\n\n  但是与马尔可夫模型相比，HMM状态路径并不直接可见\n\n- Instead, we have to infer the underling state path, based on the observable token path.\n\n  相反，我们必须基于可观察到的符号路径推断出底层的状态路径\n\n  <img src=\"http://img.peterli.club/joy/202210191612174.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- 例：取值最大的那一条路径\n\n  <img src=\"http://img.peterli.club/joy/202210191612177.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- Given a HMM, a sequence of tokens could be generated as following:\n\n  - When we \"visit\" a state, we emit a token from the state’s emission probability distribution.\n\n    当我们 \"访问 \"一个状态时，我们从该状态的生成概率分布中发射一个令牌\n\n  - Then, we choose which state to visit next, according to the state’s transition probability distribution.\n\n    然后，我们根据该状态的转移概率分布选择下一个要访问的状态\n\n    <img src=\"http://img.peterli.club/joy/202210191612175.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n## 序列比对问题\n\n之前 没有考虑到残基\n\n<img src=\"http://img.peterli.club/joy/202210191612425.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n用隐马尔可夫模型来补回这一点\n\n- 用生成概率来处理残基\n\n  <img src=\"http://img.peterli.club/joy/202210191612451.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n- Sequence alignment with HMM\n\n  - Each \"token\" of the HMM is an aligned pair of two residues\n    (M state), or of a residue and a gap (X or Y state).\n    — Transition and emission probabilities define the probability of each aligned pair of sequences.\n\n      HMM的每个 \"令牌 \"是一对比对成功的两个残基(M状态)，或一个残基和一个间隙(X或Y状态)\n      — 转换和生成概率定义了每对比对序列的概率\n\n  - Based on the HMM, each alignment of two sequences can\n    be assigned with a probability\n    — Given two input sequences, we look for an alignment with the maximum probability.\n\n      在HMM的基础上，两个序列的每一次比对都可以被分配一个概率\n      — 给定两个输入序列，我们寻找一个具有最大概率的比对\n\n      <img src=\"http://img.peterli.club/joy/202210191612492.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191612537.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n- 隐马尔可夫模型的好处\n\n  - 有效的给出了序列比对的概率解释 —— ****Probabilistic interpretation****\n\n    <img src=\"http://img.peterli.club/joy/202210191612655.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  - 有助于用概率论的知识做概率论的分析 —— **Probabilistic inference**\n\n    - For example, to calculate the probability that a given pair of sequences are related by any (unspecified) alignment\n\n      例如，计算给定的一对序列通过任何（未指定的）比对方式相关的概率\n      —— Or, what's the best likelihood we can expect for given two sequences?\n\n      —— 或者，对于给定的两个序列，我们可以期待的最佳可能性是什么？\n\n    - Given the nature of HMM, many different state paths can give rise to the same token sequence\n\n      鉴于HMM的性质，许多不同的状态路径可以产生相同的符号序列\n\n      <img src=\"http://img.peterli.club/joy/202210191612702.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n      So we can simply sum up them together to get the **full probability of a given token sequence**.\n\n      所以我们可以简单地把它们加在一起来得到给定符号序列的全部概率\n\n      <img src=\"http://img.peterli.club/joy/202210191612784.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n      <img src=\"http://img.peterli.club/joy/202210191612810.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"3.1 From States to Markov Chains","url":"/2022/3-1-From-States-to-Markov-Chains/","content":"\n# 3.1 从状态到马尔可夫链\n\n## 马尔可夫链(Markov Chain)\n\n- A Markov chain describes a discrete stochastic process at successive times. The transitions from one state to all other states, including itself, are governed by a probability distribution\n\n  马尔科夫链描述了一个连续时间的离散随机过程。从一个状态到所有其他状态的转换，包括其本身，都由一个概率分布所支配\n\n- t时刻状态的概率分布由且仅由前有限个m时刻状态的概率分布来决定\n\n  ——M阶马尔可夫链\n\n  <img src=\"http://img.peterli.club/joy/202210191550373.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- A chain of random variables in which the next one depends (only) on the current one\n\n  一条随机变量链，其中下一个随机变量（仅）取决于当前随机变量。\n\n  当前的状态与且只与前一个状态相关 —— 一阶马尔可夫链\n\n  <img src=\"http://img.peterli.club/joy/202210191550376.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  ## 转移概率(Transition Probability)\n\n  <img src=\"http://img.peterli.club/joy/202210191553303.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191551012.png\" alt=\"image-20221019155106992\" style=\"zoom: 50%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.6 BLAST","url":"/2022/2-6-BLAST/","content":"# 2.6 BLAST算法\n\n- **Intro**\n\n  <img src=\"http://img.peterli.club/joy/202210191324694.png\" alt=\"image-20221019132436668\" style=\"zoom:50%;\" />\n- **Application**\n\n  <img src=\"http://img.peterli.club/joy/202210191312564.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n- **思路 —— BLAST Ideas: Seeding‐and‐extending：种子-扩展**\n\n  1. Find matches (seed) between the query and subject；寻找查询序列和目标序列之间的匹配（种子即高度相似的序列片段）\n  2. Extend seed into High Scoring Segment Pairs (HSPs)；将种子扩展成高分段对（HSPs）\n     – Run Smith‐Waterman algorithm on the specified region only.特定区域\n  3. Assess the reliability of the alignment.计算统计显著性，评估校准的可靠性\n\n  <img src=\"http://img.peterli.club/joy/202210191312566.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  - **Seeding**\n\n    For a given word length w (usually 3 for proteins and 11 for nucleotides), slicing the query sequence into multiple\n    对于给定的单词长度w（通常3是蛋白质和11是核苷酸用），将查询序列切成多个，生成小片段seed\n    continuous “seed words” 种子单字\n\n    <img src=\"http://img.peterli.club/joy/202210191312565.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n  - **Speedup: Index database 加速：索引数据库**\n\n    The database was pre‐indexed to quickly locate all positions in the database for a given seed.\n\n    数据库被预先索引，以快速定位数据库中某个特定种子的所有位置——对每个seed提前做索引\n\n    <img src=\"http://img.peterli.club/joy/202210191312566.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n  - **Speedup: mask low-complexity 加速：屏蔽低复杂度**\n\n    为了加速屏蔽了低复杂度区域，牺牲了灵敏度\n\n    <img src=\"http://img.peterli.club/joy/202210191312571.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n  - **质量评估——计算统计显著性，为了确保这个比对不是由随机因素引起的**\n\n    <img src=\"http://img.peterli.club/joy/202210191312814.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    - **E-Value：How a match is likely to arise by chance：匹配是如何偶然产生**\n\n      **E-value**用来代表随机出现也能匹配的个数，**所以是越小越好**，一般取0.05做cutoff。数据库(n)越大，序列(m)越长（blast是局部比对），E更有可能更大\n\n      （参考：[https://zhuanlan.zhihu.com/p/62342599](https://zhuanlan.zhihu.com/p/62342599)）\n\n      - The expected number of alignments with a given score that would be expected to occur at random in the database that has been searched\n        在已经搜索过的数据库中，具有给定分数的对齐的预期数量，预计会随机出现在数据库中\n        – e.g. if E=10, 10 matches with scores this high are expected to be found by chance\n        例：如果E=10，期望随机找到与该分数相匹配的10个匹配项\n\n        （参考：[https://blog.csdn.net/GUET_DM_LQ/article/details/106185880](https://blog.csdn.net/GUET_DM_LQ/article/details/106185880)）\n\n        <img src=\"http://img.peterli.club/joy/202210191312841.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n## summary\n\n- **WHY**\n\n  <img src=\"http://img.peterli.club/joy/202210191312867.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  - BLAST is the tool most frequently used for calculating sequence similarity, by searching the database.\n\n    BLAST是一个最常用的工具，通过搜索数据库来计算序列相似性。\n  - If you work with one or a few proteins or genes, it can tell you about their conservation, active sites, structure and regulation in other organisms, etc.\n\n    如果你研究一个或几个蛋白质或基因，它可以告诉你它们的保存情况、活性位点、结构和在其他生物体中的调节等\n- **What BLAST does?**\n\n  - ldentity: the occurrence of exactly the same nucleotide or amino\n    acid in the same position in aligned sequences.\n\n    一致性：在对齐的序列中的相同位置上出现完全相同的核苷酸或氨基酸\n  - Similarity: measure the sameness or difference of the sequences\n\n    相似性：衡量序列的同一性或差异性\n  - Homology: is defined in terms of shared ancestors. Homologous\n    sequences are often similar. Sequence regions that are homologous\n    are also called conserved regions.\n\n    同源性：是以共同的祖先来定义的。同源的序列往往是相似的。具有同源性的序列区域也被称为保守区\n- **不同的比对算法之间的区别**\n\n  <img src=\"http://img.peterli.club/joy/202210191312967.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  动态规划在搜索数据库的时候速度和占用资源会有很大的瓶颈，这就引申出了BLAST、FASTA，准确度不降低很多的情况下提升了速度\n- **How BLAST works**\n\n  <img src=\"http://img.peterli.club/joy/202210191312102.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  - **Step 0 —— Filtering**\n\n    <img src=\"http://img.peterli.club/joy/202210191312137.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  - **Step 1 —— Seeding**\n\n    <img src=\"http://img.peterli.club/joy/202210191312163.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - **Step 2 —— Search word hits**\n\n    <img src=\"http://img.peterli.club/joy/202210191312186.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210191312408.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  - **Step 3 —— Scanning**\n\n    <img src=\"http://img.peterli.club/joy/202210191312462.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  - **Step 4 —— Extending ➡️ HSP**\n\n    <img src=\"http://img.peterli.club/joy/202210191312497.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  - **Step 5 —— Significance evaluation**\n\n    <img src=\"http://img.peterli.club/joy/202210191312533.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210191312559.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **BLAST programs**\n\n  <img src=\"http://img.peterli.club/joy/202210191312581.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191312654.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **Gapped BLAST**\n\n  <img src=\"http://img.peterli.club/joy/202210191312769.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **PSI-BLAST**\n\n  <img src=\"http://img.peterli.club/joy/202210191312929.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n- **Caveat emptor**\n\n  <img src=\"http://img.peterli.club/joy/202210191312957.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n[讲人话](https://www.notion.so/3-8-BLAST-7fc9b0b9b3fd4d58a125b2dbf567b61f)\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.5 Sequence Database","url":"/2022/2-5-Sequence-Database/","content":"\n# 2.5 序列数据库\n\n- 序列数据库检索\n\n  - 原理\n\n    <img src=\"http://img.peterli.club/joy/202210191308590.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210191308583.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  - Step\n\n    <img src=\"http://img.peterli.club/joy/202210191308600.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n  \n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.4 Supplementary materials","url":"/2022/2-4-Supplementary-materials/","content":"# 2.4 补充材料 同源、相似性、相似性矩阵、点阵图\n\n## S1: Alignment with Affine Gap Penalty——对于空位罚分的改进\n\n第一节 罚分：区分 opening和 extending 并对此线性组合\n\n<img src=\"http://img.peterli.club/joy/202210191148729.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n第二三节 罚分：不区分opening 和extending 统一用 d\n\n<img src=\"http://img.peterli.club/joy/202210191148731.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n这一节 分三个状态M、X、Y\n\n<img src=\"http://img.peterli.club/joy/202210191148738.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n将之前的罚分分成了d和e两种，如果是第一个gap，罚分就为d，如果是接着前面的gap后面还是gap，那后面那个gap的罚分就是e，故叫做open gap即开场，叫做extension gap即延续\n\n<img src=\"http://img.peterli.club/joy/202210191148731.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210191148732.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n例：\n\n- 第一种\n\n  <img src=\"http://img.peterli.club/joy/202210191148741.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  当前的是Xi对比上Yj\n\n  1、如果前一个也是match对比成功，那就是前面的分加上替换矩阵对应的分\n\n  2、如果前一个是Xi对比上空位，那就是用前面X的分加上替换矩阵对应的分\n\n  3、如果前一个是Yj对比上空位，那就是用前面X的分加上替换矩阵对应的分\n  最终结果取这三者中的最大值\n- 第二种\n\n  <img src=\"http://img.peterli.club/joy/202210191148049.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  当前的是Xi对比上空位即gap\n\n  1、如果前一个是match对比成功，那就是当前空位是open gap 开头的gap 即罚分为d\n\n  2、如果前一个是Xi对比上空位，那就是当前空位是extension gap 是延续上一个gap的gap 即罚分为e\n- 第三种\n\n  <img src=\"http://img.peterli.club/joy/202210191148073.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n  当前的是Yj对比上空位即gap\n\n  1、如果前一个是match对比成功，那就是当前空位是open gap 开头的gap 即罚分为d\n\n  2、如果前一个是Yj对比上空位，那就是当前空位是extension gap 是延续上一个gap的gap 即罚分为e\n\n时间复杂度分析\n\n<img src=\"http://img.peterli.club/joy/202210191148168.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n从穷举的指数级别降低到了平方级别\n\n## S2 解释几种概念\n\n<img src=\"http://img.peterli.club/joy/202210191148274.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n- Homology & Similarity\n\n  - Homology\n\n    — derived from a common ancestor 来源于共同祖先\n\n    — ortholog: derived from speciation 直系同源 不同物种中的两个序列来自历史上的共同祖先的同一个序列\n\n    <img src=\"http://img.peterli.club/joy/202210191148298.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    — paralog: derived from duplication 旁系同源 同一物种中的两个序列在历史上来自同一个序列\n\n    <img src=\"http://img.peterli.club/joy/202210191148634.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n- Similarity Matrix——相似矩阵\n\n  - For nucleotides,\n\n    - usually only distinguish match / mismatch (identity matrix) for sequence alignment\n    - but a more complicated substitution model is used for phylogeny reconstruction\n\n    <img src=\"http://img.peterli.club/joy/202210191148655.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n  - For amino acids\n\n    - PAM (1978, Margaret Dayhoff)\n\n      - Two sequnences are 1 PAM apart if they differ in 1% of the residues\n      - 1 PAM = one step of evolution\n\n        <img src=\"http://img.peterli.club/joy/202210191148747.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n        <img src=\"http://img.peterli.club/joy/202210191148792.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n        <img src=\"http://img.peterli.club/joy/202210191148816.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n    - BLOSUM (1992, Steven Henikoff & Jorja Henikoff)\n\n      - computed by looking at “blocks” of conserved sequences found in multiple protein alignments\n\n      <img src=\"http://img.peterli.club/joy/202210191148905.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n- Dot Matrix\n\n  <img src=\"http://img.peterli.club/joy/202210191148934.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210191148960.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.3 From Global To Local","url":"/2022/2-3-From-Global-To-Local/","content":"# 2.3 从全局比对到局部比对 From Global To Local\n\n- 为什么要局部比对？\n\n  全局比对——对序列的全部残基进行比对\n\n  早期在蛋白质序列比对中有广泛的应用\n\n  后面发现造成的问题：\n\n  1. 发现功能相关的蛋白质之间，虽然可能在整体序列上相差甚远，却常常会有相同的功能域，这些序列片段能够独立发挥特定的生物学功能，但却在不同蛋白质之间相当保守，仅靠全局比对算法显然是无法发现这些片段的\n\n     <img src=\"http://img.peterli.club/joy/202210191131268.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n  2. 70年代内含子的发现，使得在做核酸水平的序列比对时必须要能正确处理内含子导致的大片段的差异\n\n     <img src=\"http://img.peterli.club/joy/202210191131278.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n  80年代初，开始认识到要有方法去认识到局部相似的序列，换句话说，我们要做local alignment 局部比对\n- 局部比对算法\n\n  - 局部比对和全局比对的区别\n\n    <img src=\"http://img.peterli.club/joy/202210191131279.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n  - 每步迭代的时候给分数加入了一个下限\n\n    <img src=\"http://img.peterli.club/joy/202210191131281.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n    负数都变成了0\n\n    大的值之间的竞争\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.2 Global Comparison using Dynamic Programming","url":"/2022/2-2-Global-Comparison-using-Dynamic-Programming/","content":"# 2.2 利用动态规划进行全局比对\n\n- 输入输出\n\n  <img src=\"http://img.peterli.club/joy/202210191017993.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n- 比对方法\n\n  - 穷举法(理论可行，实践难)：\n\n    <img src=\"http://img.peterli.club/joy/202210191017723.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  - 动态规划法\n\n    - **最好的比对 = 之前最好的比对 + 当前最好的比对**\n\n      <img src=\"http://img.peterli.club/joy/202210191016302.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n    - **全局最优解 = 局部最优解之和**\n\n      <img src=\"http://img.peterli.club/joy/202210191016308.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n    - **Step**\n\n      <img src=\"http://img.peterli.club/joy/202210191016299.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n    - **Formula**\n\n      <img src=\"http://img.peterli.club/joy/202210191016304.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n      <img src=\"http://img.peterli.club/joy/202210191016315.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n      例：\n\n      <img src=\"http://img.peterli.club/joy/202210191016316.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n      1. 边边的格子\n\n         <img src=\"http://img.peterli.club/joy/202210191016646.png\" alt=\"Untitled\" style=\"zoom: 25%;\" />\n\n         左方和上方对应的格子 0 + -5 = -5 -5 + -5 = -10 -10 + -5 = -15\n      2. 中间的格子\n\n         <img src=\"http://img.peterli.club/joy/202210191330829.png\" alt=\"Untitled\" style=\"zoom:25%;\" />\n\n         三个来源：\n\n         1、左方格子 + -5 = -10\n\n         2、上方格子 + -5 = -10\n\n         3、 斜上方格子 + 替换矩阵中的AA对应分数 = 2\n\n         <img src=\"http://img.peterli.club/joy/202210191016766.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n         取最大值为2 并标出来源指向箭头\n      3. 中间的格子\n\n         <img src=\"http://img.peterli.club/joy/202210191016787.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n         问：为啥会有两个箭头指向3？\n\n         答：说明有两个来源\n\n         左方格子 + -5 = -3\n\n         上方格子 + -5 = -15\n\n         斜上方格子 + 2 = -3\n      4. 从最后回溯得到最优比对结果\n\n         <img src=\"http://img.peterli.club/joy/202210191016790.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n\n         向上向左的箭头对应着空即 ‘-’，斜对角的箭头就对应着表格里的横纵双方\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"2.1 Basic Concepts in Sequence Alignment","url":"/2022/2-1-Basic-Concepts-in-Sequence-Alignment/","content":"\n# 2.1 序列比对中的基本概念\n\n## 引言\n\n* **正确使用计算机工具的作用**\n* **避免风险**\n* **BDMA**\n  <img src=\"http://img.peterli.club/joy/202210190845533.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n## 2.1.1 序列比对的作用和重要性\n\n* **序列比对的重要性**\n  <img src=\"http://img.peterli.club/joy/202210190845938.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n  \n  \n  \n* **序列比对的目的——生物角度**\n  **根据序列之间的功能或演化关系来检测生物之间的相似性**\n  <img src=\"http://img.peterli.club/joy/202210190845874.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  \n  \n  \n* **序列比对工具网站——**[Pairwise Sequnence Alignment](https://www.ebi.ac.uk/Tools/psa/)\n  <img src=\"http://img.peterli.club/joy/202210190845067.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n  \n  \n  \n  * **Step 1 输入蛋白质序列**\n    <img src=\"http://img.peterli.club/joy/202210190845687.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    \n    \n    \n  * **Step 2 设置参数**\n    <img src=\"http://img.peterli.club/joy/202210190845571.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    \n    \n    \n  * **Step 3 提交任务**\n    <img src=\"http://img.peterli.club/joy/202210190845246.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n    \n    \n    \n  * **Step 4 分析结果**\n    <img src=\"http://img.peterli.club/joy/202210190845442.png\" alt=\"Untitled\" style=\"zoom:33%;\" />\n  \n    \n    \n    * **比较两个残基之间的相似程度**\n      ![Untitled](http://img.peterli.club/joy/202210190845886.png)\n      💡 ‘｜’ ：相同\n    \n      **‘:’：比较相似 **\n      **‘.’：一点都不相似 **\n      **`</aside>`**\n      \n      \n  \n    **衡量标准：替换矩阵**\n    ![Untitled](http://img.peterli.club/joy/202210190845744.png)\n    \n    \n    \n    **特点：**\n    \n    1. **Symmetry **\n       * **得分结果是对称的，跟方向无关**\n       * **S ➡️ T 跟 T ➡️ S 的得分是一样的**\n    \n    2. **Context-insensitive**\n       * **得分结果与上下文无关**\n       * **各个残基之间是独立的**\n       * **只与两个残基之间的关系有关**\n    \n    3. **空位 序列片段插入与删除 对应着罚分**\n       ![Untitled](http://img.peterli.club/joy/202210190844211.png)\n    \n       **第二个空位 GA ➡️   - - 1个open gap 10’ + 1个extend gap 0.5‘ = 10.5’**\n       **第四个空位 - - - - - ➡️ TPDAV  1个open gap 10’ + 4个extend gap (5-1)*0.5‘ = 12’ **\n       ![Untitled](http://img.peterli.club/joy/202210190844580.png)\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"1 Introduction and History","url":"/2022/1-Introduction-and-History/","content":"# 1.1 什么是生物信息学\n\n## 引言 生命的奥妙\n\n* ![Untitled](http://img.peterli.club/joy/202210182238459.png)\n* **基因组**\n  ![Untitled](http://img.peterli.club/joy/202210182238013.png)\n* **生命之树**\n  ![Untitled](http://img.peterli.club/joy/202210182238645.png)\n* **解码基因组**\n  ![Untitled](http://img.peterli.club/joy/202210182238228.png)\n* **机遇与挑战**\n  ![Untitled](http://img.peterli.club/joy/202210182238079.png)\n* **定义**\n  ![Untitled](http://img.peterli.club/joy/202210182238108.png)\n  ![Untitled](http://img.peterli.club/joy/202210182238820.png)\n  **生物角度**\n  ![Untitled](http://img.peterli.club/joy/202210182239383.png)\n  **信息角度**\n  ![Untitled](http://img.peterli.club/joy/202210182239388.png)\n\n# 1.2 生物信息学历史\n\n* **分子生物学角度**\n  ![Untitled](http://img.peterli.club/joy/202210190833658.png)\n* **计算机发展角度**\n  ![Untitled](http://img.peterli.club/joy/202210190833017.png)\n* **生物信息的角度**\n  ![Untitled](http://img.peterli.club/joy/202210190833326.png)\n* **journals**\n  ![Untitled](http://img.peterli.club/joy/202210190833566.png)\n\n# 1.3 中国的生物信息学\n\n![Untitled](http://img.peterli.club/joy/202210190834894.png)\n\n![Untitled](http://img.peterli.club/joy/202210190834893.png)\n\n![Untitled](http://img.peterli.club/joy/202210190834898.png)\n\n![image-20221019083547559](http://img.peterli.club/joy/202210190835581.png)\n\n![Untitled](http://img.peterli.club/joy/202210190834891.png)\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"PKUBioInformatics","url":"/2022/PKUBioInformatics/","content":"# PKU BioInformatics\n\nB站视频：[https://www.bilibili.com/video/BV13t411G7oh?p=2&amp;spm_id_from=333.880.my_history.page.click&amp;vd_source=b60872cd374332ccebd93d8a3907fbc7](https://www.bilibili.com/video/BV13t411G7oh?p=2&spm_id_from=333.880.my_history.page.click&vd_source=b60872cd374332ccebd93d8a3907fbc7)\n\n- 框架图\n\n  <img src=\"http://img.peterli.club/joy/202210182227200.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n## 第一章 简介\n\n[1 Introduction and History](/2022/1-Introduction-and-History/)\n\n---\n\n## 第二章 序列比对\n\n[2.1 Basic Concepts in Sequence Alignment](/2022/2-1-Basic-Concepts-in-Sequence-Alignment/)\n\n[2.2 Global Comparison using Dynamic Programming](/2022/2-2-Global-Comparison-using-Dynamic-Programming/)\n\n[2.3 From Global To Local](/2022/2-3-From-Global-To-Local/)\n\n[2.4 Supplementary materials](/2022/2-4-Supplementary-materials/)\n\n[2.5 Sequence Database](/2022/2-5-Sequence-Database/)\n\n[2.6 BLAST](/2022/2-6-BLAST/)\n\n---\n\n\n\n## 第三章 HMM预测\n\n[3.1 From States to Markov Chains](/2022/3-1-From-States-to-Markov-Chains/)\n\n[3.2 Hidden Markov Model](/2022/3-2-Hidden-Markov-Model/)\n\n[3.3 Predict with Hidden Markov Model](/2022/3-3-Predict-with-Hidden-Markov-Model/)\n\n---\n\n\n\n## 第四章 高通量测序\n\n[4.1 Next Generation Sequencing](/2022/4-1-Next-Generation-Sequencing/)\n\n[4.2 Sequence Reply and Mutation Identification](/2022/4-2-Sequence-Reply-and-Mutation-Identification/)\n\n[4.3 Analysis and Demonstration](/2022/4-3-Analysis-and-Demonstration/)\n\n[4.4 Supplementary materials](/2022/4-4-Supplementary-materials/)\n\n\n---\n","tags":["BioInformatics","Learning Note"],"categories":["Learning Note","BioInformatics","PKUBioInformatics"]},{"title":"Alphafold2","url":"/2022/Alphafold2/","content":"# Alphafold2\n\nTitle: Highly accurate protein structure prediction with AlphaFold\n\nAuthors: Jumper, John\n\nDOI: https://doi.org/10.1038/s41586-021-03819-2\n\nDate: October 11, 2022\n\nFinish time: 2022/10/15\n\nFuture: 应用于别的很多领域，药物靶点啥的\n\nMeaning: 解决了生物学中存在了50年的难题，二维氨基酸序列预测3D蛋白质结构\n\nTheoretical/Conceptual Framework: Transformer\n\nYear: 2021\n\n关键词: AI,Bioinformatics\n\n期刊杂志: nature2021\n\nReference: 【AlphaFold 2 论文精读【论文精读】https://www.bilibili.com/video/BV1oR4y1K7Xr?vd_source=5ec85dfc5468a21a485b1b1d4d271219\n\n# 标题&作者\n\n## Highly accurate protein structure prediction with AlphaFold\n\n使用**AlphaFold**进行非常精确的蛋白质结构预测\n\n与**RoseTTAFold**(****Accurate prediction of protein structures and interactions using a three-track neural network****)同一天发表\n\n前面四行作者后面都写了个4，4的意思是说这些作者的贡献是一样的，第一个和最后一个是通讯作者\n\n![Untitled](http://img.peterli.club/joy/202210182105664.png)\n\n# 摘要\n\n前面大篇幅介绍问题和解决这个问题的重要性，介绍实验的结果，最后很简单的提了一下他们的算法\n\n- 蛋白质结构的重要性决定功能\n\n  蛋白质通常是指比较长的一串氨基酸序列，比如100个氨基酸，那么长一串不稳定，就会卷在一起，卷完之后比较稳定，因为这些氨基酸之间相互吸引，然后形成了一个独特的3D结构，结构的形状决定了蛋白质的功能\n\n  氨基酸序列可以认为是一段代码，要让他干活的话就要把它编译成一个可执行的文件，序列卷起来的过程就可以认为是把代码编译成可执行文件，你的代码就决定了你最后的可执行文件的样子\n\n  有时候你会发生错乱，比如说一个序列可以卷成两个不同的形状，这时候通常会带来疾病，所以正常情况下都是唯一决定的\n\n  这里的工作就是，这个代码是怎么变成这个可执行文件的，这个叫做蛋白质结构的预测。我给你一串氨基酸序列，你去预测3D结构长什么样子\n- “protein folding problem”——蛋白质结构预测的困难\n\n  目前为止我们大概知道10万左右的蛋白质结构，但是我们已知的蛋白质有将近10亿种，所以我们只对很少的一部分的蛋白质了解它的功能，目前我们可能需要数月或者数年的时间才能了解一个蛋白质的结构\n\n  具体来说就是，把这个蛋白质动起来，从不同的角度用显微镜去看他的投影，再还原出他的3D结构，这个费时又费力，所以我们需要更精确的基于计算的办法，能够更便宜更方便的得到一个蛋白质的结构，这个问题叫做蛋白质折叠问题，这个问题在生物学里面存在了50年了\n- 现在方法的问题\n\n  alphafold1精度不够，不在原子级别的精度，也就是说，你对一个氨基酸位置的预测和实验真正测到的那一个结果，偏差不在一个原子大小的级别\n\n  特别是你不知道跟这个蛋白质功能比较，预测的其它蛋白质的结构的时候，这个精度会更差\n- 介绍了alphafold2\n\n  - 我们终于提供了第一个能达到一个原子精度预测的模型，即你预测的位置和你真实测到的位置之间的偏差在一个原子大小的级别\n  - 我们用这个模型参加了CASP14竞赛(这是两年一届的比赛，科学家把最近两年里面测出来结构的蛋白质但是没公布的拿出来做竞赛，看一下你跟实验室测得了精度的比较)，得到了大家的认可\n  - alphafold2是一种比较新奇的机器学习的算法，使用了物理和生物学的一些知识，同样也用了深度学习算\n\n<img src=\"http://img.peterli.club/joy/202210182105730.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n> 经验：这是一类应用型的文章，你主要的目的是用机器学习这个工具解决这个领域里面一个重要的问题，所以你关心的是两点：1、这是一个什么样的问题，这个问题重不重要，对这个领域来说有什么意义，2、你结果的好坏，你是不是解决了这个问题，具体来说你用的模型的好坏并不重要。比如说alphafold2就是一个很简单的mlp，只要你也能达到原子级别的精度，那你一样的能上nature封面。这个和你做一个特别复杂的模型，得到同样的结果，可能共识上来说是差不多的。反过来说，如果你能用很简单的算法解决这个问题的话，很有可能已经轮不到你了，别人就已经解决了。所以你就有两个办法，要么你找一个新的问题，别人都没用机器学习来做过，你把它变成一个机器学习的问题，收集数据可能用简单的模型也就行了，要么你的问题数据已经摆在那里了，之前而且有举办过竞赛，大家都做了很多不一样的方法，这时候你就需要用一个很不一样的很强大的算法，在算法上创新才行呢\n\n# 导论\n\n摘要里故事的详细版本，讲了一下问题，讲了一下实验结果，讲了一下具体的数字，没有讲太多的算法\n\n看一下图\n\n- 这次比赛的结果\n\n  x轴：参赛队伍\n\n  y轴：在某一个特定置信区间里面，你的平均预测的位置跟真实的位置的一个平均的区别。\n\n  单位：Å($10^{-10}$)\n\n  alphafold平均误差在1Å左右，别人基本上只在3Å，一个碳原子的大小大概是1.5Å的样子，所以可以达到一个原子精度的误差\n\n  <img src=\"http://img.peterli.club/joy/202210182105757.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n  > 经验：表达自己的成果的重要性时，把你的结果换算成一个大家能理解的一个概念，而不是一个纯粹数字的概念，大家对数字的敏感性远远不如对一个常见事物的比较的敏感度来的高\n  >\n  > - “我比别人的结果都要好三倍” “极大的改善了别人的结果” ➡️  “我达到了原子精度的结果”\n  >\n\n  - “我的图片识别比别人好了10%” ➡️ “我的图片识别别人类识别的精度还要高”\n- 实验结果\n\n  绿色表示实验室做出来结果，蓝色表示预测\n\n  ![Untitled](http://img.peterli.club/joy/202210182105881.png)\n\n  在PDB数据集(目前为止所有我们了解的蛋白质的结构都放在这个数据集里)上的预测结果\n\n  a:   x轴：预测的误差\n\n  y轴：有多少比例的蛋白质你的误差在这个里面\n\n  可以看到一半以上的预测在原子精度里面\n\n  <img src=\"http://img.peterli.club/joy/202210182105951.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n![Untitled](http://img.peterli.club/joy/202210182105065.png)\n\n![Untitled](http://img.peterli.club/joy/202210182105191.png)\n\n# ✨模型介绍和训练\n\n整个模型和训练的细节还是比较复杂的，但是就那么一点点篇幅，不可能在这个篇幅里面把整个东西给你写清楚，它采用的策略是说，大概给你讲一下这个模型是干什么，还是基于你的整个神经网络不那么了解的前提下，然后再高亮了一下他们的贡献，他们做的东西跟别人东西有什么不一样的地方，以至于你知道它的每个词是什么意思，但是读完下来也不知道这个模型是什么样子\n\n- 算法总览图\n\n  ![Untitled](http://img.peterli.club/joy/202210182105280.png)\n\n  输入：一个蛋白质的氨基酸序列，画了个人，表示是人的，每一个点表示一个氨基酸\n\n  输出：对每个氨基酸序列预测它在三维空间中的位置\n\n  概论：算法结构就分为了三个部分，拿到一个我们想要预测结构的蛋白质序列之后\n\n  1. 先抽取特征主要是两大类特征，一个是氨基酸序列特征，一个是氨基酸对之间特征\n  2. 进去完之后有一个编码器和解码器，编码器是一个transformer编码器的一个改进版本，因为他要处理两个不一样的输入，而且每个输入里面在行和列上是都有关系的，还要在里面不断的对两个模块进行数据的交互\n  3. 做完编码之后再进入解码器，解码器就是根据编码器的信息去还原出每个氨基酸里面在3D中的一个位置，它仍然是基于注意力的机制，但是他在里面显式的对位置信息进行了建模\n\n     这个地方更像是一个RNN的架构，这也多少解释了为什么他的编码器叫做evoformer，听上去就是一个Transformer的变种，但是他的解码器就是简简单单叫做结构模块\n  4. 然后还有一个回收机制，能把最后3D的结构的信息和编码器的输出都可以加回到编码器的输入里面，然后重复4次，就可以做成一个更长一点的网络，但是因为不进行误差反传，只是在计算上贵了4倍，但是在内存上并没有太多的瓶颈。\n\n  细节很多，附录里面有大概30个算法\n\n  <img src=\"http://img.peterli.club/joy/202210182105320.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n  <img src=\"http://img.peterli.club/joy/202210182105419.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n- 算法结构\n\n  - feature——抽特征\n\n    输入序列进来\n\n    1. 直接导入到最后的神经网络\n    2. 去一个基因数据库里面搜一搜，看一看跟这个蛋白质的序列有没有相似的蛋白质的序列。\n\n       比如说这个地方他发现了在鱼、兔子、鸡身上有相似片段的蛋白质，把它拿出来形成一个MSA(Multi sequence alignment 多序列比对)\n\n       具体来说，拿出来相似蛋白质，一一比对氨基酸，这个地方是相似而不是一样，就意味着说有一些片段是一样的，有些片段不一样，那么需要这些相似的片段或相似的氨基酸跟你一一的对起来(字符串匹配的过程)\n    3. 另一个重要的特征是氨基酸之间的关系。蛋白质卷起来是因为氨基酸之间原子的相互作用，那么我们这也可以表示每两个氨基酸之间的一些关系，如果蛋白质的长度是2的话，就会整成一个2*2的表，每一项表示的是一对氨基酸之间的关系\n\n       当然最好的是能表示成在卷完之后这两个氨基酸在3D空间之间的距离\n    4. 另外额外的特征是，把这个序列放进结构数据库里面去搜。我们知道很多蛋白质的结构信息，那我们就知道真实的氨基酸对之间在空间中它的距离什么样子，这样子就得到了很多模板\n\n    抽完特征之后，我们得到两大类特征，第一个是不同序列的特征，第二个是氨基酸之间的一类特征\n\n    这两大类特征拼上我们后面来的一些东西(后讲)，就进入我们的编码器\n\n    <img src=\"http://img.peterli.club/joy/202210182105481.png\" alt=\"Untitled\" style=\"zoom: 67%;\" />\n  - encoder —— 编码器，将输入的特征编码成神经网络想要的形状\n\n    - 输入：两个三维的向量\n\n      1. 有s行，s表示的是你一共有多少个蛋白质，第一个是要预测的人类的蛋白质，后面s-1个是我们在数据库里面匹配而来的s-1个蛋白质\n\n         长度是r，表示这个蛋白质里面一共有r个氨基酸\n\n         长度为c，表示我把每一个氨基酸表示成一个长为c的一个向量。对于图片来讲，就是每一个像素的通道数，对于文本来讲，对应的概念就是每一个词对应的那一个嵌入的长度\n\n         ![Untitled](http://img.peterli.club/joy/202210182105512.png)\n      2. 一个氨基酸对的表示\n\n         一共有r个氨基酸，所以就是一个r乘以r\n\n         每一个氨基酸对也是用一个长为c的向量来表示他的特征\n\n         ![Untitled](http://img.peterli.club/joy/202210182105535.png)\n    - Evoformer(48 blocks)——Transformer变种\n\n      - 不同点\n\n        1. 不再是一个序列里面关系，现在是一个二维之间的关系\n\n           - MSA\n\n             原来，不管是对文字还是图片来说输入都是序列；\n\n             现在，不再是一个序列，而是一个二维矩阵，在每一行上面可以认为就是这一个氨基酸的序列，列里面也是有关系的，因为这是同一个氨基酸在不同的蛋白质里面的一个表现，所以你的关系不再是一个一维的序列，而是一个二维的矩阵了\n           - Pair\n\n             每一行表示的是一个氨基酸跟别的氨基酸的关系\n\n             在列上面也表示的是一个氨基酸跟别的氨基酸的关系，这个地方有一定的对称性\n\n             而且如果你认为这个对表示就应该表示这氨基酸在3D空间中的距离的话，那么这个地方你还有些物理上的限制，比如说你要满足一个三角关系\n        2. 现在输入的是两个不同的张量\n\n           之前就输入一个，现在输入两个把数据融合起来\n      - 相同点\n\n        transformer里面基本上就是一些transformer块，每个块一个输入进去，输出应该是跟输入一样的，但是抽取了他们元素之间的相关信息，然后通过48个块之后，最后得到的输出跟输入之间的大小没有发生变化\n\n        但是输出那个地方里面的那些向量就是c那个维度，已经很好的表征了每个氨基酸之间他们的关系，这就完成了编码器的输出\n\n    <img src=\"http://img.peterli.club/joy/202210182105598.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210182105622.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n    <img src=\"http://img.peterli.club/joy/202210182105698.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n  - decoder\n\n    解码器，将编码器的输入最后变成一个3D的位置。\n\n    解码器拿到编码器的输出，具体来说，拿到要预测的那个人类蛋白质里面氨基酸所有的特征的表示以及氨基酸之间的相关信息\n\n    根据这些相关信息对每一个氨基酸来预测它的3D位置，最后得到我的输出\n\n    <img src=\"http://img.peterli.club/joy/202210182105729.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n  - Recycling(回收机制)\n\n    1. 他把编码器的输出和解码器的输出都通过回收机制变成了编码器的输入\n    2. 大概意思是说，做完这一次预测的时候，可能结构还不是那么的准确，把这个不是那么准确的结构传回我们的输入，然后在他的基础上再做一次精调，希望精度会更好，有点像循环神经网络的机制，可以认为把这个模型循环了3次，变成了一个四倍更深的一个网络，这样子达到更好的精度\n    3. 但是他比复制三次的好处在于是说，在网络里面是共享权重的，也就是说下次再做这个东西的时候，权重还是基于前面的\n    4. 另外一个是，在做回收的时候，我们的梯度是不反传的，也就是说正常从前面进来之后，损失函数在最后输出3D结构的地方就要往回了，只要经过48+8=56层，就可以计算梯度了\n\n       但如果你允许梯度在回收时候传递的话，也就是像循环神经网络一样，在一条长的序列里面传递梯度的话，那么你要经过56*4=224层才能进行梯度反传，内存有可能会吃不消，内存消耗率跟层数是线性关系\n\n    ![Untitled](http://img.peterli.club/joy/202210182105776.png)\n- 架构具体图\n\n  - encoder\n\n    通过自注意力模块来对每个元素之间的关系进行建模，然后通过一个全连接层来做一个信息的空间的转换，不同是说输入里面既有行的信息，也有列的信息，所以这些模块都是成双成对的出现的\n\n    然后有两个输入，分别表示的是氨基酸在一个序列中的信息和氨基酸对之间的信息，需要做一些信息的传递，使得这两块之间能够做信息的融合\n\n    1. 对空间信息建模\n\n       ![Untitled](http://img.peterli.club/joy/202210182105812.png)\n\n       - 跟transformer的非常相似。\n\n         首先进入的是一个多头自注意力的模块，同样道理有残差连接，然后就是一个在元素上作用的MLP\n       - 不同点\n\n         1. 氨基酸对之间的信息，会通过绿色这条路加入到对序列的建模里面，序列的信息也会通过橙色这条路来反馈到氨基酸对的信息上面，所以就完成了一次信息的交互，而且是每一块里面都有信息的交互\n         2. 每一个自注意力机制更多了，因为之前他只要在一个序列里面就做了，所以这里的话我们有个二维的关系\n\n            紫色框框处理的是按行的序列信息\n\n            绿色框框处理的是按列的序列关系\n\n            粉色框框是元素对之间的关系，要满足一些之前说到的物理关系，比如说三角不等式之间的关系，所以他也通过这个性质来控制了你在做自注意力的时候，他们是怎么样去设计你的query、key和value\n\n       <img src=\"http://img.peterli.club/joy/202210182105885.png\" alt=\"Untitled\" style=\"zoom:150%;\" />\n\n       - MSA(row-wise gated self-attention with pair bias)\n\n         按行的 带门的 自注意力机制 带按对的偏移\n\n         - 按行的\n\n           - 在MSA里面每一次拿出一行做成一个序列，但我们知道里面的每一个元素就是一个氨基酸，有一个长为cm长的长度作为他的特征，那么就可以用最原始的多头自注意力了\n           - 具体来说就是对里面每一个元素做投影，来得到我们的query、key、value，q k v长度都是等于c的，而且是多头的话我们要一共做h次，query和key做点乘，来计算他们的相似度\n           - 通过一个softmax，计算他的自注意力权重，然后拿注意力分数跟你的value做乘法，得到我的输出\n\n             因为是多头的话，所以这个地方一共要进行h次，可以看到这里是有多个矩阵，把这些输出全部跟你并在一起，最后做一次线性的投影，就得到我们的输出了，就是得到这一行他的对应的新的值\n         - 带门的\n\n           - 多了一行红色框框的东西，他对每个元素跟之前一样也做一个线性投影，然后用一个sigmoid函数将这个输出变成0和1之间，在每个头里面都做一次这样子的计算，然后跟你这个输出做点乘起来，按元素的乘法，所以这就完成了门这个操作\n           - 红色框框这一个线性投影让我们学习一个方法，使得我们来控制哪些元素应该输出出去，哪些不应该输出\n           - 输出出去的意思是说，我这个元素的值比较大，我做完sigmoid的时候变成了1，那么就是我对这个输出不改变，让他放他出去\n           - 如果对某个元素我不想把它放出去的话，那么学习一个比较小的值，通过sigmoid之后，基本上把它变成0，0乘以这个元素的话就变成0了，就能实现控制，哪些应该输出，哪些不应该输出，这就是门的意思\n         - 对偏移\n\n           - 在算注意力分数的时候，对每个q和每个k做内积得到他的相似度，也就是说作为query的那一个氨基酸和作为key的那一个氨基酸，计算他们的相似度。但是pair representation这个地方，我们还额外的建模了每一个氨基酸对他们之间的关系，所以信息跟dot-product干的事情有一定的相似度\n           - 假设我要计算第i个氨基酸作为q，然后第j个氨基酸作为key的时候，我可以把它对应的那一个第i行第j列的那一个元素拿出来，这里也表示了他们之间的关系，通过一个线性投影投影到一维，然后作为一个偏移加进去\n\n             就是之前做的那个dot-product内积再加上这一个偏移，然后再进入softmax，这样我的注意力分数既来自于我们这个投影算出来的相似度，也来自于我们额外建模的相似度，这个就是pair bias要干的事情\n\n         所以看到他就是在原始的多头注意力上面做了三个改进，用来建模我的MSA里面的每一行的序列信息，以及能用上我们之前建模的按对的那些信息\n\n         ![Untitled](http://img.peterli.club/joy/202210182105023.png)\n\n         - 伪代码\n\n           - 把张量表示成向量的一个集合\n\n             $m_{si}$表示的是第s个序列中间的第i个氨基酸对应的那一个向量他的长度是cm\n           - $z_{ij}$表示的是第i个氨基酸和第j个氨基酸，他们之间的关系的向量表示也是cm\n           - c是一个超参数，表示我每个头里面投影的大小是32，所以算法是一个多头注意力，有一个投影步骤，一个注意力步骤和一个输出步骤\n\n       ![Untitled](http://img.peterli.club/joy/202210182105087.png)\n\n       - MSA(Column-wise gated self-attention)\n\n         他跟之前唯一的区别是说，在之前是按行的，也就是只见过的每一行里面，每一个序列里面氨基酸之间的信息\n\n         现在要建模每一个列里面的信息，也就是同样一个氨基酸，但是在不同的蛋白质之间，他的表示之间的一个相关性\n\n         如果把输入那一列拿出来之后，剩下的部分跟之前是没有区别的，除了没有把对信息作为一个偏移加进去之外，其他都是一样的\n\n         ![Untitled](http://img.peterli.club/joy/202210182105136.png)\n\n         ![](http:/img.peterli.club/joy/202210182125129.png)\n\n       做完前面两个MSA块，我既对于每一行里面序列信息做了建模，也对每一列信息里面做了建模，虽然我们不是对整个行列里面所有信息同时建模，通过两个模块，计算复杂度相对来说是比较低的\n\n       - MLP模块(Transition)\n\n         - Transformer里面自注意力机制主要的作用是去混合不同元素之间的信息，真正的做信息的提炼还是在MLP模块里面\n         - 简单说就是对每一个元素，通过一个全连接层，投影到原始的4倍长的大小，4这个常数也是来自于Transformer里面，然后再通过一个ReLU的激活层，再投影回原来大小\n         - 线性层的权重对每一个元素是共享的，也就是对每个元素，我们用同样的全连接层进行投影，等价于transformer里面的那个MLP模块\n\n         你就想象的把每一个氨基酸这个矩阵拉成一条长的向量就行了，这是等价的\n\n         ![Untitled](http://img.peterli.club/joy/202210182105183.png)\n\n       这就完成了对MSA它里面空间信息的一个建模\n\n       ![Untitled](http://img.peterli.club/joy/202210182105228.png)\n    2. 对对信息进行建模\n\n       来自MSA氨基酸序列的表示是怎么样通过这个紫色的模块融入到氨基酸对的表示？\n\n       - 氨基酸i在MSA序列里面表达它是一个矩阵，具体来说，氨基酸i在每一个蛋白质里面，它是用一个长为cm的象征来表示的，然后这里一共有s条蛋白质，所以呢就表示成一个s乘以cm的一个矩阵来表示i这个氨基酸，同样道理对于氨基酸j来说，也是一个s乘以cm的矩阵来表示\n       - 在对表示里面，从氨基酸i连接到氨基酸j的这一个表示是一个长为cm的向量，那么意味着说我们要把两个矩阵转换成一条向量\n       - 具体来说，将这两个矩阵投影到c维，就得到一个s*c的矩阵，然后做外积得到(s,c,c)的张量，可认为是他先把这个向量加一个维度变成一个(s,c,1)的一个三维的张量，下面变成(s,1,c)一个三维的张量，这两个张量做batch 的dot，得到结果是(s,c,c)的张量，最后两个维度c c表示的是氨基酸i和氨基酸j的一些交互的信息，s表示的是这里一共有s个蛋白质\n       - 但我们关心的是氨基酸i到氨基酸j的信息，所以下面他取了一个均值，在s的这个维度上就得到一个c * c 的矩阵了，最后把这个矩阵拉直，然后再投影到一个长为cm的维度，就会得到一个长为cm的一个向量，把这个向量加回到最右边那里\n\n         这样就把来自MSA里面的序列的信息加入到了对表示里面，就完成了这一个信息的融合的操作\n\n       ![Untitled](http://img.peterli.club/joy/202210182105478.png)\n\n       <img src=\"http://img.peterli.club/joy/202210182105523.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n       ![Untitled](http://img.peterli.club/joy/202210182105546.png)\n    3. 对氨基酸对之间的关系做一些建模\n\n       <img src=\"http://img.peterli.club/joy/202210182105550.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n       后面红色的两块跟前面对空间信息建模的红色两块是很像的\n\n       - Triangle self-attention around starting node\n\n         跟前面很像，走势很像\n\n         ![Untitled](http://img.peterli.club/joy/202210182105597.png)\n\n         ![Untitled](http://img.peterli.club/joy/202210182105601.png)\n\n         - 输入只有$z_{ij}$，氨基酸对的表示，已经不再有前面的序列的表示了，仍然是一个自注意力的一个函数，函数有三个部分，一部分是对输入做投影，然后计算注意力分数和你的注意力输出，最后把每一个头之间全部连接起来，投影得到最终的输出\n         - 要计算的是$z_{ij}$的输出，那么就是第$i$行里面第$j$个元素，然后我们把它对应的这个query，$q_{ij}$跟所有这一行里面就是第$i$行里面别的那些元素对应的key，然后做内积，就可以得到自注意力的一个分数\n\n           同样跟之前一样加上一个偏移，这个偏移是$b_{jk}$，在每一行里面做softmax，得到结果\n         - 这个公式跟之前是一模一样的 ，就是物理意义不太一样\n\n           设计的是三个氨基酸$i,j,k$，$q_{ij}$表示的是氨基酸$i$到氨基酸$j$的这一个信息，$k_{ik}$表示的是氨基酸$i$到氨基酸$k$的这一个信息，他们做内积表示的是这两个边之间的一个关系\n\n           从$j$到$k$的这个偏移，加上偏移的意思是说想让你的自注意力机制在计算相似度的时候去看这个三角的关系，就是$i$到$k$的这个距离应该是小于$i$到$j$再$j$到$k$的这个距离，这就是叫做triangular的原因，别的跟之前的就没有太多区别\n         - 得到注意力分数之后，跟他对应的那一个v相乘，然后在每一行就是遍历k，所以相加得到我的自注意力的输出，然后再乘以我的门得到我最终的输出，最后的话我们就是把它连接在一起投影下去就行了\n       - Triangle self-attention around ending node\n\n         - 之前是按行做自注意力，现在变成了按列做自注意力，跟之前下标不一样\n\n           $q_{ij}$是第$j$列里面的第$i$个元素，我们需要把这个第$j$列里面所有的别的元素，所以$k$是从1到$r$里面全部遍历一遍，然后相乘，最后同样道理的话他也是做的是$b_{ki}$，$v_{kj}$，跟之前的相比就是按行做softmax，变成按列做softmax\n         - 所以经过红色这两个模块之后，在按行层面和按列层面都做上了信息的融合，但是有一个先做和后做的关系，导致他的结果是不对称的，所以导致第$ij$和第$ji$的向量是不相等的\n\n           但是没关系，因为我们用的是一个有向图，$ij$就表示氨基酸$i$在氨基酸$j$的前面，$ji$就表示氨基酸$j$在氨基酸$i$的前面，所以两个氨基酸在序列中的位置一调换，导致他们的关系发生变化也是没问题的\n\n         ![Untitled](http://img.peterli.club/joy/202210182105647.png)\n       - triangle update using outgoing edges\n\n         <img src=\"http://img.peterli.club/joy/202210182105768.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n         - 跟红色的一样，也是想对氨基酸对信息之间进行一些交互，但是这个模块的设计比后面的自注意力要简单很多，作者本来是想用计算量相对来说比较小的模块来替换掉自注意力，使得整个计算更简单一点\n         - 干这个事情是因为它使用的一个蛋白质序列里面氨基酸的那个个数大于你有序列的个数的，r>s，导致pair输入的大小比MSA输入大一些\n         - 主要的计算是来自于自注意力层的计算，所以如果能用便宜的模块来替代他的话，那么整个算法的计算量会降低\n\n           但是作者又发现如果只使用黄色这两个模块的话，那么精度不如在后面再加上红色这两个模块来的好\n\n         ![Untitled](http://img.peterli.club/joy/202210182105818.png)\n\n         ![Untitled](http://img.peterli.club/joy/202210182105821.png)\n\n         - 首先对所有输入做一个Layer Norm\n         - 然后对每一个对的向量$z_{ij}$，计算一个a和一个b的版本，具体的计算是先把$z_{ij}$投影到$c$就是128的维度，在ab那做了个门，就是把它做另外一次投影然后加一个sigmoid，控制的是相乘的那个Linear($z_{ij}$)里面哪一些元素我需要，哪些元素就阻止掉，不让他进去了\n         - $g_{ij}$也是一个输入的门，$z_{ij}$等于这个门再加上$a_{ik}$和$b_{jk}$做点积，然后在$k$上做求和，做一个LayerNorm再做一个投影\n\n           具体看一下是什么意思，$z_{ij}$要计算的是氨基酸$i$到氨基酸$j$的这条边，他的更新是说，我们对所有的氨基酸$k$，把$a_{ik}$拿出来，对$a_{jk}$和$b_{jk}$两条边做一个按元素的点乘，然后用一个LayerNorm把它normaliz一下，然后$z_{ij}$的信息由所有的这些$ik$、$jk$这些对的信息汇总而来\n       - triangle update using incoming edges\n\n         - 跟之前唯一的区别就是从$a_{ik}$变成了$a_{ki}$，$b_{jk}$变成了$b_{kj}$，i与k互换，j与k互换，意味着这个边，箭头从往外伸变成了往回伸，这就完成了一个对称性的一个交换\n         - 就是我既汇聚了出去的边的信息，也汇聚了回来的边的信息，所以这两个黄色的模块和后面红色的模块有一定的相似度，也就是对氨基酸对之间做一些消息的传递\n\n         ![Untitled](http://img.peterli.club/joy/202210182105876.png)\n       - MLP模块\n\n         具体来说，就是对于每一个元素里面的这个向量，都用同样一个MLP对他做一次投影，这就完成了对所有氨基酸对的表示的一个变化\n  - decoder\n\n    - 蛋白质的3D结构\n\n      - 最简单的表达方式就是把它在3D空间中的坐标记录下来，那就是三个值，然后我们对每个原子去预测这三个值就行了\n      - 一个蛋白质你把它在空间中进行旋转好位移也好，是不会影响它的结构的，但是任何的旋转或者位移，都会导致3D空间的绝对位置发生变化，所以如果你有绝对位置的话，那么你对平移、旋转这样的变化是不友好的，所以他这里用到的方法是相对位置\n\n      <img src=\"http://img.peterli.club/joy/202210182105920.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n\n      - 具体来说可以认为一个蛋白质就是n个氨基酸串起来了，中间这条网络叫做主干网，边上就是一些枝\n      - 所谓的相对位置是说，下一个氨基酸相对于上一个氨基酸的位置而言是怎么样的，只要我们知道所有这种相邻位置是怎么变换，给我任何一个起始点，都能还原出这条主干网络，用的是欧几里得的变换或者叫做刚体变换\n      - 假设这个点在空间中的位置是x，x是一个长为3的向量，这个点的位置是y，那么怎么从x变换成y呢？\n\n        在欧几里得里面可以写成，y可以等于对x做旋转就是一个R，再加上一个偏移，这个地方R是一个3*3的一个矩阵，t是一个3*1的一个向量，只要能确定r和t的值的话，那就能从x还原到y\n      - 这里的好处是说，如果我们对这个蛋白质的整体的结构做旋转或位移，就是把一个全局的r或者t加在整个蛋白质上面的话，他不会影响这些局部的变换，就是导致x变成y的这个r和t不会发生变化，这样这个表示就是对于全局的刚体变换是无关的\n      - 确定了主干的结构之后，上面的每一个氨基酸的下面这些部分可以认为是在每一个可以变换的原子那个地方，我只要去关心他旋转的角度是什么就行了，只要头是固定的话，只要关注那些能够转的地方，到底转了多少就行了，这些变化还是要满足物理和生物学的定义的\n      - 比如说两个氨基酸只能通过一些特定的角度连在一起，你可以做的办法是说在训练的时候，我可以限制住我的这些r和t可以要选取的值，或者是我在训练完之后再做一步校验，验证是这个结构在物理上和生物上是可行的\n\n      <img src=\"http://img.peterli.club/joy/202210182105943.png\" alt=\"Untitled\" style=\"zoom:50%;\" />\n    - decoder结构\n\n      ![Untitled](http://img.peterli.club/joy/202210182105968.png)\n\n      - 输入：编码器的输出，包括了之前我们讨论的氨基酸对的那个表示和氨基酸序列的那个表示\n\n        这里只画了一条人类的蛋白质，因为我们只要对这一条进行预测就行了，别的那些来自其它动物的相似的序列我们就不用管了\n      - 上面来自于对的表示(Pair representation)和序列的表示(Single repr.)，进入一个叫做IPA的模块之后，它的输出是氨基酸序列(Single repr.)，但是其中每一个氨基酸的表示，应该含有了更多的位置的信息\n      - 接下来用紫色模块(Predict relative rotations and translations)来预测主干里面那些旋转和偏移，就是这个蛋白质的主干结构是串成一条这样子(Backbone frames)的形状\n      - 拿到主干的形状和之前的氨基酸表示的话，我们再去预测那些枝叶上面的旋转，最后拿到Backbone frames和上面的结构之后，就可以还原出整个蛋白质的3D结构，然后这个更新的序列信息会进入下一个块里面\n      - 对于主干里面的那些旋转和偏移，也会输入进入下一个块里面。所以在IPA这个模块里面，其实他不仅拿到了氨基酸对的信息和不断在更新的序列信息，以及在不断的去调整的主干的旋转和偏移的信息，再拿到三个信息之后再去上面做一步的调整，再做新的预测，然后跟之前的做叠加\n\n        所谓的叠加是说，我对一个刚体做变换的时候，旋转一点位移一点，然后我还可以在上面再旋转一点偏移一点，偏移一点你可以认为是在不断的调整直到我要到最后的形态\n      - 一开始，所有的氨基酸都认为在原点，那么在解码器里面经过8个这样子的blocks，逐渐的把所有的原点的氨基酸然后慢慢的变形成我们最后要的形状(3D结构)，每一次里面做一些的结构的调整不用一步到位，所以这就是解码器块的一个整体的设计\n      - 这里面的所有解码器块是共享权重的，所有就导致整个解码器块特别像一个RNN的架构，输入进来的序列表示和他的主干的旋转和偏移都是他的隐藏状态，pair representation块可以认为是来自编码器的上下文，最后我们关心的是最后一个时刻他的输出作为我的最终的一个输出。结构跟LSTM太像了\n      - IPA模块(Invariant point attention——不动点注意力)\n\n        首先是注意力机制，另外一个叫做不动点\n\n        - 注意力机制\n\n          - 输入：\n\n            $s_i$表示的是第$i$个氨基酸在序列里面的那个表示\n\n            $z_{ij}$表示的是氨基酸对从$i$到$j$的那个表示\n\n            $T_i$表示的是第$i$个氨基酸他的那一个变化就是旋转和位移的那个变换全部写在里面了，因为是个多头注意力，所有这个地方有头的信息，有意思的是这里有3个头\n          - 接下来跟之前一样对$s_i$做投影，得到他的$q,k,v$，有意思的是下面还有一些$q,k,v$出来\n\n            具体来说，$s_i$先算了一个$q$和一个$k$，又算了一个$v$，之所以分开写是因为头的个数不一样，而且看到是说每一个是一个三维的东西，三维就是表示$q,k,v$这个结构在3D中空间中的位置是相关的，然后跟之前一样是根据你的对信息算出一个偏移\n          - 注意力分数是怎么样计算的？\n\n            第7行，跟之前一样，我们还是来自于第$i$个氨基酸的$q$和第$j$个氨基酸的key做内积，再加上来自对表示里面的偏移\n\n            这个地方新加了一项，我们可以忽略掉这些常数，可以看到是说我们先把第$i$个氨基酸上的变换作用在$q_i$上面，再减去第$j$个氨基酸的变换作用在$k_j$上面，然后计算他的距离\n\n            $q_i$可以认为是表示的是第$i$个氨基酸当前的位置，那么$k_j$也表示的是第$j$个氨基酸他的位置，分别作用变换之后，中间的圈表示的把这个变换作用在这个向量上面，就得到了第$i$个氨基酸后的那一个氨基酸的位置和第$j$个氨基酸后的那个氨基酸的位置\n\n            如果他们后面两个氨基酸的位置相隔的比较远的话，因为前面有个负号的关系，就表示他们就不应该那么相似，因为之前是说，如果你们的夹角比较大的话，那表示相似，那么他的后一个氨基酸，如果我把起始点拉到附近的话，长得很不一样的话，那表示这两个氨基酸确实是不那么一样的\n\n            所以这个新加的一项跟前面的主要的区别是我们将这个变换了和位置的信息显式的建模了进来，最后我们计算一些输出\n          - 跟之前不一样的地方，在于我们这里(8)使用的$z_{ij}$就表示了第$i$个氨基酸到第$j$个氨基酸之间的关系，或者是说他的距离就是相对距离，然后(10)也使用了跟位置和变换相关的一个输出，-1表示的是变换回去，就是对每一个氨基酸，我们把它做完变换之后，然后(10)通过权重加起来，然后再反变换回去\n\n            因为$o_i^{hp}$这个东西，我们是要在上面做变换的，这样的话(11)把三个输出全部放在一起，还有$o_i^{hp}$的长度全部放在一起来计算$s_i$，那么这个时候你就认为你的$s_i$即你更新后的氨基酸的那一个表示里面含有了我的位置信息，这也是跟之前的编码器不一样的地方，在解码器里面，我们的向量里面显式的加入了位置的信息\n\n          <img src=\"http://img.peterli.club/joy/202210182105070.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n        - 不动点——点不变的\n\n          - 具体来说先看一下第7行后面平方的那一项就是计算距离的那一项\n\n            假设我们要对所有的氨基酸做一个全局的变化，叫做$T_{global}$的话，我们就是说对$q_i$这个点先做$T_i$的变换再做全局变换，同样对于$k_j$也是做先做$T_j$变换再做全局变换\n          - 因为变换是可以展开的，那么就等价于是说先对$q_i$做变换和对$k_j$做变换，然后相减，减完出的那个值再做全局变换，就把全局写在外面\n          - 因为全局变换我们知道在做偏移的时候，因为有个减的关系，所以偏移会被抵消掉，那么剩下来的就是一个旋转，但旋转是不改变那个向量的长度的，$T_{global}$这一项就被写没了\n          - 意思是说，在计算距离的时候，我们不管做什么样的全局变换，其实他都不影响这个值，因为有个减号的关系\n          - 此外在第10行要计算输出，算$o_i$，同样道理，如果我们要对整个氨基酸做了一个全局变换的话，那么在第10行后面$T_i$有一个全局变换，因为欧几里得变换是线性可以相加的，所以可以一直往前写出来，然后这里有一个逆变换，然后作为一个全局变换之后，做一个局部的变换，再回过来做一个全局的逆变换，这两个变换会被抵消掉，这是因为矩阵的乘法是可以交换的\n\n            所以不管是第7行计算相似度也好，还是第10行计算输出也好，我对整个蛋白质做一个全局的欧几里得变换是不会影响到我整个的结果的，所以这就是IPA里面的IP是来自于什么意思\n\n          ![Untitled](http://img.peterli.club/joy/202210182105101.png)\n      - 预测模块(紫色的和绿色的)\n\n        - 预测模块比较简单，因为每一个氨基酸的向量表示里面已经含有了位置信息，那么要去预测他的实际的位置的时候，我们就做一个线性的投影层就可以得到我们的输出了\n\n          当然这个地方稍微复杂一点，是说我们的预测是有物理意义的，所以在结构上有需求，而不是我们之前预测一个类别，基本上任何输出都是行的\n        - 以紫色模块(Predict relative rotations and translations)为例，分析一下这个物理信息是怎么样放进来的\n        - 名字叫做主干更新，具体要干的事情是说，对第$i$个氨基酸的向量表示，我们去预测它对应的那一个变换，变换$T_i$里面有两个东西，一个是他的3*3旋转矩阵，另一个是一个3*1的向量，对于偏移来说我们没有任何的要求，怎么偏都可以，但是对一个旋转矩阵来说，是一个正交矩阵，而且他的Norm是等于1的，所以在做预测的时候，他不是去直接预测$R_i$里面的12个值，而是说把$s_i$投影到一个6维里面(1)，其中后面的三维$t_i$直接作为我的偏移，然后前面的三维($b_i,c_i,d_i$)用来构造旋转矩阵，虽然旋转矩阵有12个元素，但是其实就有3个值可以决定\n\n          具体来说，通过下面(3)这个变换来得到一个合法的单位旋转矩阵，这样子满足我们物理上的要求，这就是他怎么样做主干变换的预测的\n\n          ![Untitled](http://img.peterli.club/joy/202210182105155.png)\n- 训练中的细节\n\n  - 主损失函数：FAPE(Frame aligned point error)，大概意思是说，根据我预测出来的那些变换，然后我就能把整个蛋白质的结构还原出来\n  - 绿色是表示预测出来的结构，灰色表示真实的结构，然后就知道对应的原子在真实中的位置和预测出来的位置，然后把这两个距离一减就得到我的损失，距离越大损失越大，距离越小就损失越小\n\n  <img src=\"http://img.peterli.club/joy/202210182105239.png\" alt=\"Untitled\" style=\"zoom: 50%;\" />\n\n  使用了两个额外的技术\n\n  - 使用没有标号的数据\n\n    - 有标号了的数据全部在PDB里面，也就是已知结构的所有的蛋白质，但是还是觉得不够\n    - 用了一个方法叫做噪音的学生自蒸馏方法，核心思想是，现在在有标号的数据上训练一个模型，然后我用这个模型去预测一个大的没有标号的数据集，然后把那些比较置信的标号拿出来，跟之前的有标号的数据集一起拼成一个更大的一个数据，这样子我们可以在上面重新再训练一个模型出来，这样子就等于是说我的数据集变大了，然后可以重复做很多次\n    - 这里一个核心的关键点是说你要加噪音进去，不然的话如果我之前的一个模型预测一个样本预测错了，而且我又特别置信，那么这个错的标号将会进入到你的训练样本，使得你下一次训练的时候，在这个错上可能会加错，但是你加入噪音比如说大量的数据增强，甚至是把标号做一些改变的话，那么你的模型就比较好的处理这些不正确的标号\n    - 具体来说，他从一个新的数据集里面找到了35万个不是很一样的训练，他先在PDB上训练一个模型，然后把这些上面预测的置信度比较高的那一些蛋白质序列拿出来跟PDB一起做成一个新的数据集再重新开始训练，这个地方模型架构并没有变化，但是多训练几次\n  - 来自BERT\n\n    在蛋白质序列里面随机遮住一些氨基酸，或者甚至是把一些氨基酸把它做一些变换，然后像BERT去预测这些被遮住的氨基酸，然后他发现在训练的时候同时加入这个任务的话，整个模型对整个序列的建模上更加好一点\n\n  训练参数：\n\n  序列长度：256\n\n  batch size : 128\n\n  具体是用128个TPU v3训练，从随机初始训练的话，大概是一个星期，如果你做微调的话还需要额外的4天\n\n  最大的问题是内存不够，因为整个模型对内存的占用率是非常非常高的，大概需要几百GB的内存的样子，但是一个TPU v3也就是几十GB的样子\n\n  具体的做法也是个常见的做法，你的内存基本来自于误差反传的时候，因为你要算出来中间的那些结果，结果给你存下来，后面算梯度时候要用，存的那些结果跟网络的深度成正比\n\n  可以做的是说，你把中间一些算的比较快的结果丢掉，然后再算梯度的时候，发现你要的时候再去重新算一遍，这样就是用计算来换空间，使得计算也是可以往前走的\n\n  预测的一些性能，而预测性能跟你的蛋白质的长度是相关的\n\n  如果你的蛋白质里面有256个氨基酸的话，那么你在一个单卡V100的卡上面，大概是5分钟的样子，如果有384个氨基酸的话，那么是9分钟，2500个的话，那么是18个小时\n\n# 结果的分析\n\n- 主要讲的是一些消融实验，主要的结果是放在图四，左边是CASP14就是竞赛中的结果，右边是PDB也就是他用来训练模型的测试数据集上的结果。\n- 灰线，0表示的是他的基线，右边比0大的话表示比基线要好，如果在灰线的左边的话表示比0小，那么他就比基线要差。其中每一行表示的是一个方法，基线的结果大概就是在这个灰线的样子\n- 使用自蒸馏会怎么样，也就是使用额外的没有标注的训练数据集对他进行训练，在竞赛上的效果好一点，但是在PDB这个数据集上还是高了很多，很有可能是说PDB这个数据集他的多样性更大一点，他的测试数据更大一点\n- 去掉一些模块会怎么样，前面的这些都是去掉一些输入的数据，比如说把模板信息去掉，损失那么一点，如果把直方图去掉，也损失一点\n- 如果把原始的MSA但是用一个替代来去掉的话，也会损失掉一些\n- 接下来是对模型的一些变换，比如说把IPA去掉，意味着是说你在解码器的时候，我不再使用一个注意力机制，把你的位置信息放进你的氨基酸的向量里面，可以看到会损失一些，但是在PDB好像损失的并不大\n- 如果没有使用BERT那种带掩码的机制会怎么样，在竞赛数据集上还好，但是在PDB这个数据集上，损失还是比较大的\n- 如果你不做回收，不做那四次回收的话，那损失是比较大的\n- 如果你使用最简单的注意力机制，就是说我先按行做标准的自注意力，然后再按列做标准的自注意力，而不使用三角更新或者是用对信息来做偏差或者使用门，可以看到的话，这里差的还是比较大的，在PDB的数据上差的更多一些，或者说如果不做端对端的结构梯度，我理解就是你的编码器不参与计算梯度那损失更多了\n- 最后是你不做IPA也不做回收，那损失就是相当的明显\n- 所以这个图想表达的核心思想是说，我这个网络虽然复杂，但是里面没有一块能去掉，把所有块加起来都是有好处的，但反过来讲这个模型还是相当复杂，虽然这个地方已经做了很多消融实验了，但是你很难从系统的对每一个模块看一下，他到底是不是重要的，因为里面还是有很多模块没有逐一检查到的\n\n<img src=\"http://img.peterli.club/joy/202210182105315.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n- 我们的编码器里面有48块，然后我们做了4次回收，所以等价于一共做了192个块\n- 如果我给你一个蛋白质，只使用前面的一些块，效果会怎么样，y轴表示的是你的好坏越往高越好，可以看到如果是两个简单蛋白质的话，基本上到了48就差不多了，也就是说你不用做回收也没关系\n\n  如果你的编码器里面有48个块，那其实这个地方精度就差不多了，但是对于比较复杂的话，48个还是不够的，甚至做到192还是有上升的趋势，说不定可以做的更深一点\n\n<img src=\"http://img.peterli.club/joy/202210182105388.png\" alt=\"Untitled\" style=\"zoom: 33%;\" />\n\n# 评论\n\n- 文章的写作\n\n  这篇文章算比较短的，虽然页数不少，但是你把图去掉之后，他的文字的部分并不多，话特别经典，每一句话都是在讲件事情，但是这个地方的网络其实比Transformer那个地方可能还要复杂个几倍的样子，而且他的大量的篇幅都在介绍问题和对自己结果的歌颂上面，所以以至于是讲模型的部分是非常非常少，读正文读很多遍你也不知道他模型的细节在什么地方，以至于你要读它提供的补充材料，才能知道里面的细节是什么样子，或者去读源代码\n\n  可以回过去看一下他的正文是怎么写的，学习一下对一个比较复杂的算法，你怎么样用相对来说比较简单的篇幅去对他进行介绍，在结果上不需要做太多的评论，因为整篇文章的主要的卖点是我的结果非常好\n- 模型\n\n  这个模型真的是比较复杂的，细节很多，为什么想出了那么多的模块，可以用来提升精度，是他的写作的方法造成的一个误解，他的正文里面没有讲太多细节，基本上就是一个很简单的一个描述，然后大概讲了一下我的一些贡献，有一个非常简短的相关工作，然后就直接跳到了补充材料，基本上是代码的一个伪代码版本，只讲了具体的实验细节，究极融合怪\n\n你就说这是一个什么的问题，我们提出把其中一个东西改掉之后，就解决了这个问题，一句话就可以讲清楚的结论甚至可以放进标题\n","tags":["Transformer","AI","Paper"],"categories":["Paper","Alphafold2"]},{"title":"搭建Docker私有仓库","url":"/2022/Docker私有仓库搭建/","content":"\n# Harbor简介\n![]( https://goharbor.io/img/logos/harbor-horizontal-color.png)\nVMware开源的企业级Registry项目Harbor，以Docker公司开源的registry 为基础，提供了管理UI, 基于角色的访问控制(Role Based Access Control)，AD/LDAP集成、以及审计日志(Audit logging) 等企业用户需求的功能，同时还原生支持中文，主要特点：\n\n* 基于角色的访问控制 - 用户与 Docker 镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。\n* 镜像复制 - 镜像可以在多个 Registry 实例中复制（同步）。尤其适合于负载均衡，高可用，混合云和多云的场景。\n* 图形化用户界面 - 用户可以通过浏览器来浏览，检索当前 Docker 镜像仓库，管理项目和命名空间。\n* AD/LDAP 支持 - Harbor 可以集成企业内部已有的 AD/LDAP，用于鉴权认证管理。\n* 审计管理 - 所有针对镜像仓库的操作都可以被记录追溯，用于审计管理。\n* 国际化 - 已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。\n* RESTful API - RESTful API 提供给管理员对于 Harbor 更多的操控, 使得与其它管理软件集成变得更容易。\n* 部署简单 - 提供在线和离线两种安装工具， 也可以安装到 vSphere 平台(OVA 方式)虚拟设备\n\n## Harbor架构\n![Harobr架构图](https://github.com/goharbor/harbor/raw/release-2.0.0/docs/img/architecture/architecture.png)\n\nProxy: Harbor的registry、UI、token services等组件，都处在一个反向代理后边。该代理将来自浏览器、docker clients的请求转发到后端服务上。\n\nRegistry: 负责存储Docker镜像，以及处理Docker push/pull请求。因为Harbor强制要求对镜像的访问做权限控制， 在每一次push/pull请求时，Registry会强制要求客户端从token service那里获得一个有效的token。\n\nCore services: Harbor的核心功能，主要包括如下3个服务:\n1. UI: 作为Registry Webhook, 以图像用户界面的方式辅助用户管理镜像。\n2. WebHook：WebHook是在registry中配置的一种机制， 当registry中镜像发生改变时，就可以通知到Harbor的webhook endpoint。Harbor使用webhook来更新日志、初始化同步job等。\n3. Token 服务：负责根据用户权限给每个docker push/pull命令签发token. Docker 客户端向Regiøstry服务发起的请求,如果不包含token，会被重定向到这里，获得token后再重新向Registry进行请求。\nDatabase：为core services提供数据库服务，负责储存用户权限、审计日志、Docker image分组信息等数据。\n\nJob services: 主要用于镜像复制，本地镜像可以被同步到远程Harbor实例上。\n\nLog collector: 负责收集其他组件的日志到一个地方\n\nharbor-adminserver主要是作为一个后端的配置数据管理，harbor-ui所要操作的所有数据都通过harbor-adminserver这样一个数据配置管理中心来完成。\n\n# Harbor安装\n## 安装Docker\nHarbor各个组件全部基于docker，在安装Harbor之前，需要预先安装docker、docker-compose\n## 下载Harbor安装脚本\n从[https://github.com/goharbor/harbor/releases](https://github.com/goharbor/harbor/releases)仓库中下载Harbor安装脚本，如果目标机器可以连接网络的话，下载在线安装版的就可以。\n## 解压harbor安装包\n```shell\ntar xf harbor-online-installer-{}.tgz\n```\n## 生成SSL自签名证书\nHarbor使用Docker-compose启动，使用Nginx作为反响代理，建议部署在机器的80和443端口上，并且进行SSL自签名，这样的话其他机器下载镜像时就不需要配置docker的信任仓库。\n下面是生成SSL签名的流程，需要在机器上提前安装`openssl`软件包。\n\n把下边代码中所有的IP地址都替换成您自己的IP地址。\n```shell\n# 生成默认 ca\nopenssl genrsa -out ca.key 2048\nopenssl req -x509 -new -nodes -key ca.key -subj \"/CN=192.168.0.8\" -days 5000 -out ca.crt\n\n# 生成证书\nopenssl req -new -sha256 \\\n-key ca.key \\\n-subj \"/C=CN/ST=Beijing/L=Beijing/O=UnitedStack/OU=Devops/CN=192.168.0.8\" \\\n-reqexts SAN \\\n-config <(cat /etc/pki/tls/openssl.cnf \\\n<(printf \"[SAN]\\nsubjectAltName=IP:192.168.0.8\")) \\\n-out ca.csr\n\n# 签名证书\nopenssl x509 -req -days 365000 \\\n-in zchd.csr -CA ca.crt -CAkey ca.key -CAcreateserial \\\n-extfile <(printf \"subjectAltName=IP:192.168.0.8\") \\\n-out ca.pem\n\n```\n如果运行成功，会在当前目录下生成`ca.key`和`ca.pem`两个证书文件，在接下来修改Harbor配置文件的时候需要修改这两个文件路径\n## 修改Harbor配置文件\n重命名`harbor.yml.tmpl`为`harbor.yml`。该配置文件中需要注意的参数如下，其他的保持默认配置即可。\n```yaml\nhostname: 192.168.0.8         //设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhost。默认情况下，harbor使用的端口是80，若使用自定义的端口，除了要改docker-compose.yml文件中的配置外，这里的hostname也要加上自定义的端口，否则在docker login、push时会报错\n#http配置\n# http related config\nhttp:\n# port for http, default is 80. If https enabled, this port will redirect to https port\n  port: 80\n\n#https配置（如不需要可不配置,注释掉）\n# https related config\nhttps:\n# https port for harbor, default is 443\n  port: 443\n  # The path of cert and key files for nginx\n  certificate: /path_to/ca.pem\n  private_key: /path_to/ca.key\n\nharbor_admin_password: Harbor12345         # admin密码\n\n#持久化数据目录\ndata_volume: /opt/application/harbor\n```\n\n# 运行Harbor\n配置完毕之后就可以启动Harbor，运行安装脚本，Harbor就会自动安装。\n```shell\n./install.sh\n```\n# 进入Harbor\n之后访问Harbor的WEB页面，默认用户名为`admin`，密码为`Harbor12345`\n","tags":["Docker","DockerHub"],"categories":["Docker","DockerHub"]},{"title":"Transformer","url":"/2022/Transformer/","content":"# Transformer\n\nTitle: Attention Is All You Need\n\nAuthors: Vaswani, Ashish\n\nDOI: https://doi.org/10.48550/arXiv.1706.03762\n\nDate: October 1, 2022\n\nFinish time: 2022/10/07\n\nFuture: 应用于别的任务上面，图片语音视频，生物信息学\n\nMeaning: 舍去了特征提取这一步，只关于注意力机制，训练速度更快效果更好\n\nYear: 2017\n\n关键词: AI, NLP\n\n期刊杂志: NIPS2017\n\nReference: 【Transformer论文逐段精读【论文精读】】https://www.bilibili.com/video/BV1pu411o7BE?vd_source=5ec85dfc5468a21a485b1b1d4d271219\n\n# 标题+作者\n\n## Attention Is All You Need\n\n> 经验：每个作者名字后面都打了一个🌟号（行内打🌟一般都表示同等贡献，listing oder is random，每个作者都做出了贡献\n\n![Untitled](http://img.peterli.club/joy/202210141048133.png)\n\n# 摘要(Abstract)\n\n- 序列转录模型：给你一个序列，你生成另外一个序列，这样的模型主要是依赖于比较复杂的循环或者卷积神经网络，它一般是用一个叫做encoder和decoder的架构\n- 性能比较好的模型一般都会在编码器和解码器之间使用一个叫做注意力机制的东西\n- 这篇文章提供了一个简单的网络架构（只要结果好），仅仅是依赖于注意力机制，而没有用之前的循环或者是卷积\n- 做了两个机器翻译的实验，显示这个模型在性能上特别好，可以并行度更好，需要更少的时间来训练，达到了28.4 BLEU——机器翻译中的一个衡量标准，英语到德语好了2个BLEU，英语到法语做了一个单模型，比所有模型效果好，只在8个GPU上训练了3.5天，模型在泛化效果也好\n\n<img src=\"http://img.peterli.club/joy/202210141049748.png\" alt=\"abstract\" style=\"zoom:50%;\" />\n\n# 结论(Conclusion)\n\n- 提出了什么东西跟别人的区别\n\n  我们介绍了Transformer这个模型，这是第一个做序列转录的模型，仅仅使用注意力，把之前所有的循环程全部换成了multi-head self-attention，基本上可以看到这篇文章主要用的是提出了是这样一个层\n- 效果和别人相比怎么样\n\n  在机器翻译这个任务上面，Transformer能够训练的比其他的架构都要快很多，而且在实际的结果上确实是效果比较好\n- 未来期望的发展\n\n  想把这种纯注意力机制的模型用在别的任务上面，除了文本以外包括图片、语音、video等，使得生成不那么时序化\n- 代码放在[tensor2tensor](https://github.com/tensorflow/tensor2tensor)\n\n<img src=\"http://img.peterli.club/joy/202210141049582.png\" alt=\"conclusion\" style=\"zoom: 50%;\" />\n\n# 导言(Introduction)\n\n前面摘要的一半的一个扩充\n\n- 当前的行内用的最多的东西\n\n  在时序模型里面，当前(2022)常用的是RNN，2017最常用的是LSTM(long short-term memory)、GRU，在这里有两个比较主流的模型，一个是语言模型，另一个是当输出结构化信息比较多的时候，会用一个叫做编码器和解码器的架构\n- RNN优点和缺点\n\n  - RNN是什么\n\n    RNN里面，给你一个序列的话，它的计算是把这个序列从左往右移一步一步往前做，一个句子的话就是一个词一个词往前看，对第t个词，它会计算一个输出叫做$h_t$，也叫做隐藏状态\n\n    $h_t$是由前面一个词的隐藏状态叫$h_{t-1}$和当前第t个词本身决定的，这样可以把前面学到的历史信息通过$h_{t-1}$放到当下，和当前的词做一些计算，然后得到输出\n\n    这也是RNN如何能够有效处理时序信息的一个关键之处，他把之前的信息全部放在隐藏状态里面，一个个放在里面，但是这也导致了问题\n  - 缺点\n\n    - 计算性能较差，它是一个时序，就是一步一步计算的过程，难并行，计算上性能较差\n    - 内存消耗大，历史信息一步一步往后传递，如果时序比较长的话，在很早期的时序信息，在后面的时候可能会丢掉，不想丢掉的话，$h_t$比较大，每个$h_t$都存下来的话，内存消耗大\n\n    很多人做了很多改进，包括提升并行度，但是本质上还是没有解决太多问题\n- attention在RNN中的应用\n\n  在Transformer之前，attention已经被成功地用在编码器的解码器里面了，主要是用在怎么样把编码器的东西很有效的传给解码器，attention跟RNN一起使用\n- 提出Transformer，不再使用之前被大家使用的循环神经层了，而是使用纯注意力机制，而且是可以并行的，之前时序神经网络要按时序地做运算，换成attention之后可以完全做并行，速度快了\n\n<img src=\"http://img.peterli.club/joy/202210141050248.png\" alt=\"introduction-1\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141050030.png\" alt=\"introduction-2\" style=\"zoom:50%;\" />\n\n# 相关工作(Background)\n\n> 经验：关键是要讲清楚跟你论文相关的那些论文是谁，跟你的联系是什么，你跟他们的区别是什么\n\n- 卷积神经网络的参考价值\n\n  如何使用卷积神经网络来替换掉循环神经网络，使得减少时序的计算\n\n  - 缺点\n\n    对于比较长的序列使用卷积神经网络难以建模，这是因为卷积做计算的时候每一次它去看一个一个比较小的一个窗口\n\n    比如看一个3x3的像素块，如果两个像素隔得比较远的话，需要很多层卷积，一层一层上去，才能够最后把这两个隔得远的像素融合起来\n\n    但是如果使用Transformer里面的注意力机制的话，每一次能看到所有的像素，一层就能够把整个序列看到，相对来说就没有这个问题\n  - 优点\n\n    可以做多个输出通道，一个输出通道可以认为是它可以去识别不一样的模式\n\n  作者说想要注意力机制也想要这样子的多输出通道的效果，所以它提出了一个叫做Muti-Headed Attention，可以模拟卷积神经网络多输出通道的一个效果\n- 介绍自注意力机制，这是Transformer里面一个关键性的点，其中提到这个工作之前已经有人提出来了，并不是我这个工作的创新\n\n  自注意力将单个序列的不同位置联系起来，来计算出序列的表示\n- 提到memory networks，这个在17年的时候算是一个研究的重点\n- best knowledge 里面，我们的Transformer是第一个只依赖于自注意力来做这种encode到decode的架构的模型\n\n<img src=\"http://img.peterli.club/joy/202210141050281.png\" alt=\"background\" style=\"zoom:50%;\" />\n\n# ✨模型架构(Model Architecture)\n\n- 这些序列模型里面现在比较好的是一个叫做编码器和解码器的架构，然后解释了一下什么是编码器什么是解码器\n\n  - 编码器\n\n    将一个输入的符号表示序列($x_1$,$x_2$…$x_n$)即一个长为n的$x_1$一直到$x_n$的东西，映射到连续表示序列$z=(z_1,z_2,…z_n)$\n\n    假设一个句子的话，有n个词的话，$x_t$就表示第t个词，编码器会把这个序列表示一个也是长为n，但是每一个$z_t$对应的是$x_t$的一个向量的表示，$z_t$就表示第t个词的一个向量的表示，这就是编码器的输出\n\n    原始的输入变成机器学习可以理解的一系列的向量\n  - 解码器\n\n    拿到编码器的输出，会生成一个长为m的输出序列$(y_1,…,y_m)$，跟编码器不一样的是，在解码器里面这个词是一个一个生成的\n\n    因为对于编码器来讲，很有可能是一次性能看全整个句子，但是解码器里面只能一个一个生成，这就是自回归，叫做auto-regressive的一个模型，在这个里面你的输入又是你的输出\n\n  在最开始给定z，那么你要去生成第一个输出叫做$y_1$，拿到$y_1$之后就可以去生成$y_2$，一般来说要生成$y_t$的话，可以把之前所有的$y_1$到$y_{t-1}$全部拿到，也就是过去时刻的输出也会作为你当前时刻的输入，所以这个叫做自回归\n\n  ![model-1](http://img.peterli.club/joy/202210141051159.png)\n- Transformer是使用了一个编码器解码器的架构，具体来说它是将一些自注意力和point-wise，full-connected layers，然后把一个一个堆在一起的，下面统一展示这个架构\n\n  解码器在做预测的时候是没有输入的，实际上它就是解码器在之前时刻的一些输出作为输入在这个地方\n\n  ![model-2](http://img.peterli.club/joy/202210141051059.png)\n\n  > 经验：写论文的时候有一张能够把整个全局画清楚的图是非常重要的，神经网络年代，画图是一个基础的技能\n  >\n\n  <img src=\"http://img.peterli.club/joy/202210141051925.jpg\" alt=\"model-3\" style=\"zoom: 25%;\" />\n\n## 具体介绍\n\n## Encoder and Decoder Stacks\n\n- 编码器(Encoder)\n\n  - 结构\n\n    - 编码器是一个N=6个完全一样的layer组成的block\n    - 每个layer里面有两个sub-layer\n\n      第一个sub-layer叫做multi-head self-attention\n\n      第二个sub-layer叫做position-wise fully connected feed-forward network(说白了就是一个MLP，显得fancy一点就写的长一点)。\n    - 对每一个sub-layer用了一个残差连接，最后再使用layer normalization\n    - 这个sub-layer的公式为LayerNorm($x$ + Sublayer($x$))\n  - 流程\n\n    输入$x$进来，先进入子层，因为是残差连接，就把输入和输出加在一起，最后进入LayerNorm\n  - 维度\n\n    因为残差连接需要输入和输出是一样大小，如果不一样的话要做投影，为了简单起见，就把每一个层的输出的维度变成512，也就是说对每一个词，不管在哪一层都做了这个512长度的表示\n\n    跟CNN不一样，之前做MLP的时候经常维度要么是往下减，要么CNN的话是空间维度往下减，channel维度往上拉，但是这里就是固定长度来表示，使得这个模型相对来说是比较简单的，调参也只是调一个而已，另一个参就是有几个block\n\n  > 经验：用了别人的东西，最好在文章里真的讲一下它是什么东西，不能指望别人都知道所有的细节，能够花几句话讲清楚是不错的，不然还要别人去点开链接看到底是什么东西给大家带来了困难\n  >\n\n  - 补充：LayerNorm\n\n    用与batchnorm作对比来解释layernorm是什么以及为什么在变长的应用里面不使用batchnorm\n\n    - 二维情况\n\n      - batchnorm\n\n        - 干了什么？\n\n          输入一个矩阵，每一行是一个样本，每一列是一个特征，把每一个**特征**在一个小mini-batch里面，均值变成0，方差变成1\n        - 怎么实现？\n\n          把它的这个向量本身的均值减掉，然后再除以它的方差，算均值，就是在每一个小批量里面，就这条向量里面算出它的一个均值和方差。\n        - 训练和预测时的差别\n\n          在训练的时候，可以用小批量，但是在预测的时候，是算全局的均值，整个数据扫一遍之后，在所有数据上那些平均的那个均值方差存起来，预测的时候使用\n\n          batchnorm还会去学一个λ、β，就是说可以把这个向量通过学习放成一个任意方差为某个值，均值为某个值的东西\n      - layernorm\n\n        跟batchnorm很多时候是几乎是一样的，除了他做的方法不一样，layernorm干的事情就是对每个**样本**做了normalization，而不是对特征做，之前是把每一个列的均值变成0方差变成1，现在是把每一个行的均值变成0方差变成1\n\n        就是相当于这个layernorm整个把数据转置一下放到batchnorm里面出来的结果再转置回去一下基本上就可以得到自己的东西了\n\n      <img src=\"http://img.peterli.club/joy/202210141051311.png\" alt=\"encoder-1\" style=\"zoom:33%;\" />\n    - 三维情况\n\n      但是在Transformer或者RNN里面，输入是一个三维的东西，因为它输的是一个序列的样本，就是每一个样本其实是里面有很多个元素，它是一个序列，一个句子里面有n个词，所以每个词有个向量的话，还有一个batch的话就是个3D的东西，列不再是特征了，列变成了序列的长度，写成sequence，然后对每一个sequence就是对每个词有自己的向量\n\n      - batchnorm(蓝色)：\n\n        每次是取一根特征，然后把它的每个样本里面所有的元素，这整个序列里的元素以及它的整个batch全部搞出来，把它的均值变成0，方差变成1，纵向切一块出来拉成一个向量，然后跟之前做一样的运算\n      - layernorm(黄色)，就是对每个样本，那样做\n      - 为什么用layernorm多一点\n\n        因为在持续的序列模型里面，每个样本的长度可能会发生变化\n\n        batchnorm在长度变化比较大的时候，每次做小批量的时候，算出来的均值方差抖动相对来说比较大，做预测的时候是要把全局的均值和方差记录下来，那么这个全局的均值方差，如果遇到一个新的预测样本，特别特别长的东西，那是不是在训练的时候没见过伸出去那么多的，那之前算的均值和方差就可能没那么好用\n\n        对于layernorm相对来说没有太多这个问题，是因为它是每个样本自己来算均值和方差，也不需要存在一个全局的一个均值方差，所以相对来说，不管样本是长是短，反正算均值是在自己里面算的，相对来说稳定一点\n\n      <img src=\"http://img.peterli.club/joy/202210141051515.png\" alt=\"encoder-2\" style=\"zoom:33%;\" />\n- 解码器(decoder)\n\n  - 结构\n\n    - 解码器也是一个N=6个完全一样的layer组成的block\n    - 每个layer里面有两个跟编码器一样的sub-layer，与编码器不一样的是，解码器里面用了一个第三个sub-layer，同样是一个多头的注意力机制\n    - 跟编码器一样同样的用了残差连接和layernorm\n  - 自回归\n\n    解码器做的是一个自回归，当前输出的输入集是上面一些时刻的输出，意味着你在预测的时候不能看到之后的那些时刻的输出，但是在注意力机制里面每一次能看到整个完整的输入\n\n    为了避免这个事情的发生，在解码器训练的时候，在预测第t个时刻的输出的时候不应该看到t时刻以后的那些输入，做法是通过一个带掩码的注意力机制，保证输入进来的时候，在t时刻是不会看到t时刻以后的那些输入，从而保证训练和预测的时候行为是一致的\n\n<img src=\"http://img.peterli.club/joy/202210141052559.png\" alt=\"decoder-1\" style=\"zoom:50%;\" />\n\n## Attention\n\n对注意力层的一般化的介绍，注意力函数是一个将一个query和一些keys-values pairs映射成一个输出的一个函数，这里的query、keys、values、output都是一些向量\n\n- output\n\n  values的一个加权和，所以输出的维度跟value的维度是一样的\n- weight\n\n  每一个value的权重，它是这个value对应的key和这个查询的query的相似度算来的或者叫做compatibility function\n- 不同的注意力机制有不同的算法\n\n  假设有三个value和三个对应的key，输出是这三个v的相加，假设给一个query，这个query跟第一第二个key比较近(黄)，前两个的权重会大点，第三个权重会小一点，因为这个权重是等价于你的query和你对应的key的那个相似度\n\n  假设再给一个query，但是它是跟最后那一个key比较像的话(绿)，再去算v的话就会发现它对后面的权重会比较高一点，中间权重还不错，最后权重小一点，会得到新的输出。虽然key value没有变，但是随着query的改变，因为权重分配不一样，导致你的输出会有不一样，这就是注意力机制\n\n<img src=\"http://img.peterli.club/joy/202210141052252.png\" alt=\"attention-1\" style=\"zoom:33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141052333.png\" alt=\"attention-2\" style=\"zoom:50%;\" />\n\n### Scaled Dot-Product Attention\n\n因为不同的相似函数导致不一样的注意力的版本，接下来介绍Transformer自己用到的这一个注意力Scaled Dot-Product Attention是什么样子计算的，最简单的注意力机制\n\n- 计算方法\n\n  - query和key是等长的，都等于$d_k$(可以不等长，不等长的话可以用别的方法算)，它的value是$d_v$，输出一样也是$d_v$\n  - 对每一个query和key做内积作为相似度，如果这两个向量的norm是一样的话，那么你的内积的值越大，就是它的余弦值，就表示这两个向量的相似度就越高，如果内积等于0了，那就等于是两个向量是正交的，就是没有相似度\n  - 算出来之后，再除以$\\sqrt{d_k}$即向量的长度，然后再用一个softmax来得到values的权重\n  - 因为给一个query，假设给n个key-value pair的话，那么就会算出n个值，因为这个query会跟每个key做内积，算出来之后再放进softmax就会得到n个非负的而且加起来和等于1的一个权重，对于权重我们觉得当然是非负加起来等于1就是比较好的权重，然后我们把这些权重作用在我们的value上面就会得到我们的输出了，我们不能一个一个这么做运算，算起来比较慢\n- 实际计算\n\n  1. query矩阵与key矩阵相乘\n\n     query写成一个矩阵，n个query即n行，维度为$d_k$（query个数和key value的个数可能是不一样的，但是长度一定是一样的，这样子才能做内积），query矩阵与key矩阵相乘得到了n*m的一个矩阵\n  2. 得到weight权重\n\n     n*m矩阵每一行蓝色的线就是一个query对所有key的内积值，再除以$\\sqrt{d_k}$，再做softmax（对每一行做softmax），然后每行之间是独立的，就能得到权重\n  3. 得到output输出\n\n     weight乘以v，v是一个m行，列数是dv的矩阵，两个矩阵相乘得到一个n乘以dv的矩阵，这个矩阵每一行就是output输出\n\n  所以对于key-value pair和n个query的话，可以通过两次矩阵乘法来把整个计算做掉，这些query、key、value在实际中对应的就是序列，所以导致基本上可以并行计算里面每个元素，因为矩阵乘法是一个非常好并行的东西\n\n  <img src=\"http://img.peterli.club/joy/202210141052029.png\" alt=\"scaled-1\" style=\"zoom: 33%;\" />\n- 我的注意力机制和别人的区别\n\n  一般有两种比较常见的注意力机制，一种叫做加型的注意力机制，可以处理query和key不等长的情况，另一种叫做点积的注意力机制，他说点积的注意力跟我的机制是一样的，因为这个实现起来比较简单并有效\n- 为什么要除以$\\sqrt{d_k}$\n\n  如果$d_k$小的话除不除以都没关系，两种机制都差不多\n\n  但是$d_k$比较大的话，即两个向量长度比较长的时候，你做点积的时候这些值可能就会比较大，但也可能比较小了，当你的值比较大的时候，你之间的相对的那些差距就会变大，就导致说你值最大的那一个值做出来softmax就会更加靠近于1，剩下的那些值就会更加靠近于0，你的值就会更加向两端靠拢\n\n  当你出现这样子的情况的时候，你算梯度的时候，你会发现梯度比较小，因为softmax最后的结果就是我希望我的预测值置信的地方尽量靠近1，不置信的地方尽量靠近0，这样子的时候就是收敛的差不多了，这个时候梯度就是比较小，就会跑不动\n\n  为了抵消这种影响，用$\\frac{1}{\\sqrt{d_k}}$来缩放点积，所以说Transformer里的$d_k$会比较大\n\n<img src=\"http://img.peterli.club/joy/202210141053603.png\" alt=\"scaled-2\" style=\"zoom: 50%;\" />\n\n- 怎么样做mask?\n  - mask作用\n\n    为了避免你在第t时间的时候看到以后时间的东西\n  - 为什么要用mask?\n\n    假设query和key是等长的，长度都为n，而且在时间上是能对应起来的，然后对第t时刻的$Q_t$即query，在计算的时候，应该只是看$k_1$到$k_{t-1}$，不应该看$k_t$和它之后的东西，因为$k_t$在当前时刻还没有，但是在注意力机制的时候，$Q_t$会跟所有$k$里面的东西全部做运算，就是$k_t$一直算到$k_n$，算还是可以算的，在算出来之后只要保证说在计算权重的时候，就是算输出的时候，不要用到后面的一些东西就行了。就是加一个mask\n  - 怎么样做mask?\n\n    mask意思是说对于$Q_t$和$k_t$和它之后的计算那些值换成一个很大很大的负数，那这么大一个负数进入softmax做指数的时候，他就会变成0，所以导致softmax之后出来的这些东西它对应的那些权重都会变成0，而只会前面那些值有效果\n\n    这样在算我的output的时候，只用上了$v_1$一直到$v_{t-1}$的结果，而后面的东西我没有看。所以这个mask效果是在训练的时候，让t个时间的query只看对应的前面那一些的key-value pair，使得我在做预测的时候，我跟现在这个是能够一一对应上\n\n<img src=\"http://img.peterli.club/joy/202210141053030.png\" alt=\"scaled-3\" style=\"zoom: 50%;\" />\n\n### Multi-Head Attention\n\n- 怎么做？\n\n  与其做一个单个的注意力函数，不如说把整个query、key、value投影到一个低维，投影h次，然后再做h次的注意力函数，然后把每一个函数的输出并在一起，然后再投影来得到最终的输出。\n\n  进入一个linear线性层，线性层就是把你投影到比较低的维度，然后进入Scaled Dot-Product Attention，做h次，会得到h的输出，把这些输出向量全部合到一起，最后做一次线性的投影\n\n  <div align=center><img src=\"http://img.peterli.club/joy/202210181434055.png\" style=\"zoom:50%;\" />\n- 为什么要做？\n\n  回头看Scaled Dot-Product Attention，会发现没有什么参数可以学习，你的具体函数就是你的内积，但有时候为了识别不一样的那些模式，希望可能有一些不一样的计算像素的办法\n\n  如果是用的加型attention，这里没有提到的，那里面其实还是有一个权重可以学习的，那我用这个的多，我先让你投影到低维，这个投影的w是可以学的，也就是说我给你h次机会，希望你能学到不一样的投影的方法，使得在那个投影进去的那个度量空间里面，能够去匹配不同模式它需要的一些相似函数，然后最后把这些东西回来，最后再做一次投影。跟之前说的卷积神经网络有多个输出通道的感觉\n\n  具体公式如下，Q，K，V还是以前那个，但是你的输出已经是你不同的头的那一个输出的做concat起来，再投影到一个$W^O$里面，对每一个头，他就是把你的Q、K、V然后通过一个不同的可以学习的$W^Q$、$W^K$、$W^V$，投影到一个dv上面，再做我们之前提到的注意力函数，然后再出来就行了\n\n  <img src=\"http://img.peterli.club/joy/202210141054998.png\" alt=\"multi-2\" style=\"zoom:50%;\" />\n\n  这里h是8，使用的是8个头，注意力的时候有残差连接的存在，使得你的输入和输出的维度至少是一样的，它的做法是说你投影的时候，它投影的就是你的输出的维度除以h，因为我们之前我的输出维度是512，除以8之后呢，就是每一次我们把它投影到一个64维的一个维度，然后在上面算你的注意力函数，然后再并起来再投影回来\n\n  虽然这个地方看到的是非常多的小矩阵的乘法，实际上在实现的时候也可以通过一次的矩阵乘法来实现\n\n<img src=\"http://img.peterli.club/joy/202210141054797.png\" alt=\"multi-3\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141054369.png\" alt=\"multi-4\" style=\"zoom:50%;\" />\n\n### Applications of Attention in our Model\n\n在Transformer这个模型里面是如何使用注意力的，下面为三种使用情况\n\n- 编码器\n\n  假设你的句子长度是n的话，输入是一个n个长为d的向量，假设我们的$p_n$大小设成了1，每个输入它的词对应的是一个长为d的向量，然后我们这里一共有n个这样的东西\n\n  注意力层有三个输入，分别是key、value、query，图里一根线过来然后复制了三下，同样一个东西既作为key、也作为value和query，所以这个东西叫做自注意力机制，key、value和query其实是一个东西，就是自己本身\n\n  输入了n个query，每个query会拿到一个输出，即有n个输出，而且这个输出和value因为长度是一样的，那么输出的维度其实也是那个d，意味着输入和输出的大小其实是一个东西\n\n  对每个query都会计算一个输出，输出是value的一个加权和，权重是来自于query和key的一些东西，但它本身就是一个东西，意味着这个东西实际上本身就是你的输入的一个加权的一个和，绿色线代表权重的话，因为这个权重本身就是这个向量跟每一个输入的别的向量计算相似度，那么他跟自己算肯定是最大的\n\n  假设我们不考虑多头和有投影的情况，你的输出就是你的输入的一个加权和，你的权重来自于你自己本身跟各个向量之间的一个相似度，但是如果有多头的话，因为有投影，其实我们在这个地方会学习h个不一样的距离空间出来，使得你出来的东西当然是会有一点点不一样了\n\n  <img src=\"http://img.peterli.club/joy/202210141054763.png\" alt=\"application-1\" style=\"zoom: 33%;\" />\n- 解码器(Masked Multi-Head Attention)\n\n  解码器输入跟编码器一样也是一个线进来复制三次，只是长度可能是m，维度其实也是一样的，所以跟编码器一样的自注意力，唯一不一样的是这里有masked，意味着在解码器你的这些后面的东西这些权重要设为0，这是mask的作用\n- 解码器(Multi-Head Attention)\n\n  这个地方不再是自注意力，key和value来自于编码器的输出，query是来自于解码器下一个attention的输入，编码器最后一层的输出是n个长为d的向量，那么解码器的masked attention就是最下面那个attention的输出是m个也是长为d的向量\n\n  编码器的输出作为key和value进来，然后解码器下一层的输出作为query进来，意味着对解码器的每一个输出作为query，我要算一个我要的输出，那么输出是来自于value的一个加权和，那就是来自于编码器输出的加权和\n\n  意味着在这个attention干的事情，其实就是去有效的把你的编码器里面的一些输出根据我想要的东西把它拎出来\n\n  举个具体的例子，假设你是在做英文翻译中文，假设第一个词是hello，第二个词是hello world的话，那么你的中文他就是第一个当是“你”，“你好”，所以你会知道说在算好的时候，如果它作为query的时候，那么去看hello的这个向量应该是会相近一点，给他比较大的权重，但是world是后面的次相关，我发现到word这个词跟我这个query相关度没那么高，在计算你的相似度的时候，那么就是说在算“好”的时候呢，我会给他一个比较大的权重，但是我在后面如果还有“你好世界”，如果是个“世”的话，那么在这个query的时候，我再去算它的输出这个东西的时候，它那么就会给第二个向量给一个比较大的一个权重出来\n\n  意味着根据你在解码器的时候，你的输入的不一样，那么我会去根据你的当前的那一个向量去在编码器的输出里面去挑我感兴趣的东西，也就是注意到你感兴趣的东西，忽略掉你不感兴趣的东西，这个也是说attention是如何在编码器和解码器之间传递信息的时候起到的一个作用\n\n<img src=\"http://img.peterli.club/joy/202210141055976.png\" alt=\"application-2\" style=\"zoom: 33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141055338.png\" alt=\"application-3\" style=\"zoom:50%;\" />\n\n## Position-wise Feed-Forward Networks\n\n其实就是一个fully connected feed-forward network，他就是一个MLP了，他不一样的是说它是applied to each position seperately and identically ，position就是你输入的那一个序列，每个词就是一个点，那就是一个position，然后他就是把一个MLP对每一个词作用一次，然后对每个词作用的是同样一个MLP，所以这个就是point wise的意思，说白了就是MLP只是作用在最后一个维度\n\n<img src=\"http://img.peterli.club/joy/202210141055321.png\" alt=\"position-1\" style=\"zoom:50%;\" />\n\n在注意力层每一个query对应的那一个输出就是长为512，那就是说这个$x$是一个512的一个向量，它说$W_1$会把512投影成2048，这个维度就等于是我把它的维度扩大了四倍，因为最后你有一个残差连接，你还得投影回去，所以$W_2$又把2048投影回了512\n\n说白了上面那个就是一个单隐藏层的MLP，然后中间隐藏层把你的输入扩大四倍，最后输出的时候也回到你输入的大小，用pytorch来实现的话，其实就是把两个线性层放在一起，你都不需要改任何参数，因为pytorch去当你的输入是一个3D的时候，它默认就是在最后一个维度做计算\n\n- 与RNN做对比\n\n  考虑最简单的情况，没有残差连接也没有layernorm，attention也是单头然后没有投影\n\n  attention的作用是把整个序列里面的信息抓取出来，做一次汇聚，感兴趣的东西已经抓取出来了，以至于在做投影在做MLP的时候，映射成我想要的那个语义空间的时候，因为已经含有序列信息，所以每个MLP只要在对每个点独立做就行了\n\n  这就是整个Transformer是如何抽取序列信息，然后把这些信息加工成我最后要的那个语义空间那个向量的过程\n\n  对于第一个点，说白了也是做一个线性层，没有隐藏层的MLP就是一个纯线性的层，第一个点就是直接做出去就完事了，我还是用之前这个MLP它的权重跟之前是一样的，但是我的时序信息用绿色表示，他就是把这个东西它的上一个时刻的输出放回来，作为跟输入一起并入进去，这样子就完成了我信息的一个传递，然后用绿色的线表示的是之前的信息，蓝色的线表示的是当前的信息，这样子会得到一个当前的一个输出，历史信息就是上一次的那个输出作为历史信息进来，然后得到我当前的一个输出\n\n  RNN是跟Transformer是一样的，都是用一个线性层或者说一个MLP来做一个语义空间的一个转换，但是不一样的是你如何传递序列的信息\n\n  RNN是把上一个时刻的信息输出传入下一个时刻做输入，但是在Transformer里面它是通过一个attention层然后再全局的去拉到整个序列里面信息，然后用MLP做语义的转换\n\n  这就是两者的区别，但是他们的关注点都是在你怎么有效的去使用你的序列的信息\n\n  <img src=\"http://img.peterli.club/joy/202210141117036.png\" alt=\"position-2\" style=\"zoom: 33%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141118537.png\" alt=\"position-3\" style=\"zoom:50%;\" />\n\n## Embeddings and softmax\n\n输入是一个个的词或者一个叫词源叫token，那我需要把它映射成一个向量，embedding就是说给任何一个词，我学习一个长为d的一个向量来表示它，这个$d_{model}$可以认为是等于512了\n\n这里是说你的编码器要一个embedding，你的解码器输入也要一个embedding，在softmax前面那个线性也需要一个embedding，他说我这三个是一样的权重，这样子我训练起来会简单一点\n\n另外一个有意思的是说他把权重乘了一个根号$\\sqrt{d_{model}}$，做这个事情是因为你在学embedding的时候，多多少少会把每一个向量它的L2 Norm学成比较小的，比如说学成1，就不管你的维度多大最后你的值都会等于1，也就是说你的维度一大呢，你学的一些权重值就会变小\n\n如果加上了Positional Encoding，他不会随着你的长度变成了他把你的Norm固定住，所以乘了$\\sqrt{d_{model}}$之后，使得他们两个相加的时候在一个scale上大家都差不多，就是它做了一个hat\n\n<img src=\"http://img.peterli.club/joy/202210141118127.png\" alt=\"embedding\" style=\"zoom:50%;\" />\n\n## Positional Encoding\n\n为什么要有这个东西，因为attention这个东西是不会有时序信息的，你的输出是你的value的一个加权和，你这个权重是query和key之间的那个距离，他跟你的序列信息是无关的，就我根本就不会去看你那个key value里面那些对在序列里面哪个地方\n\n意味着说我给你一句话，我把顺序任何打乱之后，我attention出来结果都是一样的，顺序会变，但是值不会变，但是这个会有问题的，所以需要加时序信息\n\n它的做法是说在我的输入里面加入时序信息，就是说你这一个词，他在一个位置i，i这个位置这个数字(12345)加到你的输入里面，所以这个东西叫做positional encoding ，有公式计算提供\n\n其中pos是位置，i是维度。也就是说，位置编码的每个维度对应于一个正弦波。波长形成一个从2$\\pi$到10000*2$\\pi$的几何级数。我们选择这个函数是因为我们假设它可以让模型很容易地学会通过相对位置来参加，因为对于任何固定的偏移量$k$，$PE_{pos+k}$可以表示为$PE_{pos}$的线性函数\n\n我们还试验了使用学习的位置嵌入来代替，并发现两个版本产生了几乎相同的结果（见表3行(E)）。我们选择了正弦波版本，因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列\n\n<img src=\"http://img.peterli.club/joy/202210141118413.png\" alt=\"encoding-1\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141118587.png\" alt=\"encoding-2\" style=\"zoom:50%;\" />\n\n# Why self-attention？\n\n解释了一个表\n\n<img src=\"http://img.peterli.club/joy/202210141118604.png\" alt=\"attention-1\" style=\"zoom:50%;\" />\n\n横坐标：Self-Attention、Recurrent、Convolutional、Self-Attention(restricted)\n\n纵坐标：Complexity per Layer(每层的计算复杂度)、Sequential Operations(顺序操作)、Maximum Path Length(最大路径长度)\n\n- 第一列是说我的计算复杂度当然是越低越好\n- 第二列是说我的顺序的计算越少越好，顺序的计算就是说你下一步计算必须要等前面多少步计算完成，在算一个Layer的时候，你越不要等那么你的并行度就越高\n- 第三列是说一个信息从一个数据点走到另一个数据点要走多远，这也是越短越好。\n\n看一下每一个层它代表的数值是什么意思。\n\n- Self-Attention(自注意力层)\n\n  - Complexity per Layer\n\n    n这个地方是你序列的长度，d是你向量的长度，整个自注意力说白了就是几个矩阵做运算，其中一个矩阵是query矩阵乘以key矩阵，n行n个query，d列维度是d，k也是一样的，两个矩阵一乘，算法复杂度就是n的平方乘以d\n  - Sequential Operations\n\n    因为你就是那么几个矩阵乘法，矩阵里面它可以认为是并行度比较高的，所以是O(1)\n  - Maximum Path Length\n\n    你从一个点的信息想跳到另外一个点要走多少步，在attention里面就是一个query可以跟所有的key去做运算，输出是value加权和，就是说query跟任何一个很远的一个key value pair，我只要一次就能过来，所以这个长度是比较短的\n- Recurrent(循环层)\n\n  - Complexity per Layer\n\n    如果你的序列是乘了n的话，他就一个一个做运算，每个里面它的主要的计算就是一个n乘以n的一个矩阵，就是一个dense layer，然后再乘以一个长为d的一个输入，所以是d的平方，然后要做n次，所以复杂度是n乘以d的平方\n  - Sequential Operations\n\n    在循环的时候，因为你是要一步一步做运算，当前时间刻的那个词需要等待前面那个东西完成，所以导致你是一个成为n的一个序列化的操作，在并行上是比较吃亏的\n  - Maximum Path Length\n\n    你最初点的那个历史信息需要到最后那一个点的话，需要走过n步才能过去，所以这个地方最长是O(n)\n- Convolutional(卷积)\n\n  - Complexity per Layer\n\n    没有特别解释卷积在序列上怎么做\n\n    具体做法是他用一个ed的卷积，所以它的kernel就是个k，n是你的长度，d就是你的输入通道数和输出通道数，k一般不大可以认为是常数。所以跟RNN的复杂度是差不多的\n  - Sequential Operations\n\n    卷积操作里面的并行度很高，做起来通常比RNN要快一点\n  - Maximum Path Length\n\n    卷积每一次一个点是有一个长为k的一个窗口来看，所以它一次一个信息在k距离内是能够一次就能传递，如果超过k的话，要传递信息就要通过多层一层一层上去\n- Self-Attention(restricted)受限的自注意力\n\n  当我做注意力的时候，我的query只跟我最近的r个邻居去做运算，这样子就不用去算n平方这个东西，但是这个问题就是，这样子的话有两个比较长的远的一个点需要走几步才能过来\n\n  self-attention主要关心说的是特别长的序列，你真的能够把整个信息揉的比较好一点，所以这个受限的用的不是很多，大家都是用最原始的版本，不用太做受限\n\n<img src=\"http://img.peterli.club/joy/202210141119639.png\" alt=\"attention-2\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141119810.png\" alt=\"attention-3\" style=\"zoom: 50%;\" />\n\n# Training\n\n讲训练的一些设置\n\n## Training Data and Batching\n\n我的训练数据集和我的batching怎么做的，数据集一个是英语翻德语，用的是标准的WMT 2014的数据，它这个里面有4.5万个句子的对，用的是byte-pair encoding，就是bpe\n\n大概思想是说，你不管是英语还是德语，其实一个词里面有很多种变化，但是你如果直接把每一个词做成一个token的话，你会导致你的字典里面的东西会比较多，而且一个动词的可能有几种变化形式，你做成不一样的词的时候，他们之间的区别模型是不知道了，bpe相对来说就是把你那些词根跟你提出来，这样好处是说它可以把整个字典这样的比较小\n\n这里用的是37000个token的一个字典，而且他是在英语和德语之间是共享的，就是说我们不再为英语构造一个字典，不再为德语构造一个字典，这样的好处是说，我整个编码器和解码器的一个embedding就可以用一个东西了，而且整个模型变得更加简单，编码器解码器那个embedding它是共享权重的\n\n另一个是英语翻法语的话，他用了一个更大的一个数据集\n\n## Hardware and Schedule\n\n训练使用了八个P100的GPU，base模型是用的一个小一点的参数，他每一个batch训练的时间是0.4秒，然后一共训练了10万步，一共就是在8个GPU上训练了12个小时，一个大的模型，这样的一个batch训练需要一秒钟，然后一共训练了30万步，最后是一台机器3.5天\n\n## Optimizer\n\n使用的是Adam，图下是他的参数，学习率是图下公式算出来的，它的学习率是根据你的模型那个宽度的-0.5次方，当你的模型越宽的时候，你学的那些向量越长的时候，你的学习率要低一点，另一个它是有warmup，就是从一个小的值慢慢地爬到了一个高的值，爬到之后再根据你的步数按照0.5次方衰减，最后他说我的warmup是4000\n\n这里基本没有东西可以调，就是你的学习率几乎是不用调的，取决于第一adam对学习率确实不那么敏感，第二是说他这个地方也是把整个就两个模型这个东西已经考虑进来了，这个schedule也算是不错的schedule了，所以在学习率是不需要调的\n\n<img src=\"http://img.peterli.club/joy/202210141119601.png\" alt=\"optimizer-1\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141119799.png\" alt=\"optimizer-2\" style=\"zoom:50%;\" />\n\n## Regularization\n\n用了三个正则化\n\n- Residual Dropout\n\n  说白了就是说对每一个子层，子层就是包括了你的多头的注意力层和你之后的MLP，在每个层的输出上，在他进入残差连接之前和在进入layernorm之前，它使用了一个dropout，它的dropout率是0.1，也就是说把这些输出的10%的那些元素只乘0.1，剩下的那些只能乘以1.1\n\n  另外一个他在输入加上你的词嵌入，再加上你的positional encoding的时候，在他上面也用了一个dropout，也就是把10%的元素值乘了一个0.1，有意思的是说你基本看到对每一个带权重的乘，他在输出上都使用的dropout，虽然dropout率并不是特别高，但是它使用了大量的dropout层，来对它的模型做正则化\n- Label Smoothing\n\n  这个技术是在inception net v3中让大家知道的，意思是说我们用softmax去学一个东西的时候，我的标号是正确的是1，错误的是0，对于正确的那一个label的softmax的值去逼近于1，但是我们知道softmax是很难逼近于1的，因为它里面是一个指数，它是一个很soft的一个东西，就是说它需要你的输出接近无限大的时候才能逼近于1，这个使得训练比较难\n\n  一般的做法是说你不要让搞成那么特别难的0和1，你可以把1的值往下降点，但是这个地方降得比较狠，是降成了0.1，就是说对于正确的那个词，我只需要我的softmax的输出是到0.1就行了，叫置信度是0.1就行了，不需要做的很高，剩下的那些值就可以是0.9除以你的字典的大小\n\n  他说这里会损失你的perplexity，log lost做指数，基本上可以认为是你的模型不确信度，因为你这个地方让你学说我正确答案，我也只要给个10%是对的就行了，所以当然你的不确信度会增加，你这个值会变高，但是他说我的模型会不那么确信，会提升我的精度和我的BLEU分数，因为精度和bleu的分数才是我们关心的重点\n- 不同的超参数之间的一些对比\n\n  ![regular-1](http://img.peterli.club/joy/202210141119647.png)\n\n  N是堆了多少层\n\n  $d_{model}$是你这个模型的宽度，就是一个token进来要表示成一个多长的向量\n\n  $d_{ff}$表示的是你拿MLP中间那个隐藏层的输出的大小\n\n  h是你的头的个数就是你注意力层的头的个数\n\n  $d_k$、$d_v$分别是你一个头里面那个key和value的维度\n\n  $P_{drop}$是你dropout是你的丢弃的率\n\n  $E_{ls}$是说你最后label smoothing的时候，你这个要学的那个label的真实值是等于多少，train steps是说你要训练多少个batch\n\n  看一下base模型，他用了六个层，每一层的宽度是512，$d_{ff}$是它的4倍，头的数($h$)乘以你的维度数($d_k$)是等于$d_{model}$，$P_{drop}$=0.1，$E_{ls}$=0.1\n\n  看一下big模型，基本上可以看到是说，层数没有变，但是模型的宽度乘了两倍，$d_{ff}$自然就翻了倍，头的个数乘了两倍，另外一个模型变得更加复杂，所以他用了一个比较大的丢弃率0.3，而且模型更加复杂了收敛会慢一点，因为你的学习率是变低了的，这个地方训练了30万个批量大小\n\n  所以可以看到整个模型参数相对来说还是比较简单的，模型层数、宽度、有多少个头，剩下的这些东西基本上都是可以按比例算过来的，这也是Transformer架构的一个好处，虽然模型比较复杂，但是也没有太多东西可以调，这个设计上来说让后面的人更加方便一点\n\n  bert其实就是把这个架构拉过去，然后把几个参数改了一下，GPT也是这样的\n\n<img src=\"http://img.peterli.club/joy/202210141119896.png\" alt=\"training-1\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141120317.png\" alt=\"training-2\" style=\"zoom:50%;\" />\n\n# Results\n\n## Machine Translation\n\n<img src=\"http://img.peterli.club/joy/202210141120776.png\" alt=\"translation-1\" style=\"zoom:50%;\" />\n\n## Model Variations\n\n<img src=\"http://img.peterli.club/joy/202210141120215.png\" alt=\"variation-1\" style=\"zoom:50%;\" />\n\n## English Constituency Parsing\n\n<img src=\"http://img.peterli.club/joy/202210141120838.png\" alt=\"parsing-1\" style=\"zoom:50%;\" />\n\n<img src=\"http://img.peterli.club/joy/202210141120740.png\" alt=\"parsing-2\" style=\"zoom:50%;\" />\n\n# 评论\n\n- 写作\n\n  这篇文章的写作是非常简洁的，因为他每一句话就基本上在讲一件事情\n\n  另一个是说没有用太多的写作技巧，基本上就是说你看我提出一个什么东西，这个模型长什么样子，跟CNN和RNN比是什么样子，最后的结论是什么，最后的实验结果是什么东西，这个写法不推荐，因为对一篇文章来说，你需要在讲一个故事，让你的读者有代入感\n\n  但是你要在一篇文章里面发现那么多东西的话，也没那么多篇幅来讲一个很好的故事\n\n  > 经验：假设你写篇文章的话，你可以选择把你的东西减少一点，甚至把一些东西，不那么重要的东西放到你的附录里面，但是在正文的时候，你还是最好讲个故事，说你为什么做这个事情，你的一些设计的理念是什么样子的，你对整个文字的一些思考是什么样子的，这个东西让大家会觉得你的文章更加有深度一些\n  >\n- Transformer这个模型本身\n\n  我们现在当然可以看到Transformer模型不仅仅是用在机器翻译上面，它也能够用在几乎所有的NLP的任务上面，在后续的工作，BERT、GPT，让大家能够训练很大的易训的模型，能够极大的提升所有NLP里面的任务的性能，这个有点像CNN在对整个计算机视觉的改变，我们能够训练一个大的CNN的模型，使得别的人物也能够从中受益\n\n  另外一个是说，CNN给整个计算机视觉的研究者提供了一个同样的一个框架，使得我只要学会CNN就行了，而不需要去管以前跟任务相关的那么多的专业的知识，比如说做特征提取，对整个任务怎么建模，Transformer之前我们要做各种各样的数据文本的预处理，然后我要根据NLP的任务给你设计不一样的架构，现在不需要了\n\n  我们用整个transformer这个架构就能够在各个任务上做的非常好的成绩，而且他预设的模型也让大家的训练变得更加简单\n","tags":["Transformer","AI","Paper"],"categories":["Paper","Transformer"]}]